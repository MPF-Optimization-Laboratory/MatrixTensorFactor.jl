
@book{fletcher_physics_2012,
   title = {The {Physics} of {Musical} {Instruments}},
   isbn = {978-1-4612-2980-3},
   abstract = {The history of musical instruments is nearly as old as the history of civilization itself, and the aesthetic principles upon which judgments of musical quality are based are intimately connected with the whole culture within which the instruments have evolved. An educated modem Western player or listener can make critical judgments about particular instruments or particular per formances but, to be valid, those judgments must be made within the appro priate cultural context. The compass of our book is much less sweeping than the first paragraph might imply, and indeed our discussion is primarily confined to Western musical instruments in current use, but even here we must take account of centuries of tradition. A musical instrument is designed and built for the playing of music of a particular type and, conversely, music is written to be performed on particular instruments. There is no such thing as an "ideal" instrument, even in concept, and indeed the unbounded possibilities of modem digital sound-synthesis really require the composer or performer to define a whole set of instruments if the result is to have any musical coherence. Thus, for example, the sound and response of a violin are judged against a mental image of a perfect violin built up from experience of violins playing music written for them over the centuries. A new instrument may be richer in sound quality and superior in responsiveness, but if it does not fit that image then it is not a better violin.},
   language = {en},
   publisher = {Springer Science \& Business Media},
   author = {Fletcher, Neville H. and Rossing, Thomas D.},
   month = dec,
   year = {2012},
   note = {Google-Books-ID: gvDSBwAAQBAJ},
   keywords = {Science / Acoustics \& Sound, Science / Physics / General, Science / Waves \& Wave Mechanics, Technology \& Engineering / Engineering (General), Technology \& Engineering / General},
   file = {Fletcher and Rossing - 2012 - The Physics of Musical Instruments.pdf:C\:\\Users\\Nicholas\\Zotero\\storage\\VXFNX9F4\\Fletcher and Rossing - 2012 - The Physics of Musical Instruments.pdf:application/pdf},
}

@book{burnham_model_1998,
   address = {New York, NY},
   title = {Model {Selection} and {Inference}},
   copyright = {http://www.springer.com/tdm},
   isbn = {978-1-4757-2919-1 978-1-4757-2917-7},
   url = {http://link.springer.com/10.1007/978-1-4757-2917-7},
   abstract = {We wrote this book to introduce graduate students and research workers in var­ ious scientific disciplines to the use of information-theoretic approaches in the analysis of empirical data. In its fully developed form, the information-theoretic approach allows inference based on more than one model (including estimates of unconditional precision); in its initial form, it is useful in selecting a "best" model and ranking the remaining models. We believe that often the critical issue in data analysis is the selection of a good approximating model that best represents the inference supported by the data (an estimated "best approximating model"). In­ formation theory includes the well-known Kullback-Leibler "distance" between two models (actually, probability distributions), and this represents a fundamental quantity in science. In 1973, Hirotugu Akaike derived an estimator of the (relative) Kullback-Leibler distance based on Fisher's maximized log-likelihood. His mea­sure, now called Akaike 's information criterion (AIC), provided a new paradigm for model selection in the analysis of empirical data. His approach, with a funda­ mental link to information theory, is relatively simple and easy to use in practice, but little taught in statistics classes and far less understood in the applied sciences than should be the case. We do not accept the notion that there is a simple, "true model" in the biological sciences.},
   language = {en},
   urldate = {2024-09-04},
   publisher = {Springer},
   author = {Burnham, Kenneth P. and Anderson, David R.},
   year = {1998},
   doi = {10.1007/978-1-4757-2917-7},
   keywords = {data analysis, Inference, information theory, Likelihood, Model Selection, optimization, statistics},
   file = {Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\S9GFBPIN\\Burnham and Anderson - 1998 - Model Selection and Inference.pdf:application/pdf},
}

@article{schwarz_estimating_1978,
   title = {Estimating the {Dimension} of a {Model}},
   volume = {6},
   issn = {0090-5364, 2168-8966},
   url = {https://projecteuclid.org/journals/annals-of-statistics/volume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.1214/aos/1176344136.full},
   doi = {10.1214/aos/1176344136},
   abstract = {The problem of selecting one of a number of models of different dimensions is treated by finding its Bayes solution, and evaluating the leading terms of its asymptotic expansion. These terms are a valid large-sample criterion beyond the Bayesian context, since they do not depend on the a priori distribution.},
   number = {2},
   urldate = {2024-09-04},
   journal = {The Annals of Statistics},
   author = {Schwarz, Gideon},
   month = mar,
   year = {1978},
   note = {Publisher: Institute of Mathematical Statistics},
   keywords = {62F99, 62J99, Akaike information criterion, asymptotics, dimension},
   pages = {461--464},
   file = {Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\5XCS6FED\\Schwarz - 1978 - Estimating the Dimension of a Model.pdf:application/pdf},
}

@article{neath_bayesian_2012,
   title = {The {Bayesian} information criterion: background, derivation, and applications},
   volume = {4},
   copyright = {Copyright © 2011 Wiley Periodicals, Inc.},
   issn = {1939-0068},
   shorttitle = {The {Bayesian} information criterion},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wics.199},
   doi = {10.1002/wics.199},
   abstract = {The Bayesian information criterion (BIC) is one of the most widely known and pervasively used tools in statistical model selection. Its popularity is derived from its computational simplicity and effective performance in many modeling frameworks, including Bayesian applications where prior distributions may be elusive. The criterion was derived by Schwarz (Ann Stat 1978, 6:461–464) to serve as an asymptotic approximation to a transformation of the Bayesian posterior probability of a candidate model. This article reviews the conceptual and theoretical foundations for BIC, and also discusses its properties and applications. WIREs Comput Stat 2012, 4:199–203. doi: 10.1002/wics.199 This article is categorized under: Statistical and Graphical Methods of Data Analysis {\textgreater} Bayesian Methods and Theory Statistical and Graphical Methods of Data Analysis {\textgreater} Information Theoretic Methods Statistical Learning and Exploratory Methods of the Data Sciences {\textgreater} Modeling Methods},
   language = {en},
   number = {2},
   urldate = {2024-09-04},
   journal = {WIREs Computational Statistics},
   author = {Neath, Andrew A. and Cavanaugh, Joseph E.},
   year = {2012},
   note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/wics.199},
   keywords = {Bayes factors, BIC, model selection criterion, Schwarz information criterion},
   pages = {199--203},
   file = {Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\WTAEYUYL\\Neath and Cavanaugh - 2012 - The Bayesian information criterion background, de.pdf:application/pdf;Snapshot:C\:\\Users\\Nicholas\\Zotero\\storage\\UUA99DWY\\wics.html:text/html},
}

@article{akaike_new_1974,
   title = {A new look at the statistical model identification},
   volume = {19},
   issn = {1558-2523},
   url = {https://ieeexplore.ieee.org/document/1100705},
   doi = {10.1109/TAC.1974.1100705},
   abstract = {The history of the development of statistical hypothesis testing in time series analysis is reviewed briefly and it is pointed out that the hypothesis testing procedure is not adequately defined as the procedure for statistical model identification. The classical maximum likelihood estimation procedure is reviewed and a new estimate minimum information theoretical criterion (AIC) estimate (MAICE) which is designed for the purpose of statistical identification is introduced. When there are several competing models the MAICE is defined by the model and the maximum likelihood estimates of the parameters which give the minimum of AIC defined by AIC = (-2)log-(maximum likelihood) + 2(number of independently adjusted parameters within the model). MAICE provides a versatile procedure for statistical model identification which is free from the ambiguities inherent in the application of conventional hypothesis testing procedure. The practical utility of MAICE in time series analysis is demonstrated with some numerical examples.},
   number = {6},
   urldate = {2024-09-04},
   journal = {IEEE Transactions on Automatic Control},
   author = {Akaike, H.},
   month = dec,
   year = {1974},
   note = {Conference Name: IEEE Transactions on Automatic Control},
   keywords = {Art, Estimation theory, History, Linear systems, Maximum likelihood estimation, Roundoff errors, Sampling methods, Stochastic processes, Testing, Time series analysis},
   pages = {716--723},
   file = {IEEE Xplore Abstract Record:C\:\\Users\\Nicholas\\Zotero\\storage\\YZ6VP86Q\\1100705.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\CFC9R9QP\\Akaike - 1974 - A new look at the statistical model identification.pdf:application/pdf},
}

@article{monti_consensus_2003,
   title = {Consensus {Clustering}: {A} {Resampling}-{Based} {Method} for {Class} {Discovery} and {Visualization} of {Gene} {Expression} {Microarray} {Data}},
   volume = {52},
   issn = {1573-0565},
   shorttitle = {Consensus {Clustering}},
   url = {https://doi.org/10.1023/A:1023949509487},
   doi = {10.1023/A:1023949509487},
   abstract = {In this paper we present a new methodology of class discovery and clustering validation tailored to the task of analyzing gene expression data. The method can best be thought of as an analysis approach, to guide and assist in the use of any of a wide range of available clustering algorithms. We call the new methodology consensus clustering, and in conjunction with resampling techniques, it provides for a method to represent the consensus across multiple runs of a clustering algorithm and to assess the stability of the discovered clusters. The method can also be used to represent the consensus over multiple runs of a clustering algorithm with random restart (such as K-means, model-based Bayesian clustering, SOM, etc.), so as to account for its sensitivity to the initial conditions. Finally, it provides for a visualization tool to inspect cluster number, membership, and boundaries. We present the results of our experiments on both simulated data and real gene expression data aimed at evaluating the effectiveness of the methodology in discovering biologically meaningful clusters.},
   language = {en},
   number = {1},
   urldate = {2024-09-04},
   journal = {Machine Learning},
   author = {Monti, Stefano and Tamayo, Pablo and Mesirov, Jill and Golub, Todd},
   month = jul,
   year = {2003},
   keywords = {Artificial Intelligence, class discovery, gene expression microarrays, model selection, unsupervised learning},
   pages = {91--118},
   file = {Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\LM75BQTQ\\Monti et al. - 2003 - Consensus Clustering A Resampling-Based Method fo.pdf:application/pdf},
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Rank and number of sources IDing %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{ubaru_fast_2016,
   title = {Fast methods for estimating the {Numerical} rank of large matrices},
   url = {https://proceedings.mlr.press/v48/ubaru16.html},
   abstract = {We present two computationally inexpensive techniques for estimating the numerical rank of a matrix, combining powerful tools from computational linear algebra. These techniques exploit three key ingredients. The first is to approximate the projector on the non-null invariant subspace of the matrix by using a polynomial filter. Two types of filters are discussed, one based on Hermite interpolation and the other based on Chebyshev expansions. The second ingredient employs stochastic trace estimators to compute the rank of this wanted eigen-projector, which yields the desired rank of the matrix. In order to obtain a good filter, it is necessary to detect a gap between the eigenvalues that correspond to noise and the relevant eigenvalues that correspond to the non-null invariant subspace. The third ingredient of the proposed approaches exploits the idea of spectral density, popular in physics, and the Lanczos spectroscopic method to locate this gap.},
   language = {en},
   urldate = {2024-08-19},
   booktitle = {Proceedings of {The} 33rd {International} {Conference} on {Machine} {Learning}},
   publisher = {PMLR},
   author = {Ubaru, Shashanka and Saad, Yousef},
   month = jun,
   year = {2016},
   note = {ISSN: 1938-7228},
   pages = {468--477},
   file = {Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\LM7SYJBR\\Ubaru and Saad - 2016 - Fast methods for estimating the Numerical rank of .pdf:application/pdf},
}

@article{cai_rank_2023,
   title = {Rank selection for non-negative matrix factorization},
   volume = {42},
   copyright = {© 2023 The Authors. Statistics in Medicine published by John Wiley \& Sons Ltd.},
   issn = {1097-0258},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.9934},
   doi = {10.1002/sim.9934},
   abstract = {Non-Negative Matrix Factorization (NMF) is a widely used dimension reduction method that factorizes a non-negative data matrix into two lower dimensional non-negative matrices: one is the basis or feature matrix which consists of the variables and the other is the coefficients matrix which is the projections of data points to the new basis. The features can be interpreted as sub-structures of the data. The number of sub-structures in the feature matrix is also called the rank. This parameter controls the model complexity and is the only tuning parameter for the NMF model. An appropriate rank will extract the key latent features while minimizing the noise from the original data. However due to the large amount of optimization error always present in the NMF computation, the rank selection has been a difficult problem. We develop a novel rank selection method based on hypothesis testing, using a deconvolved bootstrap distribution to assess the significance level accurately. Through simulations, we compare our method with a rank selection method based on hypothesis testing using bootstrap distribution without deconvolution and a method based on cross-validation; we demonstrate that our method is not only accurate at estimating the true ranks for NMF, especially when the features are hard to distinguish, but also efficient at computation. When applied to real microbiome data (eg, OTU data and functional metagenomic data), our method also shows the ability to extract interpretable subcommunities in the data.},
   language = {en},
   number = {30},
   urldate = {2024-07-29},
   journal = {Statistics in Medicine},
   author = {Cai, Yun and Gu, Hong and Kenney, Toby},
   year = {2023},
   note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.9934},
   keywords = {metagenomics, microbial communities, non-negative matrix factorization, rank selection, subcommunities},
   pages = {5676--5693},
   file = {Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\PMDVVA8B\\Cai et al. - 2023 - Rank selection for non-negative matrix factorizati.pdf:application/pdf;Snapshot:C\:\\Users\\Nicholas\\Zotero\\storage\\94T2IKIK\\sim.html:text/html},
}

@article{brunet_metagenes_2004,
   title = {Metagenes and molecular pattern discovery using matrix factorization},
   number = {101},
   journal = {Proc. Natl. Acad. Sci.},
   author = {Brunet, J. P. and Tamayo, P. and Golub, T. R. and Mesirov, J. P.},
   year = {2004},
   pages = {4164--4169},
}

@article{fogel_rank_2023,
   title = {On {Rank} {Selection} in {Non}-{Negative} {Matrix} {Factorization} {Using} {Concordance}},
   volume = {11},
   copyright = {http://creativecommons.org/licenses/by/3.0/},
   issn = {2227-7390},
   url = {https://www.mdpi.com/2227-7390/11/22/4611},
   doi = {10.3390/math11224611},
   abstract = {The choice of the factorization rank of a matrix is critical, e.g., in dimensionality reduction, filtering, clustering, deconvolution, etc., because selecting a rank that is too high amounts to adjusting the noise, while selecting a rank that is too low results in the oversimplification of the signal. Numerous methods for selecting the factorization rank of a non-negative matrix have been proposed. One of them is the cophenetic correlation coefficient (ccc), widely used in data science to evaluate the number of clusters in a hierarchical clustering. In previous work, it was shown that ccc performs better than other methods for rank selection in non-negative matrix factorization (NMF) when the underlying structure of the matrix consists of orthogonal clusters. In this article, we show that using the ratio of ccc to the approximation error significantly improves the accuracy of the rank selection. We also propose a new criterion, concordance, which, like ccc, benefits from the stochastic nature of NMF; its accuracy is also improved by using its ratio-to-error form. Using real and simulated data, we show that concordance, with a CUSUM-based automatic detection algorithm for its original or ratio-to-error forms, significantly outperforms ccc. It is important to note that the new criterion works for a broader class of matrices, where the underlying clusters are not assumed to be orthogonal.},
   language = {en},
   number = {22},
   urldate = {2024-01-30},
   journal = {Mathematics},
   author = {Fogel, Paul and Geissler, Christophe and Morizet, Nicolas and Luta, George},
   month = jan,
   year = {2023},
   note = {Number: 22
Publisher: Multidisciplinary Digital Publishing Institute},
   keywords = {NMF, clustering, concordance, cophenetic correlation coefficient, CUSUM, dimensionality reduction, machine learning},
   pages = {4611},
   file = {Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\BJY7YFT2\\Fogel et al. - 2023 - On Rank Selection in Non-Negative Matrix Factoriza.pdf:application/pdf},
}

@article{fu_model_2019,
   title = {Model {Selection} for {Non}-{Negative} {Tensor} {Factorization} with {Minimum} {Description} {Length}},
   volume = {21},
   issn = {1099-4300},
   url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7515125/},
   doi = {10.3390/e21070632},
   abstract = {Non-negative tensor factorization (NTF) is a widely used multi-way analysis approach that factorizes a high-order non-negative data tensor into several non-negative factor matrices. In NTF, the non-negative rank has to be predetermined to specify the model and it greatly influences the factorized matrices. However, its value is conventionally determined by specialists’ insights or trial and error. This paper proposes a novel rank selection criterion for NTF on the basis of the minimum description length (MDL) principle. Our methodology is unique in that (1) we apply the MDL principle on tensor slices to overcome a problem caused by the imbalance between the number of elements in a data tensor and that in factor matrices, and (2) we employ the normalized maximum likelihood (NML) code-length for histogram densities. We employ synthetic and real data to empirically demonstrate that our method outperforms other criteria in terms of accuracies for estimating true ranks and for completing missing values. We further show that our method can produce ranks suitable for knowledge discovery.},
   number = {7},
   urldate = {2023-08-16},
   journal = {Entropy},
   author = {Fu, Yunhui and Matsushima, Shin and Yamanishi, Kenji},
   month = jun,
   year = {2019},
   pmid = {33267345},
   pmcid = {PMC7515125},
   pages = {632},
   file = {PubMed Central Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\DKWFI9PC\\Fu et al. - 2019 - Model Selection for Non-Negative Tensor Factorizat.pdf:application/pdf},
}

@misc{austin_tensor_2014,
   address = {Sandia National Labs, Livermore, CA},
   title = {Tensor {Rank} {Prediction} via {Cross} {Validation}},
   language = {en},
   urldate = {2023-08-16},
   author = {Austin, Woody and Kolda, Tamara G. and Plantenga, Todd},
   month = aug,
   year = {2014},
   file = {Austin - Tensor Rank Prediction via Cross Validation.pdf:C\:\\Users\\Nicholas\\Zotero\\storage\\V6EM6YBP\\Austin - Tensor Rank Prediction via Cross Validation.pdf:application/pdf},
}

@inproceedings{satopaa_finding_2011,
   title = {Finding a "{Kneedle}" in a {Haystack}: {Detecting} {Knee} {Points} in {System} {Behavior}},
   shorttitle = {Finding a "{Kneedle}" in a {Haystack}},
   doi = {10.1109/ICDCSW.2011.20},
   abstract = {Computer systems often reach a point at which the relative cost to increase some tunable parameter is no longer worth the corresponding performance benefit. These "knees” typically represent beneficial points that system designers have long selected to best balance inherent trade-offs. While prior work largely uses ad hoc, system-specific approaches to detect knees, we present Kneedle, a general approach to on line and off line knee detection that is applicable to a wide range of systems. We define a knee formally for continuous functions using the mathematical concept of curvature and compare our definition against alternatives. We then evaluate Kneedle's accuracy against existing algorithms on both synthetic and real data sets, and evaluate its performance in two different applications.},
   booktitle = {2011 31st {International} {Conference} on {Distributed} {Computing} {Systems} {Workshops}},
   author = {Satopaa, Ville and Albrecht, Jeannie and Irwin, David and Raghavan, Barath},
   month = jun,
   year = {2011},
   note = {ISSN: 2332-5666},
   keywords = {Accuracy, Algorithm design and analysis, Congestion control, Curvature, Detection algorithms, Knee, Knee detection, MapReduce, Noise measurement, Protocols, Sensitivity, System behavior},
   pages = {166--171},
   file = {IEEE Xplore Abstract Record:C\:\\Users\\Nicholas\\Zotero\\storage\\E58V29GU\\5961514.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\TELUCPBX\\Satopaa et al. - 2011 - Finding a Kneedle in a Haystack Detecting Knee .pdf:application/pdf},
}


@techreport{ratsimalahelo_rank_2001,
   type = {Working {Paper}},
   title = {Rank {Test} {Based} {On} {Matrix} {Perturbation} {Theory}},
   copyright = {http://www.econstor.eu/dspace/Nutzungsbedingungen},
   url = {https://www.econstor.eu/handle/10419/142479},
   abstract = {In this paper, we propose methods of the determination of the rank of matrix. We consider a rank test for an unobserved matrix for which an estimate exists having normal asymptotic distribution of order N1/2 where N is the sample size. The test statistic is based on the smallest estimated singular values. Using Matrix Perturbation Theory, the smallest singular values of random matrix converge asymptotically to zero in the order O(N-1) and the corresponding left and right singular vectors converge asymptotically in the order O(N-1/2). Moreover, the asymptotic distribution of the test statistic is seen to be chi-squared. The test has advantages over standard tests in being easier to compute. Two approaches are be considered sequential testing strategy and information theoretic criterion. We establish a strongly consistent of the determination of the rank of matrix using both the two approaches. Some economic applications are discussed and simulation evidence is given for this test. Its performance is compared to that of the LDU rank tests of Gill and Lewbel (1992) and Cragg and Donald (1996).},
   language = {eng},
   number = {04/2001},
   urldate = {2024-02-13},
   institution = {EERI Research Paper Series},
   author = {Ratsimalahelo, Zaka},
   year = {2001},
   file = {Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\39QLFBIN\\Ratsimalahelo - 2001 - Rank Test Based On Matrix Perturbation Theory.pdf:application/pdf},
}




%%%%%%%%%%%%%%%%%%%%%%%
%% Probibility & KDE %%
%%%%%%%%%%%%%%%%%%%%%%%

@article{weglarczyk_kernel_2018,
   title = {Kernel density estimation and its application},
   volume = {23},
   copyright = {© The Authors, published by EDP Sciences, 2018},
   issn = {2271-2097},
   url = {https://www.itm-conferences.org/articles/itmconf/abs/2018/08/itmconf_sam2018_00037/itmconf_sam2018_00037.html},
   doi = {10.1051/itmconf/20182300037},
   abstract = {Kernel density estimation is a technique for estimation of probability density function that is a must-have enabling the user to better analyse the studied probability distribution than when using a traditional histogram. Unlike the histogram, the kernel technique produces smooth estimate of the pdf, uses all sample points' locations and more convincingly suggest multimodality. In its two-dimensional applications, kernel estimation is even better as the 2D histogram requires additionally to define the orientation of 2D bins. Two concepts play fundamental role in kernel estimation: kernel function shape and coefficient of smoothness, of which the latter is crucial to the method. Several real-life examples, both for univariate and bivariate applications, are shown.},
   language = {en},
   urldate = {2023-11-09},
   journal = {ITM Web of Conferences},
   author = {Węglarczyk, Stanisław},
   year = {2018},
   note = {Publisher: EDP Sciences},
   pages = {00037},
   file = {Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\GSZNR4ID\\Węglarczyk - 2018 - Kernel density estimation and its application.pdf:application/pdf},
}

@article{obrien_fast_2016,
   title = {A fast and objective multidimensional kernel density estimation method: {fastKDE}},
   volume = {101},
   issn = {0167-9473},
   shorttitle = {A fast and objective multidimensional kernel density estimation method},
   url = {https://www.sciencedirect.com/science/article/pii/S0167947316300408},
   doi = {10.1016/j.csda.2016.02.014},
   abstract = {Numerous facets of scientific research implicitly or explicitly call for the estimation of probability densities. Histograms and kernel density estimates (KDEs) are two commonly used techniques for estimating such information, with the KDE generally providing a higher fidelity representation of the probability density function (PDF). Both methods require specification of either a bin width or a kernel bandwidth. While techniques exist for choosing the kernel bandwidth optimally and objectively, they are computationally intensive, since they require repeated calculation of the KDE. A solution for objectively and optimally choosing both the kernel shape and width has recently been developed by Bernacchia and Pigolotti (2011). While this solution theoretically applies to multidimensional KDEs, it has not been clear how to practically do so. A method for practically extending the Bernacchia–Pigolotti KDE to multidimensions is introduced. This multidimensional extension is combined with a recently-developed computational improvement to their method that makes it computationally efficient: a 2D KDE on 105 samples only takes 1 s on a modern workstation. This fast and objective KDE method, called the fastKDE method, retains the excellent statistical convergence properties that have been demonstrated for univariate samples. The fastKDE method exhibits statistical accuracy that is comparable to state-of-the-science KDE methods publicly available in R, and it produces kernel density estimates several orders of magnitude faster. The fastKDE method does an excellent job of encoding covariance information for bivariate samples. This property allows for direct calculation of conditional PDFs with fastKDE. It is demonstrated how this capability might be leveraged for detecting non-trivial relationships between quantities in physical systems, such as transitional behavior.},
   urldate = {2024-05-15},
   journal = {Computational Statistics \& Data Analysis},
   author = {O'Brien, Travis A. and Kashinath, Karthik and Cavanaugh, Nicholas R. and Collins, William D. and O'Brien, John P.},
   month = sep,
   year = {2016},
   keywords = {Kernel density estimation, ECF, Empirical characteristic function, Histogram, KDE, Multidimensional, Nonuniform FFT, NuFFT},
   pages = {148--160},
   file = {Full Text:C\:\\Users\\Nicholas\\Zotero\\storage\\TD5MUXUY\\O'Brien et al. - 2016 - A fast and objective multidimensional kernel densi.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Nicholas\\Zotero\\storage\\K9XJKRT5\\S0167947316300408.html:text/html},
}

@article{parzen_estimation_1962,
   title = {On {Estimation} of a {Probability} {Density} {Function} and {Mode}},
   volume = {33},
   issn = {0003-4851, 2168-8990},
   url = {https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-33/issue-3/On-Estimation-of-a-Probability-Density-Function-and-Mode/10.1214/aoms/1177704472.full},
   doi = {10.1214/aoms/1177704472},
   abstract = {The Annals of Mathematical Statistics},
   number = {3},
   urldate = {2023-11-17},
   journal = {The Annals of Mathematical Statistics},
   author = {Parzen, Emanuel},
   month = sep,
   year = {1962},
   note = {Publisher: Institute of Mathematical Statistics},
   pages = {1065--1076},
   file = {Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\VPV867RP\\Parzen - 1962 - On Estimation of a Probability Density Function an.pdf:application/pdf},
}




%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Tensor Factorizations %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{chi_tensors_2012,
   title = {On {Tensors}, {Sparsity}, and {Nonnegative} {Factorizations}},
   volume = {33},
   issn = {0895-4798},
   url = {https://epubs.siam.org/doi/10.1137/110859063},
   doi = {10.1137/110859063},
   abstract = {A simple nonrecursive form of the tensor decomposition in d dimensions is presented. It does not inherently suffer from the curse of dimensionality, it has asymptotically the same number of parameters as the canonical decomposition, but it is stable and its computation is based on low-rank approximation of auxiliary unfolding matrices. The new form gives a clear and convenient way to implement all basic operations efficiently. A fast rounding procedure is presented, as well as basic linear algebra operations. Examples showing the benefits of the decomposition are given, and the efficiency is demonstrated by the computation of the smallest eigenvalue of a 19-dimensional operator.},
   number = {4},
   urldate = {2024-07-19},
   journal = {SIAM Journal on Matrix Analysis and Applications},
   author = {Chi, Eric C. and Kolda, Tamara G.},
   month = jan,
   year = {2012},
   note = {Publisher: Society for Industrial and Applied Mathematics},
   pages = {1272--1299},
   file = {Submitted Version:C\:\\Users\\Nicholas\\Zotero\\storage\\BI6IIWMI\\Chi and Kolda - 2012 - On Tensors, Sparsity, and Nonnegative Factorizatio.pdf:application/pdf},
}

@article{chen_tensor_2020,
   title = {On the tensor spectral p-norm and its dual norm via partitions},
   volume = {75},
   issn = {1573-2894},
   url = {https://doi.org/10.1007/s10589-020-00177-z},
   doi = {10.1007/s10589-020-00177-z},
   abstract = {This paper presents a generalization of the spectral norm and the nuclear norm of a tensor via arbitrary tensor partitions, a much richer concept than block tensors. We show that the spectral p-norm and the nuclear p-norm of a tensor can be lower and upper bounded by manipulating the spectral p-norms and the nuclear p-norms of subtensors in an arbitrary partition of the tensor for \$\$1{\textbackslash}le p{\textbackslash}le {\textbackslash}infty\$\$. Hence, it generalizes and answers affirmatively the conjecture proposed by Li (SIAM J Matrix Anal Appl 37:1440–1452, 2016) for a tensor partition and \$\$p=2\$\$. We study the relations of the norms of a tensor, the norms of matrix unfoldings of the tensor, and the bounds via the norms of matrix slices of the tensor. Various bounds of the tensor spectral and nuclear norms in the literature are implied by our results.},
   language = {en},
   number = {3},
   urldate = {2023-08-04},
   journal = {Computational Optimization and Applications},
   author = {Chen, Bilian and Li, Zhening},
   month = apr,
   year = {2020},
   keywords = {15A60, 15A69, 68Q17, 90C59, Block tensor, Nuclear norm, Spectral norm, Tensor norm bound, Tensor partition},
   pages = {609--628},
   file = {Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\6K6ZM8JK\\Chen and Li - 2020 - On the tensor spectral p-norm and its dual norm vi.pdf:application/pdf},
}

@article{wu_manifold_2022,
   title = {Manifold {Regularization} {Nonnegative} {Triple} {Decomposition} of {Tensor} {Sets} for {Image} {Compression} and {Representation}},
   volume = {192},
   issn = {1573-2878},
   url = {https://doi.org/10.1007/s10957-022-02001-6},
   doi = {10.1007/s10957-022-02001-6},
   abstract = {The image processing usually depends on exploring the structure and the geometric information of the tensor objects generated by image data. In the process, the decomposition of the tensor objects is very significant for the dimension reduction and the low-rank representation of image data. In this paper, based on the triple decomposition of third-order tensors and the correlation between different nonnegative tensor objects, a nonnegative triple decomposition model with manifold regularization terms is constructed. Then, an algorithm for the manifold regularization nonnegative triple decomposition is proposed, and the convergence of the algorithm is discussed. Furthermore, experiments on some real-world image data sets are given to illustrate the feasibility and effectiveness of the proposed algorithms.},
   language = {en},
   number = {3},
   urldate = {2023-08-01},
   journal = {Journal of Optimization Theory and Applications},
   author = {Wu, Fengsheng and Li, Chaoqian and Li, Yaotang},
   month = mar,
   year = {2022},
   keywords = {65D15, 65F10, Image compression, Low-rank approximation, Manifold regularization, Nonnegative triple decomposition},
   pages = {979--1000},
   file = {Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\L73B2WIB\\Wu et al. - 2022 - Manifold Regularization Nonnegative Triple Decompo.pdf:application/pdf},
}

@article{oseledets_tensor-train_2011,
   title = {Tensor-{Train} {Decomposition}},
   volume = {33},
   issn = {1064-8275},
   url = {https://epubs.siam.org/doi/10.1137/090752286},
   doi = {10.1137/090752286},
   abstract = {A new method for structured representation of matrices and vectors is presented. The method is based on the representation of a matrix as a d-dimensional tensor and applying the TT-decomposition proposed recently. It turned out that for many important cases the number of parameters to represent an \$n{\textbackslash}times n\$ matrix falls down to \${\textbackslash}mathcal\{O\}({\textbackslash}log{\textasciicircum}\{{\textbackslash}alpha\}n)\$, giving a logarithmic storage. It is shown that this format can be used not only for storage reduction, but also for linear algebra operations. Possible applications include differential and integral equations, and data and image compression.},
   number = {5},
   urldate = {2023-10-12},
   journal = {SIAM Journal on Scientific Computing},
   author = {Oseledets, I. V.},
   month = jan,
   year = {2011},
   note = {Publisher: Society for Industrial and Applied Mathematics},
   pages = {2295--2317},
   file = {Tensor-Train Decomposition  SIAM Journal on Scien.pdf:C\:\\Users\\Nicholas\\Zotero\\storage\\IE6YSRPB\\Tensor-Train Decomposition  SIAM Journal on Scien.pdf:application/pdf},
}

@article{liu_generalized_2024,
   title = {The {Generalized} {Tensor} {Decomposition} with {Heterogeneous} {Tensor} {Product} for {Third}-{Order} {Tensors}},
   volume = {100},
   issn = {1573-7691},
   url = {https://doi.org/10.1007/s10915-024-02637-8},
   doi = {10.1007/s10915-024-02637-8},
   abstract = {Recently, tensor decompositions have attracted increasing attention and shown promising performance in processing multi-dimensional data. However, the existing tensor decompositions assume that the correlation along one mode is homogeneous and thus cannot characterize the multiple types of correlations (i.e., heterogeneous correlation) along the mode in real data. To address this issue, we propose a heterogeneous tensor product that allows us to explore this heterogeneous correlation, which can degenerate into the classic tensor products (e.g., mode product and tensor–tensor product). Equipped with this heterogeneous tensor product, we develop a generalized tensor decomposition (GTD) framework for third-order tensors, which not only induces many novel tensor decompositions but also helps us to better understand the interrelationships between the new tensor decompositions and the existing tensor decompositions. Especially, under the GTD framework, we find that new tensor decompositions can faithfully characterize the multiple types of correlations along the mode. To examine the effectiveness of the new tensor decomposition, we evaluate its performance on a representative data compression task. Extensive experimental results on multispectral images, light field images, and videos compression demonstrate the superior performance of our developed tensor decomposition compared to other existing tensor decompositions.},
   language = {en},
   number = {3},
   urldate = {2024-07-31},
   journal = {Journal of Scientific Computing},
   author = {Liu, Yun-Yang and Zhao, Xi-Le and Ding, Meng and Wang, Jianjun and Jiang, Tai-Xiang and Huang, Ting-Zhu},
   month = jul,
   year = {2024},
   keywords = {Data compression, Generalized tensor decomposition, Heterogeneous tensor product},
   pages = {78},
   file = {Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\CNF3JRBL\\Liu et al. - 2024 - The Generalized Tensor Decomposition with Heteroge.pdf:application/pdf},
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Machine Learning and Software %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{garcia-martin_estimation_2019,
   title = {Estimation of energy consumption in machine learning},
   volume = {134},
   issn = {0743-7315},
   url = {https://www.sciencedirect.com/science/article/pii/S0743731518308773},
   doi = {10.1016/j.jpdc.2019.07.007},
   abstract = {Energy consumption has been widely studied in the computer architecture field for decades. While the adoption of energy as a metric in machine learning is emerging, the majority of research is still primarily focused on obtaining high levels of accuracy without any computational constraint. We believe that one of the reasons for this lack of interest is due to their lack of familiarity with approaches to evaluate energy consumption. To address this challenge, we present a review of the different approaches to estimate energy consumption in general and machine learning applications in particular. Our goal is to provide useful guidelines to the machine learning community giving them the fundamental knowledge to use and build specific energy estimation methods for machine learning algorithms. We also present the latest software tools that give energy estimation values, together with two use cases that enhance the study of energy consumption in machine learning.},
   urldate = {2024-08-22},
   journal = {Journal of Parallel and Distributed Computing},
   author = {García-Martín, Eva and Rodrigues, Crefeda Faviola and Riley, Graham and Grahn, Håkan},
   month = dec,
   year = {2019},
   keywords = {Deep learning, Energy consumption, GreenAI, High performance computing, Machine learning},
   pages = {75--88},
   file = {Full Text:C\:\\Users\\Nicholas\\Zotero\\storage\\FCT5IHCP\\García-Martín et al. - 2019 - Estimation of energy consumption in machine learni.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Nicholas\\Zotero\\storage\\R89CLGKW\\S0743731518308773.html:text/html},
}

@article{patterson_carbon_2022,
   title = {The {Carbon} {Footprint} of {Machine} {Learning} {Training} {Will} {Plateau}, {Then} {Shrink}},
   volume = {55},
   issn = {1558-0814},
   url = {https://ieeexplore.ieee.org/abstract/document/9810097?casa_token=tfp9WGpkT8AAAAAA:_lky4KTVyTHgpsBLzcFzEQrBhruKBUT0DBS53b-qv94-Lv3l9gkHoD5_wiR5zF1YkZi_bSDxpA},
   doi = {10.1109/MC.2022.3148714},
   abstract = {Machine learning (ML) workloads have rapidly grown, raising concerns about their carbon footprint. We show four best practices to reduce ML training energy and carbon dioxide emissions. If the whole ML field adopts best practices, we predict that by 2030, total carbon emissions from training will decline.},
   number = {7},
   urldate = {2024-08-22},
   journal = {Computer},
   author = {Patterson, David and Gonzalez, Joseph and Hölzle, Urs and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David R. and Texier, Maud and Dean, Jeff},
   month = jul,
   year = {2022},
   note = {Conference Name: Computer},
   keywords = {Best practices, Carbon dioxide, Carbon footprint, Emissions, Machine learning, Training data},
   pages = {18--28},
   file = {IEEE Xplore Abstract Record:C\:\\Users\\Nicholas\\Zotero\\storage\\XLDMR7H5\\9810097.html:text/html;Submitted Version:C\:\\Users\\Nicholas\\Zotero\\storage\\MMGIV7SR\\Patterson et al. - 2022 - The Carbon Footprint of Machine Learning Training .pdf:application/pdf},
}

@article{zhu_algorithm_1997,
   title = {Algorithm 778: {L}-{BFGS}-{B}: {Fortran} subroutines for large-scale bound-constrained optimization},
   volume = {23},
   issn = {0098-3500},
   shorttitle = {Algorithm 778},
   url = {https://dl.acm.org/doi/10.1145/279232.279236},
   doi = {10.1145/279232.279236},
   abstract = {L-BFGS-B is a limited-memory algorithm for solving large nonlinear optimization problems subject to simple bounds on the variables. It is intended for problems in which information on the Hessian matrix is difficult to obtain, or for large dense problems. L-BFGS-B can also be used for unconstrained problems and in this case performs similarly to its predessor, algorithm L-BFGS (Harwell routine VA15). The algorithm is implemented in Fortran 77.},
   number = {4},
   urldate = {2024-08-27},
   journal = {ACM Trans. Math. Softw.},
   author = {Zhu, Ciyou and Byrd, Richard H. and Lu, Peihuang and Nocedal, Jorge},
   month = dec,
   year = {1997},
   pages = {550--560},
   file = {Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\KLGYEPSP\\Zhu et al. - 1997 - Algorithm 778 L-BFGS-B Fortran subroutines for l.pdf:application/pdf},
}

@inproceedings{novak_sensitivity_2018,
   title = {Sensitivity and {Generalization} in {Neural} {Networks}: an {Empirical} {Study}},
   url = {https://openreview.net/pdf?id=HJC2SzZCW},
   abstract = {In practice it is often found that large over-parameterized neural networks generalize better than their smaller counterparts, an observation that appears to conflict with classical notions of function complexity, which typically favor smaller models. In this work, we investigate this tension between complexity and generalization through an extensive empirical exploration of two natural metrics of complexity related to sensitivity to input perturbations. Our experiments survey thousands of models with various fully-connected architectures, optimizers, and other hyper-parameters, as well as four different image classification datasets. We find that trained neural networks are more robust to input perturbations in the vicinity of the training data manifold, as measured by the norm of the input-output Jacobian of the network, and that it correlates well with generalization. We further establish that factors associated with poor generalization \$-\$ such as full-batch training or using random labels \$-\$ correspond to lower robustness, while factors associated with good generalization \$-\$ such as data augmentation and ReLU non-linearities \$-\$ give rise to more robust functions. Finally, we demonstrate how the input-output Jacobian norm can be predictive of generalization at the level of individual test points.},
   booktitle = {The {International} {Conference} on {Learning} {Representations}},
   author = {Novak, Roman and Bahri, Yasaman and Abolafia, Dan and Pennington, Jeffrey and Sohl-dickstein, Jascha},
   year = {2018},
   file = {Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\9WBEPSAJ\\Novak et al. - 2018 - Sensitivity and Generalization in Neural Networks.pdf:application/pdf},
}

@misc{chollet_keras_2015,
   title = {Keras},
   url = {https://keras.io},
   author = {Chollet, François and {others}},
   year = {2015},
}

@misc{paszke_pytorch_2019,
   title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
   shorttitle = {{PyTorch}},
   url = {https://arxiv.org/abs/1912.01703v1},
   abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
   language = {en},
   urldate = {2024-08-15},
   journal = {arXiv.org},
   author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Köpf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
   month = dec,
   year = {2019},
   file = {Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\5ES77XWE\\Paszke et al. - 2019 - PyTorch An Imperative Style, High-Performance Dee.pdf:application/pdf},
}

@inproceedings{ansel_pytorch_2024,
   address = {New York, NY, USA},
   series = {{ASPLOS} '24},
   title = {{PyTorch} 2: {Faster} {Machine} {Learning} {Through} {Dynamic} {Python} {Bytecode} {Transformation} and {Graph} {Compilation}},
   volume = {2},
   isbn = {9798400703850},
   shorttitle = {{PyTorch} 2},
   url = {https://dl.acm.org/doi/10.1145/3620665.3640366},
   doi = {10.1145/3620665.3640366},
   abstract = {This paper introduces two extensions to the popular PyTorch machine learning framework, TorchDynamo and TorchInductor, which implement the torch.compile feature released in PyTorch 2. TorchDynamo is a Python-level just-in-time (JIT) compiler that enables graph compilation in PyTorch programs without sacrificing the flexibility of Python. It achieves this by dynamically modifying Python bytecode before execution and extracting sequences of PyTorch operations into an FX graph, which is then JIT compiled using one of many extensible backends. TorchInductor is the default compiler backend for TorchDynamo, which translates PyTorch programs into OpenAI's Triton for GPUs and C++ for CPUs. Results show that TorchDynamo is able to capture graphs more robustly than prior approaches while adding minimal overhead, and TorchInductor is able to provide a 2.27× inference and 1.41× training geometric mean speedup on an NVIDIA A100 GPU across 180+ real-world models, which outperforms six other compilers. These extensions provide a new way to apply optimizations through compilers in eager mode frameworks like PyTorch.},
   urldate = {2024-08-15},
   booktitle = {Proceedings of the 29th {ACM} {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}, {Volume} 2},
   publisher = {Association for Computing Machinery},
   author = {Ansel, Jason and Yang, Edward and He, Horace and Gimelshein, Natalia and Jain, Animesh and Voznesensky, Michael and Bao, Bin and Bell, Peter and Berard, David and Burovski, Evgeni and Chauhan, Geeta and Chourdia, Anjali and Constable, Will and Desmaison, Alban and DeVito, Zachary and Ellison, Elias and Feng, Will and Gong, Jiong and Gschwind, Michael and Hirsh, Brian and Huang, Sherlock and Kalambarkar, Kshiteej and Kirsch, Laurent and Lazos, Michael and Lezcano, Mario and Liang, Yanbo and Liang, Jason and Lu, Yinghai and Luk, C. K. and Maher, Bert and Pan, Yunjie and Puhrsch, Christian and Reso, Matthias and Saroufim, Mark and Siraichi, Marcos Yukio and Suk, Helen and Zhang, Shunting and Suo, Michael and Tillet, Phil and Zhao, Xu and Wang, Eikan and Zhou, Keren and Zou, Richard and Wang, Xiaodong and Mathews, Ajit and Wen, William and Chanan, Gregory and Wu, Peng and Chintala, Soumith},
   month = apr,
   year = {2024},
   pages = {929--947},
   file = {Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\H5942VYZ\\Ansel et al. - 2024 - PyTorch 2 Faster Machine Learning Through Dynamic.pdf:application/pdf},
}

@misc{martin_abadi_tensorflow_2015,
   title = {{TensorFlow}: {Large}-{Scale} {Machine} {Learning} on {Heterogeneous} {Systems}},
   url = {https://www.tensorflow.org/},
   author = {{Martín Abadi} and {Ashish Agarwal} and {Paul Barham} and {Eugene Brevdo} and {Zhifeng Chen} and {Craig Citro} and {Greg S. Corrado} and {Andy Davis} and {Jeffrey Dean} and {Matthieu Devin} and {Sanjay Ghemawat} and {Ian Goodfellow} and {Andrew Harp} and {Geoffrey Irving} and {Michael Isard} and Jia, Yangqing and {Rafal Jozefowicz} and {Lukasz Kaiser} and {Manjunath Kudlur} and {Josh Levenberg} and {Dandelion Mané} and {Rajat Monga} and {Sherry Moore} and {Derek Murray} and {Chris Olah} and {Mike Schuster} and {Jonathon Shlens} and {Benoit Steiner} and {Ilya Sutskever} and {Kunal Talwar} and {Paul Tucker} and {Vincent Vanhoucke} and {Vijay Vasudevan} and {Fernanda Viégas} and {Oriol Vinyals} and {Pete Warden} and {Martin Wattenberg} and {Martin Wicke} and {Yuan Yu} and {Xiaoqiang Zheng}},
   year = {2015},
   annote = {Software available from tensorflow.org},
}

@article{bezanson_julia_2017,
   title = {Julia: {A} {Fresh} {Approach} to {Numerical} {Computing}},
   volume = {59},
   issn = {0036-1445},
   shorttitle = {Julia},
   url = {https://epubs.siam.org/doi/abs/10.1137/141000671},
   doi = {10.1137/141000671},
   abstract = {Bridging cultures that have often been distant, Julia combines expertise from the diverse fields of computer science and computational science to create a new approach to numerical computing. Julia is designed to be easy and fast and questions notions generally held to be “laws of nature" by practitioners of numerical computing: {\textbackslash}beginlist {\textbackslash}item High-level dynamic programs have to be slow. {\textbackslash}item One must prototype in one language and then rewrite in another language for speed or deployment. {\textbackslash}item There are parts of a system appropriate for the programmer, and other parts that are best left untouched as they have been built by the experts. {\textbackslash}endlist We introduce the Julia programming language and its design---a dance between specialization and abstraction. Specialization allows for custom treatment. Multiple dispatch, a technique from computer science, picks the right algorithm for the right circumstance. Abstraction, which is what good computation is really about, recognizes what remains the same after differences are stripped away. Abstractions in mathematics are captured as code through another technique from computer science, generic programming. Julia shows that one can achieve machine performance without sacrificing human convenience.},
   number = {1},
   urldate = {2022-10-31},
   journal = {SIAM Review},
   author = {Bezanson, Jeff and Edelman, Alan and Karpinski, Stefan and Shah, Viral B.},
   month = jan,
   year = {2017},
   note = {Publisher: Society for Industrial and Applied Mathematics},
   keywords = {65Y05, 68N15, 97P40, Julia, numerical, parallel, scientific computing},
   pages = {65--98},
   file = {Full Text:C\:\\Users\\Nicholas\\Zotero\\storage\\CXGJB3RB\\Bezanson et al. - 2017 - Julia A Fresh Approach to Numerical Computing.pdf:application/pdf},
}


@article{innes_fashionable_2018,
   title = {Fashionable {Modelling} with {Flux}},
   volume = {abs/1811.01457},
   url = {https://arxiv.org/abs/1811.01457},
   journal = {CoRR},
   author = {Innes, Michael and Saba, Elliot and Fischer, Keno and Gandhi, Dhairya and Rudilosso, Marco Concetto and Joy, Neethu Mariya and Karmali, Tejan and Pal, Avik and Shah, Viral},
   year = {2018},
   note = {\_eprint: 1811.01457},
}

@article{innes_flux_2018,
   title = {Flux: {Elegant} {Machine} {Learning} with {Julia}},
   doi = {10.21105/joss.00602},
   journal = {Journal of Open Source Software},
   author = {Innes, Mike},
   year = {2018},
}

@misc{jutho_juthotensorkitjl_2024,
   title = {Jutho/{TensorKit}.jl},
   copyright = {MIT},
   url = {https://github.com/Jutho/TensorKit.jl},
   abstract = {A Julia package for large-scale tensor computations, with a hint of category theory},
   urldate = {2024-08-15},
   author = {Jutho},
   month = aug,
   year = {2024},
   note = {original-date: 2017-10-03T20:31:22Z},
}

@misc{peter_under-peteromeinsumjl_2024,
   title = {under-{Peter}/{OMEinsum}.jl},
   copyright = {MIT},
   url = {https://github.com/under-Peter/OMEinsum.jl},
   abstract = {One More Einsum for Julia! With runtime order-specification and high-level adjoints for AD},
   urldate = {2024-08-16},
   author = {Peter, Andreas},
   month = aug,
   year = {2024},
   note = {original-date: 2019-05-11T15:56:47Z},
   keywords = {automatic-differentiation, contraction, einsum},
}

@misc{lukas_devos_lukasdevosugentbe_tensoroperationsjl_2023,
   title = {{TensorOperations}.jl},
   url = {https://github.com/Jutho/TensorOperations.jl},
   author = {Lukas Devos {\textless}lukas.devos@ugent.be{\textgreater}, Maarten Van Damme {\textless}maartenvd1994@gmail.com{\textgreater}, Jutho Haegeman {\textless}jutho.haegeman@ugent.be{\textgreater} and {contributors}},
   month = oct,
   year = {2023},
   doi = {10.5281/zenodo.3245496},
}

@misc{abbott_mcabbotttulliojl_2023,
   title = {mcabbott/{Tullio}.jl: v0.3.7},
   url = {https://doi.org/10.5281/zenodo.10035615},
   publisher = {Zenodo},
   author = {Abbott, Michael and Aluthge, Dilum and {N3N5} and Puri, Vedant and Elrod, Chris and Schaub, Simeon and Lucibello, Carlo and Bhattacharya, Jishnu and Chen, Johnny and Carlsson, Kristoffer and Gelbrecht, Maximilian},
   month = oct,
   year = {2023},
   doi = {10.5281/zenodo.10035615},
}

@misc{bader_tensor_2023,
   title = {Tensor {Toolbox} for {MATLAB}},
   url = {www.tensortoolbox.org},
   author = {Bader, Brett W. and Kolda, Tamara G.},
   month = sep,
   year = {2023},
}

@article{kossaifi_tensorly_2019,
   title = {{TensorLy}: {Tensor} {Learning} in {Python}},
   volume = {20},
   issn = {1533-7928},
   shorttitle = {{TensorLy}},
   url = {http://jmlr.org/papers/v20/18-277.html},
   abstract = {Tensors are higher-order extensions of matrices. While matrix methods form the cornerstone of traditional machine learning and data analysis, tensor methods have been gaining increasing traction. However, software support for tensor operations is not on the same footing. In order to bridge this gap, we have developed TensorLy, a Python library that provides a high-level API for tensor methods and deep tensorized neural networks. TensorLy aims to follow the same standards adopted by the main projects of the Python scientific community, and to seamlessly integrate with them. Its BSD license makes it suitable for both academic and commercial applications. TensorLy's backend system allows users to perform computations with several libraries such as NumPy or PyTorch to name but a few. They can be scaled on multiple CPU or GPU machines. In addition, using the deep-learning frameworks as backend allows to easily design and train deep tensorized neural networks. TensorLy is available at https://github.com/tensorly/tensorly},
   number = {26},
   urldate = {2024-08-16},
   journal = {Journal of Machine Learning Research},
   author = {Kossaifi, Jean and Panagakis, Yannis and Anandkumar, Anima and Pantic, Maja},
   year = {2019},
   pages = {1--6},
   file = {Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\EAFGN8VN\\Kossaifi et al. - 2019 - TensorLy Tensor Learning in Python.pdf:application/pdf;Source Code:C\:\\Users\\Nicholas\\Zotero\\storage\\4S28XISR\\tensorly.html:text/html},
}

@article{li_rtensor_2018,
   title = {{rTensor}: {An} {R} {Package} for {Multidimensional} {Array} ({Tensor}) {Unfolding}, {Multiplication}, and {Decomposition}},
   volume = {87},
   copyright = {Copyright (c) 2018 James Li, Jacob Bien, Martin T. Wells},
   issn = {1548-7660},
   shorttitle = {{rTensor}},
   url = {https://doi.org/10.18637/jss.v087.i10},
   doi = {10.18637/jss.v087.i10},
   abstract = {rTensor is an R package designed to provide a common set of operations and decompositions for multidimensional arrays (tensors). We provide an S4 class that wraps around the base 'array' class and overloads familiar operations to users of 'array', and we provide additional functionality for tensor operations that are becoming more relevant in recent literature. We also provide a general unfolding operation, for which the k-mode unfolding and the matrix vectorization are special cases of. Finally, package rTensor implements common tensor decompositions such as canonical polyadic decomposition, Tucker decomposition, multilinear principal component analysis, t-singular value decomposition, as well as related matrix-based algorithms such as generalized low rank approximation of matrices and popular value decomposition.},
   language = {en},
   urldate = {2024-08-16},
   journal = {Journal of Statistical Software},
   author = {Li, James and Bien, Jacob and Wells, Martin T.},
   month = nov,
   year = {2018},
   keywords = {CANDECOMP/PARAFAC, generalized low rank approximation of matrices, multidimensional arrays, multilinear principal components analysis, population valued decomposition, S4, tensor, tensor singular value decomposition, Tucker decomposition},
   pages = {1--31},
   file = {Full Text:C\:\\Users\\Nicholas\\Zotero\\storage\\LKPN4CRB\\Li et al. - 2018 - rTensor An R Package for Multidimensional Array (.pdf:application/pdf},
}


@misc{wu_yunjhongwutensordecompositionsjl_2024,
   title = {yunjhongwu/{TensorDecompositions}.jl},
   url = {https://github.com/yunjhongwu/TensorDecompositions.jl},
   abstract = {A Julia implementation of tensor decomposition algorithms},
   urldate = {2024-08-16},
   author = {Wu, Yun-Jhong},
   month = feb,
   year = {2024},
   note = {original-date: 2015-08-01T22:34:32Z},
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Signal Processing
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


@article{mallat_matching_1993,
   title = {Matching pursuits with time-frequency dictionaries},
   volume = {41},
   issn = {1941-0476},
   url = {https://ieeexplore.ieee.org/abstract/document/258082},
   doi = {10.1109/78.258082},
   abstract = {The authors introduce an algorithm, called matching pursuit, that decomposes any signal into a linear expansion of waveforms that are selected from a redundant dictionary of functions. These waveforms are chosen in order to best match the signal structures. Matching pursuits are general procedures to compute adaptive signal representations. With a dictionary of Gabor functions a matching pursuit defines an adaptive time-frequency transform. They derive a signal energy distribution in the time-frequency plane, which does not include interference terms, unlike Wigner and Cohen class distributions. A matching pursuit isolates the signal structures that are coherent with respect to a given dictionary. An application to pattern extraction from noisy signals is described. They compare a matching pursuit decomposition with a signal expansion over an optimized wavepacket orthonormal basis, selected with the algorithm of Coifman and Wickerhauser see (IEEE Trans. Informat. Theory, vol. 38, Mar. 1992).{\textless}{\textgreater}},
   number = {12},
   urldate = {2024-10-31},
   journal = {IEEE Transactions on Signal Processing},
   author = {Mallat, S.G. and Zhang, Zhifeng},
   month = dec,
   year = {1993},
   note = {Conference Name: IEEE Transactions on Signal Processing},
   keywords = {Dictionaries, Fourier transforms, Interference, Matching pursuit algorithms, Natural languages, Pursuit algorithms, Signal processing algorithms, Signal representations, Time frequency analysis, Vocabulary},
   pages = {3397--3415},
   file = {Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\Z48X2WQH\\Mallat and Zhang - 1993 - Matching pursuits with time-frequency dictionaries.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Nicholas\\Zotero\\storage\\F3ZCAR5L\\258082.html:text/html},
}


@unpublished{wescott_sampling_2018,
   title = {Sampling: {What} {Nyquist} {Didn}'t {Say}, and {What} to {Do} {About} {It}},
   url = {https://www.wescottdesign.com/articles.html},
   abstract = {The Nyquist Theorem is useful, but often misused when engineers establish sampling rates or design anti-aliasing filters. This article explains how sampling affects a signal, and how to use this information to design a sampling system with known performance.},
   language = {en},
   urldate = {2024-09-04},
   author = {Wescott, Tim},
   month = aug,
   year = {2018},
   file = {Wescott and Services - Sampling What Nyquist Didn't Say, and What to Do .pdf:C\:\\Users\\Nicholas\\Zotero\\storage\\U35PKK9P\\Wescott and Services - Sampling What Nyquist Didn't Say, and What to Do .pdf:application/pdf},
}

@article{mishali_theory_2010,
   title = {From {Theory} to {Practice}: {Sub}-{Nyquist} {Sampling} of {Sparse} {Wideband} {Analog} {Signals}},
   volume = {4},
   issn = {1941-0484},
   shorttitle = {From {Theory} to {Practice}},
   url = {https://ieeexplore.ieee.org/abstract/document/5419072},
   doi = {10.1109/JSTSP.2010.2042414},
   abstract = {Conventional sub-Nyquist sampling methods for analog signals exploit prior information about the spectral support. In this paper, we consider the challenging problem of blind sub-Nyquist sampling of multiband signals, whose unknown frequency support occupies only a small portion of a wide spectrum. Our primary design goals are efficient hardware implementation and low computational load on the supporting digital processing. We propose a system, named the modulated wideband converter, which first multiplies the analog signal by a bank of periodic waveforms. The product is then low-pass filtered and sampled uniformly at a low rate, which is orders of magnitude smaller than Nyquist. Perfect recovery from the proposed samples is achieved under certain necessary and sufficient conditions. We also develop a digital architecture, which allows either reconstruction of the analog input, or processing of any band of interest at a low rate, that is, without interpolating to the high Nyquist rate. Numerical simulations demonstrate many engineering aspects: robustness to noise and mismodeling, potential hardware simplifications, real-time performance for signals with time-varying support and stability to quantization effects. We compare our system with two previous approaches: periodic nonuniform sampling, which is bandwidth limited by existing hardware devices, and the random demodulator, which is restricted to discrete multitone signals and has a high computational load. In the broader context of Nyquist sampling, our scheme has the potential to break through the bandwidth barrier of state-of-the-art analog conversion technologies such as interleaved converters.},
   number = {2},
   urldate = {2024-09-04},
   journal = {IEEE Journal of Selected Topics in Signal Processing},
   author = {Mishali, Moshe and Eldar, Yonina C.},
   month = apr,
   year = {2010},
   note = {Conference Name: IEEE Journal of Selected Topics in Signal Processing},
   keywords = {Analog-to-digital conversion (ADC), Bandwidth, compressive sampling (CS), Frequency, Hardware, infinite measurement vectors (IMV), Low pass filters, multiband sampling, Numerical simulation, Robust stability, Sampling methods, Signal sampling, spectrum-blind reconstruction, sub-Nyquist sampling, Sufficient conditions, Wideband},
   pages = {375--391},
   file = {IEEE Xplore Abstract Record:C\:\\Users\\Nicholas\\Zotero\\storage\\6JFGKB6H\\5419072.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\QQMWTEHF\\Mishali and Eldar - 2010 - From Theory to Practice Sub-Nyquist Sampling of S.pdf:application/pdf},
}

@misc{eldar_compressed_2020,
   address = {Online},
   title = {From {Compressed} {Sensing} to {Deep} {Learning}: {Tasks}, {Structures}, and {Models}},
   url = {https://sites.google.com/view/oneworldimagine/1st-season-aprjuly-2020},
   abstract = {The famous Shannon-Nyquist theorem has become a landmark in the development of digital signal and image processing. However, in many modern applications, the signal bandwidths have increased tremendously, while the acquisition capabilities have not scaled sufficiently fast. Consequently, conversion to digital has become a serious bottleneck.  Furthermore, the resulting high rate digital data requires storage, communication and processing at very high rates which is computationally expensive and requires large amounts of power.  In the context of medical imaging sampling at high rates often translates to high radiation dosages, increased scanning times, bulky medical devices, and limited resolution.

In this talk, we present a framework for sampling and processing a wide class of wideband analog signals at rates far below Nyquist by exploiting signal structure and the processing task and show several demos of real-time sub-Nyquist prototypes. We consider applications of these ideas to a variety of problems in imaging including fast and quantitative MRI, wireless ultrasound, fast Doppler imaging, and correlation based super-resolution in microscopy and ultrasound which combines high spatial resolution with short integration time. We then show how the ideas of exploiting the task, structure and model can be used to develop interpretable model-based deep learning methods that can adapt to existing structure and are trained from small amounts of data. These networks achieve a more favorable trade-off between increase in parameters and data and improvement in performance while remaining interpretable.},
   urldate = {2024-09-04},
   author = {Eldar, Yonina C.},
   month = may,
   year = {2020},
   file = {Eldar - 2020 - From Compressed Sensing to Deep Learning Tasks, S.pdf:C\:\\Users\\Nicholas\\Zotero\\storage\\KCMDJC7L\\Eldar - 2020 - From Compressed Sensing to Deep Learning Tasks, S.pdf:application/pdf},
}

@article{rabiner_theory_1978,
   title = {Theory and {Application} of {Digital} {Signal} {Processing}},
   volume = {8},
   issn = {2168-2909},
   doi = {10.1109/TSMC.1978.4309918},
   number = {2},
   journal = {IEEE Transactions on Systems, Man, and Cybernetics},
   author = {Rabiner, L. R. and Gold, B. and Yuen, C. K.},
   month = feb,
   year = {1978},
   keywords = {Books, Digital filters, Digital signal processing, Discrete Fourier transforms, Fast Fourier transforms, Filtering, Fourier series, Gold, Hardware, Signal processing algorithms},
   pages = {146--146},
}

@book{nebeker_ieee_1998,
   address = {New Brunswick, NJ},
   title = {The {IEEE} {Signal} {Processing} {Society}: {Fifty} {Years} of {Service}, 1948 to 1998},
   isbn = {978-0-7803-9911-2 978-0-7803-9910-5},
   shorttitle = {The {IEEE} signal processing society},
   abstract = {For its 50th anniversary in 1998, the IEEE Signal processing Society worked with the IEEE History Center to prepare a monograph outlining the history of the Society. This is the introduction to the publication, and a link to the full text appears at the bottom of this page.

The IEEE Signal Processing Society is now 50 years old. When it began in 1948 as the Professional Group on Audio of the Institute of Radio Engineers, there was no discipline of signal processing. Over the next decades, as the IRE Professional Group on Audio evolved into the IEEE Signal Processing Society, it helped create the discipline of signal processing, which today is a vital and rapidly growing branch of engineering. The Society history, then, is an important part of the history of signal processing. It is valuable also for members of today's Signal Processing Society. It provides many examples of dedicated and unselfish service to the profession. It describes difficulties that Society leaders faced and overcame. It shows how a professional organization adapted to rapid technological change, to the emergence of quite new technologies, and to changing social, political, and economic conditions. These are worthwhile things to know for today's and tomorrow's leaders of the Society, since the next 50 years are hardly likely to be more settled, and dedication, resourcefulness, and adaptability will certainly be required.

This monograph tells the story of the Signal Processing Society. A companion monograph tells the story of signal-processing technologies over the same 50-year period. Neither could have been written without the help of a great many people giving interviews, providing documentary materials, and reviewing chapter drafts. The members of the Signal Processing Society's Ad-Hoc Committee for the History Project should be thanked by name: David C. Munson, Jr. (Chair), Dan E. Dudgeon, Tariq S. Durrani, Don E. Johnson, and H. Joel Trussell. Most of the surviving Society Presidents contributed: besides Ad-Hoc Committee members Munson, Durrani, and Johnson, this group includes John Ackenhusen, Oliver Angevine, Leo Beranek, John Bouyoucos, Donald Brinkerhoff, Ronald Crochiere, Thomas Crystal, Rex Dixon, Delores Etter, James Flanagan, Cyril Harris, Howard Helms, William Ihde, Reg Kaenel, William Lang, Daniel Martin, Lawrence Rabiner, Charles Rader, Ronald Schafer, and Frank Slaymaker. Of these Beranek, Flanagan, Lang, Rabiner, and Rader contributed oral-history interviews; others who did were Maurice Bellanger, James Cooley, Alfred Fettweis, Ben Gold, Thomas Huang, Fumitada Itakura, Thomas Kailath, James Kaiser, Bede Liu, Sanjit Mitra, Hans Georg Musmann, Alan Oppenheim, Enders Robinson, Manfred Schroeder, Hans Wilhelm Schuessler, Stan White, and Bernard Widrow. A special thanks to Beranek, who made available his unpublished manuscript "History of the beginnings of the Professional Group on Audio", and to Kaiser, who helped to answer what must have seemed to him as countless questions. The professional staff of the Society, headed by Mercy Kowalczyk, gave invaluable assistance. Many others gave informal interviews, answered questions, offered advice, and helped in other ways. I would like to acknowledge also the special assistance given by Michael Geselowitz, Andrew Goldstein, Daniel Katz, David Morton, and Sheila Plotnick, all at the IEEE History Center.

The story is told chronologically. The first chapter tells how the Professional Group on Audio came to be founded, an important story not only for the Society but for all of IEEE in that it was the first of the professional groups of the IRE. Since the IRE structure of professional groups carried over to the IEEE structure of technical societies (following the merger of the IRE and the American Institute of Electrical Engineers in 1963 to form the IEEE), the Signal Processing Society has a claim to the title of the oldest of the IEEE Societies. Each of the other chapters covers one decade, naming Society Presidents, discussing Society publications, conferences, workshops, and other activities, and describing changes in scope, structure, and governance of the organization. An appendix contains lists of Society Presidents, award winners, and editors.},
   language = {eng},
   publisher = {IEEE History Center},
   author = {Nebeker, Frederik and {IEEE Signal Processing Society}},
   year = {1998},
   annote = {Literaturangaben},
   file = {history.pdf:C\:\\Users\\Nicholas\\Zotero\\storage\\WPEWUVSZ\\history.pdf:application/pdf},
}

@article{tosic_dictionary_2011,
   title = {Dictionary {Learning}},
   volume = {28},
   issn = {1558-0792},
   url = {https://ieeexplore.ieee.org/document/5714407},
   doi = {10.1109/MSP.2010.939537},
   abstract = {We describe methods for learning dictionaries that are appropriate for the representation of given classes of signals and multisensor data. We further show that dimensionality reduction based on dictionary representation can be extended to address specific tasks such as data analy sis or classification when the learning includes a class separability criteria in the objective function. The benefits of dictionary learning clearly show that a proper understanding of causes underlying the sensed world is key to task-specific representation of relevant information in high-dimensional data sets.},
   number = {2},
   urldate = {2024-08-22},
   journal = {IEEE Signal Processing Magazine},
   author = {Tošić, Ivana and Frossard, Pascal},
   month = mar,
   year = {2011},
   note = {Conference Name: IEEE Signal Processing Magazine},
   keywords = {Approximation algorithms, Approximation methods, Dictionaries, Encoding, Learning systems, Sensors, Signal processing algorithms},
   pages = {27--38},
   file = {IEEE Xplore Abstract Record:C\:\\Users\\Nicholas\\Zotero\\storage\\6WK7Y3T6\\5714407.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\Y9UNRW3F\\Tošić and Frossard - 2011 - Dictionary Learning.pdf:application/pdf},
}

@article{jacques_panorama_2011,
   series = {Advances in {Multirate} {Filter} {Bank} {Structures} and {Multiscale} {Representations}},
   title = {A panorama on multiscale geometric representations, intertwining spatial, directional and frequency selectivity},
   volume = {91},
   issn = {0165-1684},
   url = {https://www.sciencedirect.com/science/article/pii/S0165168411001356},
   doi = {10.1016/j.sigpro.2011.04.025},
   abstract = {The richness of natural images makes the quest for optimal representations in image processing and computer vision challenging. The latter observation has not prevented the design of image representations, which trade off between efficiency and complexity, while achieving accurate rendering of smooth regions as well as reproducing faithful contours and textures. The most recent ones, proposed in the past decade, share a hybrid heritage highlighting the multiscale and oriented nature of edges and patterns in images. This paper presents a panorama of the aforementioned literature on decompositions in multiscale, multi-orientation bases or dictionaries. They typically exhibit redundancy to improve sparsity in the transformed domain and sometimes its invariance with respect to simple geometric deformations (translation, rotation). Oriented multiscale dictionaries extend traditional wavelet processing and may offer rotation invariance. Highly redundant dictionaries require specific algorithms to simplify the search for an efficient (sparse) representation. We also discuss the extension of multiscale geometric decompositions to non-Euclidean domains such as the sphere or arbitrary meshed surfaces. The etymology of panorama suggests an overview, based on a choice of partially overlapping “pictures”. We hope that this paper will contribute to the appreciation and apprehension of a stream of current research directions in image understanding.},
   number = {12},
   urldate = {2024-08-15},
   journal = {Signal Processing},
   author = {Jacques, Laurent and Duval, Laurent and Chaux, Caroline and Peyré, Gabriel},
   month = dec,
   year = {2011},
   keywords = {Atoms, Bases, Edges, Frames, Geometric representations, Haar wavelet, Image processing, Multiscale, Non-Euclidean wavelets, Oriented decompositions, Redundancy, Review, Scale-space, Sparsity, Textures, Wavelets},
   pages = {2699--2730},
   file = {Jacques et al. - 2011 - A panorama on multiscale geometric representations.pdf:C\:\\Users\\Nicholas\\Zotero\\storage\\YGDJZV5Q\\Jacques et al. - 2011 - A panorama on multiscale geometric representations.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Nicholas\\Zotero\\storage\\AG5QDB8L\\S0165168411001356.html:text/html},
}

@article{cooley_algorithm_1965,
   title = {An {Algorithm} for the {Machine} {Calculation} of {Complex} {Fourier} {Series}},
   volume = {19},
   issn = {0025-5718},
   url = {https://www.jstor.org/stable/2003354},
   doi = {10.2307/2003354},
   number = {90},
   urldate = {2024-08-15},
   journal = {Mathematics of Computation},
   author = {Cooley, James W. and Tukey, John W.},
   year = {1965},
   note = {Publisher: American Mathematical Society},
   pages = {297--301},
   file = {JSTOR Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\F6PSMBK5\\Cooley and Tukey - 1965 - An Algorithm for the Machine Calculation of Comple.pdf:application/pdf},
}

@article{yilmaz_blind_2004,
   title = {Blind separation of speech mixtures via time-frequency masking},
   volume = {52},
   issn = {1941-0476},
   url = {https://ieeexplore.ieee.org/abstract/document/1306640?casa_token=LUaJpqvD0kwAAAAA:33pc7PxjPvdA5WsetIRlgou__XG-WzWeBerTaaAgf7VNLK0Vr2Ia1RAGIz3QOvmijl24oCSUoQ},
   doi = {10.1109/TSP.2004.828896},
   abstract = {Binary time-frequency masks are powerful tools for the separation of sources from a single mixture. Perfect demixing via binary time-frequency masks is possible provided the time-frequency representations of the sources do not overlap: a condition we call W-disjoint orthogonality. We introduce here the concept of approximate W-disjoint orthogonality and present experimental results demonstrating the level of approximate W-disjoint orthogonality of speech in mixtures of various orders. The results demonstrate that there exist ideal binary time-frequency masks that can separate several speech signals from one mixture. While determining these masks blindly from just one mixture is an open problem, we show that we can approximate the ideal masks in the case where two anechoic mixtures are provided. Motivated by the maximum likelihood mixing parameter estimators, we define a power weighted two-dimensional (2-D) histogram constructed from the ratio of the time-frequency representations of the mixtures that is shown to have one peak for each source with peak location corresponding to the relative attenuation and delay mixing parameters. The histogram is used to create time-frequency masks that partition one of the mixtures into the original sources. Experimental results on speech mixtures verify the technique. Example demixing results can be found online at http://alum.mit.edu/www/rickard/bss.html.},
   number = {7},
   urldate = {2024-08-14},
   journal = {IEEE Transactions on Signal Processing},
   author = {Yilmaz, O. and Rickard, S.},
   month = jul,
   year = {2004},
   note = {Conference Name: IEEE Transactions on Signal Processing},
   keywords = {Delay estimation, Time frequency analysis, Attenuation, Fourier transforms, Histograms, Lattices, Maximum likelihood estimation, Parameter estimation, Speech coding, Two dimensional displays},
   pages = {1830--1847},
   file = {IEEE Xplore Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\PD3IR72L\\Yilmaz and Rickard - 2004 - Blind separation of speech mixtures via time-frequ.pdf:application/pdf},
}

@article{sidiropoulos_tensor_2017,
	title = {Tensor {Decomposition} for {Signal} {Processing} and {Machine} {Learning}},
	volume = {65},
	issn = {1941-0476},
	url = {https://ieeexplore.ieee.org/abstract/document/7891546},
	doi = {10.1109/TSP.2017.2690524},
	abstract = {Tensors or multiway arrays are functions of three or more indices (i, j, k, . . . )-similar to matrices (two-way arrays), which are functions of two indices (r, c) for (row, column). Tensors have a rich history, stretching over almost a century, and touching upon numerous disciplines; but they have only recently become ubiquitous in signal and data analytics at the confluence of signal processing, statistics, data mining, and machine learning. This overview article aims to provide a good starting point for researchers and practitioners interested in learning about and working with tensors. As such, it focuses on fundamentals and motivation (using various application examples), aiming to strike an appropriate balance of breadth and depth that will enable someone having taken first graduate courses in matrix algebra and probability to get started doing research and/or developing tensor algorithms and software. Some background in applied optimization is useful but not strictly required. The material covered includes tensor rank and rank decomposition; basic tensor factorization models and their relationships and properties (including fairly good coverage of identifiability); broad coverage of algorithms ranging from alternating optimization to stochastic gradient; statistical performance analysis; and applications ranging from source separation to collaborative filtering, mixture and topic modeling, classification, and multilinear subspace learning.},
	number = {13},
	urldate = {2024-08-02},
	journal = {IEEE Transactions on Signal Processing},
	author = {Sidiropoulos, Nicholas D. and De Lathauwer, Lieven and Fu, Xiao and Huang, Kejun and Papalexakis, Evangelos E. and Faloutsos, Christos},
	month = jul,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Signal Processing},
	keywords = {Signal processing, source separation, Signal processing algorithms, Optimization, Tensile stress, Matrix decomposition, tensor factorization, classification, alternating direction method of multipliers, alternating optimization, canonical polyadic decomposition (CPD), collaborative filtering, communications, Cramér–Rao bound, Gauss–Newton, gradient descent, harmonic retrieval, higher-order singular value decomposition (HOSVD), mixture modeling, multilinear singular value decomposition (MLSVD), NP-hard problems, parallel factor analysis (PARAFAC), rank, speech separation, stochastic gradient, subspace learning, Tensor decomposition, topic modeling, Tucker model, Tutorials, uniqueness},
	pages = {3551--3582},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Nicholas\\Zotero\\storage\\IC7TYMN7\\7891546.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\N3BWIS9M\\Sidiropoulos et al. - 2017 - Tensor Decomposition for Signal Processing and Mac.pdf:application/pdf},
}

@article{sejdic_fractional_2011,
	series = {Fourier {Related} {Transforms} for {Non}-{Stationary} {Signals}},
	title = {Fractional {Fourier} transform as a signal processing tool: {An} overview of recent developments},
	volume = {91},
	issn = {0165-1684},
	shorttitle = {Fractional {Fourier} transform as a signal processing tool},
	url = {https://www.sciencedirect.com/science/article/pii/S0165168410003956},
	doi = {10.1016/j.sigpro.2010.10.008},
	abstract = {Fractional Fourier transform (FRFT) is a generalization of the Fourier transform, rediscovered many times over the past 100 years. In this paper, we provide an overview of recent contributions pertaining to the FRFT. Specifically, the paper is geared toward signal processing practitioners by emphasizing the practical digital realizations and applications of the FRFT. It discusses three major topics. First, the manuscripts relates the FRFT to other mathematical transforms. Second, it discusses various approaches for practical realizations of the FRFT. Third, we overview the practical applications of the FRFT. From these discussions, we can clearly state that the FRFT is closely related to other mathematical transforms, such as time–frequency and linear canonical transforms. Nevertheless, we still feel that major contributions are expected in the field of the digital realizations and its applications, especially, since many digital realizations of the FRFT still lack properties of the continuous FRFT. Overall, the FRFT is a valuable signal processing tool. Its practical applications are expected to grow significantly in years to come, given that the FRFT offers many advantages over the traditional Fourier analysis.},
	number = {6},
	urldate = {2024-08-02},
	journal = {Signal Processing},
	author = {Sejdić, Ervin and Djurović, Igor and Stanković, LJubiša},
	month = jun,
	year = {2011},
	keywords = {Applications, Discrete realizations, Fractional Fourier transform, Time–frequency},
	pages = {1351--1369},
	file = {ScienceDirect Snapshot:C\:\\Users\\Nicholas\\Zotero\\storage\\VYGF9HJR\\S0165168410003956.html:text/html;Sejdić et al. - 2011 - Fractional Fourier transform as a signal processin.pdf:C\:\\Users\\Nicholas\\Zotero\\storage\\EKNXPWJS\\Sejdić et al. - 2011 - Fractional Fourier transform as a signal processin.pdf:application/pdf},
}

@article{hashemi_generalization_2023,
	title = {Generalization bounds for sparse random feature expansions},
	volume = {62},
	issn = {1063-5203},
	url = {https://www.sciencedirect.com/science/article/pii/S1063520322000653},
	doi = {10.1016/j.acha.2022.08.003},
	abstract = {Random feature methods have been successful in various machine learning tasks, are easy to compute, and come with theoretical accuracy bounds. They serve as an alternative approach to standard neural networks since they can represent similar function spaces without a costly training phase. However, for accuracy, random feature methods require more measurements than trainable parameters, limiting their use for data-scarce applications. We introduce the sparse random feature expansion to obtain parsimonious random feature models. We leverage ideas from compressive sensing to generate random feature expansions with theoretical guarantees even in the data-scarce setting. We provide generalization bounds for functions in a certain class depending on the number of samples and the distribution of features. By introducing sparse features, i.e. features with random sparse weights, we provide improved bounds for low order functions. We show that our method outperforms shallow networks in several scientific machine learning tasks.},
	urldate = {2024-08-02},
	journal = {Applied and Computational Harmonic Analysis},
	author = {Hashemi, Abolfazl and Schaeffer, Hayden and Shi, Robert and Topcu, Ufuk and Tran, Giang and Ward, Rachel},
	month = jan,
	year = {2023},
	keywords = {Sparse optimization, Compressive sensing, Generalization error, Random features},
	pages = {310--330},
	file = {ScienceDirect Snapshot:C\:\\Users\\Nicholas\\Zotero\\storage\\KPN294X2\\S1063520322000653.html:text/html;Submitted Version:C\:\\Users\\Nicholas\\Zotero\\storage\\KC9UXQ2X\\Hashemi et al. - 2023 - Generalization bounds for sparse random feature ex.pdf:application/pdf},
}

@inproceedings{rahimi_uniform_2008,
	title = {Uniform approximation of functions with random bases},
	url = {https://ieeexplore.ieee.org/abstract/document/4797607?casa_token=KOR9exG_cQMAAAAA:01STEFTnHY6H-wVsAPZlLNLEe73QcMDvVqfouhQTnx0o2ldHDMm0iqEe4r7xhHOkqT2yaCcKQA},
	doi = {10.1109/ALLERTON.2008.4797607},
	abstract = {Random networks of nonlinear functions have a long history of empirical success in function fitting but few theoretical guarantees. In this paper, using techniques from probability on Banach Spaces, we analyze a specific architecture of random nonlinearities, provide Linfin and L2 error bounds for approximating functions in Reproducing Kernel Hilbert Spaces, and discuss scenarios when these expansions are dense in the continuous functions. We discuss connections between these random nonlinear networks and popular machine learning algorithms and show experimentally that these networks provide competitive performance at far lower computational cost on large-scale pattern recognition tasks.},
	urldate = {2024-08-02},
	booktitle = {2008 46th {Annual} {Allerton} {Conference} on {Communication}, {Control}, and {Computing}},
	author = {Rahimi, Ali and Recht, Benjamin},
	month = sep,
	year = {2008},
	keywords = {Machine learning, Pattern recognition, Matching pursuit algorithms, Computer architecture, Greedy algorithms, Hilbert space, Kernel, Large-scale systems, Machine learning algorithms, Support vector machines},
	pages = {555--561},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Nicholas\\Zotero\\storage\\Z4EXBF39\\4797607.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\SZHWH4CD\\Rahimi and Recht - 2008 - Uniform approximation of functions with random bas.pdf:application/pdf},
}

@inproceedings{ester_density-based_1996,
   address = {Portland, Oregon},
   series = {{KDD}'96},
   title = {A density-based algorithm for discovering clusters in large spatial databases with noise},
   abstract = {Clustering algorithms are attractive for the task of class identification in spatial databases. However, the application to large spatial databases rises the following requirements for clustering algorithms: minimal requirements of domain knowledge to determine the input parameters, discovery of clusters with arbitrary shape and good efficiency on large databases. The well-known clustering algorithms offer no solution to the combination of these requirements. In this paper, we present the new clustering algorithm DBSCAN relying on a density-based notion of clusters which is designed to discover clusters of arbitrary shape. DBSCAN requires only one input parameter and supports the user in determining an appropriate value for it. We performed an experimental evaluation of the effectiveness and efficiency of DBSCAN using synthetic data and real data of the SEQUOIA 2000 benchmark. The results of our experiments demonstrate that (1) DBSCAN is significantly more effective in discovering clusters of arbitrary shape than the well-known algorithm CLAR-ANS, and that (2) DBSCAN outperforms CLARANS by a factor of more than 100 in terms of efficiency.},
   urldate = {2024-08-09},
   booktitle = {Proceedings of the {Second} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
   publisher = {AAAI Press},
   author = {Ester, Martin and Kriegel, Hans-Peter and Sander, Jörg and Xu, Xiaowei},
   month = aug,
   year = {1996},
   pages = {226--231},
   file = {Ester et al. - 1996 - A density-based algorithm for discovering clusters.pdf:C\:\\Users\\Nicholas\\Zotero\\storage\\CWAMG2RS\\Ester et al. - 1996 - A density-based algorithm for discovering clusters.pdf:application/pdf},
}

@book{devito_harmonic_2007,
   address = {Sudbury, MA},
   title = {Harmonic {Analysis}: {A} {Gentle} {Introduction}},
   isbn = {0-7637-3893-X},
   publisher = {Jones and Barlett Publishers},
   author = {DeVito, Carl L.},
   year = {2007},
}

@article{daubechies_synchrosqueezed_2011,
   title = {Synchrosqueezed wavelet transforms: {An} empirical mode decomposition-like tool},
   volume = {30},
   issn = {1063-5203},
   shorttitle = {Synchrosqueezed wavelet transforms},
   url = {https://www.sciencedirect.com/science/article/pii/S1063520310001016},
   doi = {10.1016/j.acha.2010.08.002},
   abstract = {The EMD algorithm is a technique that aims to decompose into their building blocks functions that are the superposition of a (reasonably) small number of components, well separated in the time–frequency plane, each of which can be viewed as approximately harmonic locally, with slowly varying amplitudes and frequencies. The EMD has already shown its usefulness in a wide range of applications including meteorology, structural stability analysis, medical studies. On the other hand, the EMD algorithm contains heuristic and ad hoc elements that make it hard to analyze mathematically. In this paper we describe a method that captures the flavor and philosophy of the EMD approach, albeit using a different approach in constructing the components. The proposed method is a combination of wavelet analysis and reallocation method. We introduce a precise mathematical definition for a class of functions that can be viewed as a superposition of a reasonably small number of approximately harmonic components, and we prove that our method does indeed succeed in decomposing arbitrary functions in this class. We provide several examples, for simulated as well as real data.},
   number = {2},
   urldate = {2024-08-09},
   journal = {Applied and Computational Harmonic Analysis},
   author = {Daubechies, Ingrid and Lu, Jianfeng and Wu, Hau-Tieng},
   month = mar,
   year = {2011},
   keywords = {Empirical mode decomposition, Synchrosqueezing, Time-frequency analysis, Wavelet},
   pages = {243--261},
   file = {Daubechies et al. - 2011 - Synchrosqueezed wavelet transforms An empirical m.pdf:C\:\\Users\\Nicholas\\Zotero\\storage\\VC5VH7JJ\\Daubechies et al. - 2011 - Synchrosqueezed wavelet transforms An empirical m.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Nicholas\\Zotero\\storage\\MKD6K5BD\\S1063520310001016.html:text/html},
}

@book{grochenig_foundations_2001,
   address = {Boston, MA},
   series = {Applied and {Numerical} {Harmonic} {Analysis}},
   title = {Foundations of {Time}-{Frequency} {Analysis}},
   copyright = {http://www.springer.com/tdm},
   isbn = {978-1-4612-6568-9 978-1-4612-0003-1},
   url = {http://link.springer.com/10.1007/978-1-4612-0003-1},
   urldate = {2024-08-09},
   publisher = {Birkhäuser},
   author = {Gröchenig, Karlheinz},
   editor = {Benedetto, John J.},
   year = {2001},
   doi = {10.1007/978-1-4612-0003-1},
   keywords = {analysis, Fourier analysis, Fourier transform, harmonic analysis, information, information theory, Modulation, quantum mechanics, Signal, signal analysis, wavelets},
   file = {Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\V9BZDL9A\\Gröchenig - 2001 - Foundations of Time-Frequency Analysis.pdf:application/pdf},
}

@mastersthesis{richardson_sparse_2022,
   title = {A {Sparse} {Random} {Feature} {Model} for {Signal} {Decomposition}},
   copyright = {All rights reserved},
   url = {https://uwspace.uwaterloo.ca/handle/10012/18262},
   abstract = {Signal decomposition and multiscale signal analysis provide useful tools for time-frequency analysis. In this thesis, an overview of the signal decomposition problem is given and popular methods are discussed. A novel signal decomposition algorithm is presented: Sparse Random Mode Decomposition (SRMD). This method sparsely represents a signal as a sum of random windowed-sinusoidal features before clustering the time-frequency localized features into the constituent modes. SRMD outperforms state-of-the-art methods on a variety of mathematical signals, and is applied to real-world astronomical and musical examples. Finally, we discuss a neural network approach to tackle challenging musical signals.},
   language = {en},
   urldate = {2022-12-13},
   school = {University of Waterloo},
   author = {Richardson, Nicholas Joseph Emile},
   month = may,
   year = {2022},
   note = {Accepted: 2022-05-11T20:19:12Z},
   file = {Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\ZJJFHB6P\\Richardson - 2022 - A Sparse Random Feature Model for Signal Decomposi.pdf:application/pdf},
}

@article{richardson_srmd_2023,
   title = {{SRMD}: {Sparse} {Random} {Mode} {Decomposition}},
   copyright = {All rights reserved},
   issn = {2661-8893},
   shorttitle = {{SRMD}},
   url = {https://doi.org/10.1007/s42967-023-00273-x},
   doi = {10.1007/s42967-023-00273-x},
   abstract = {Signal decomposition and multiscale signal analysis provide many useful tools for time-frequency analysis. We proposed a random feature method for analyzing time-series data by constructing a sparse approximation to the spectrogram. The randomization is both in the time window locations and the frequency sampling, which lowers the overall sampling and computational cost. The sparsification of the spectrogram leads to a sharp separation between time-frequency clusters which makes it easier to identify intrinsic modes, and thus leads to a new data-driven mode decomposition. The applications include signal representation, outlier removal, and mode decomposition. On benchmark tests, we show that our approach outperforms other state-of-the-art decomposition methods.},
   language = {en},
   urldate = {2023-07-06},
   journal = {Communications on Applied Mathematics and Computation},
   author = {Richardson, Nicholas and Schaeffer, Hayden and Tran, Giang},
   month = jun,
   year = {2023},
   keywords = {42A99, 65T99, Short-time Fourier transform, Signal decomposition, Sparse random features},
   file = {Submitted Version:C\:\\Users\\Nicholas\\Zotero\\storage\\7JMT67GZ\\Richardson et al. - 2023 - SRMD Sparse Random Mode Decomposition.pdf:application/pdf},
}

@article{fan_polar_2022,
   title = {Polar {Deconvolution} of {Mixed} {Signals}},
   volume = {70},
   issn = {1941-0476},
   url = {https://ieeexplore.ieee.org/document/9783095},
   doi = {10.1109/TSP.2022.3178191},
   abstract = {The signal demixing problem seeks to separate a superposition of multiple signals into its constituent components. This paper studies a two-stage approach that first decompresses and subsequently deconvolves the noisy and undersampled observations of the superposition using two convex programs. Probabilistic error bounds are given on the accuracy with which this process approximates the individual signals. The theory of polar convolution of convex sets and gauge functions plays a central role in the analysis and solution process. If the measurements are random and the noise is bounded, this approach stably recovers low-complexity and mutually incoherent signals, with high probability and with near optimal sample complexity. We develop an efficient algorithm, based on level-set and conditional-gradient methods, that solves the convex optimization problems with sublinear iteration complexity and linear space requirements. Numerical experiments on both real and synthetic data confirm the theory and the efficiency of the approach.},
   urldate = {2024-08-14},
   journal = {IEEE Transactions on Signal Processing},
   author = {Fan, Zhenan and Jeong, Halyun and Joshi, Babhru and Friedlander, Michael P.},
   year = {2022},
   note = {Conference Name: IEEE Transactions on Signal Processing},
   keywords = {Convolution, Convex functions, Noise measurement, Complexity theory, Atomic measurements, atomic sparsity, convex optimization, Deconvolution, Measurement uncertainty, polar convolution, Signal demixing},
   pages = {2713--2727},
   file = {IEEE Xplore Abstract Record:C\:\\Users\\Nicholas\\Zotero\\storage\\8V483IZK\\9783095.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\294PH3RL\\Fan et al. - 2022 - Polar Deconvolution of Mixed Signals.pdf:application/pdf},
}

@article{chen_atomic_2001,
   title = {Atomic {Decomposition} by {Basis} {Pursuit}},
   volume = {43},
   issn = {0036-1445},
   url = {https://epubs.siam.org/doi/abs/10.1137/S003614450037906X},
   doi = {10.1137/S003614450037906X},
   abstract = {A full-rank matrix \$\{{\textbackslash}bf A\}{\textbackslash}in {\textbackslash}mathbb\{R\}{\textasciicircum}\{n{\textbackslash}times m\}\$ with \$n{\textless}m\$ generates an underdetermined system of linear equations \$\{{\textbackslash}bf Ax\} = \{{\textbackslash}bf b\}\$ having infinitely many solutions. Suppose we seek the sparsest solution, i.e., the one with the fewest nonzero entries. Can it ever be unique? If so, when? As optimization of sparsity is combinatorial in nature, are there efficient methods for finding the sparsest solution? These questions have been answered positively and constructively in recent years, exposing a wide variety of surprising phenomena, in particular the existence of easily verifiable conditions under which optimally sparse solutions can be found by concrete, effective computational methods. Such theoretical results inspire a bold perspective on some important practical problems in signal and image processing. Several well-known signal and image processing problems can be cast as demanding solutions of undetermined systems of equations. Such problems have previously seemed, to many, intractable, but there is considerable evidence that these problems often have sparse solutions. Hence, advances in finding sparse solutions to underdetermined systems have energized research on such signal and image processing problems—to striking effect. In this paper we review the theoretical results on sparse solutions of linear systems, empirical results on sparse modeling of signals and images, and recent applications in inverse problems and compression in image processing. This work lies at the intersection of signal processing and applied mathematics, and arose initially from the wavelets and harmonic analysis research communities. The aim of this paper is to introduce a few key notions and applications connected to sparsity, targeting newcomers interested in either the mathematical aspects of this area or its applications.},
   number = {1},
   urldate = {2024-08-15},
   journal = {SIAM Review},
   author = {Chen, Scott Shaobing and Donoho, David L. and Saunders, Michael A.},
   month = jan,
   year = {2001},
   note = {Publisher: Society for Industrial and Applied Mathematics},
   pages = {129--159},
   file = {Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\D7S4VWC4\\Chen et al. - 2001 - Atomic Decomposition by Basis Pursuit.pdf:application/pdf},
}

@book{benedetto_wavelets_1993,
   address = {Boca Raton},
   edition = {1st},
   title = {Wavelets: {Mathematics} and {Applications}},
   isbn = {978-1-00-321045-0},
   url = {https://doi.org/10.1201/9781003210450},
   abstract = {Wavelets is a carefully organized and edited collection of extended survey papers addressing key topics in the mathematical foundations and applications of wavelet theory. The first part of the book is devoted to the fundamentals of wavelet analysis. The construction of wavelet bases and the fast computation of the wavelet transform in both continuous and discrete settings is covered. The theory of frames, dilation equations, and local Fourier bases are also presented.

The second part of the book discusses applications in signal analysis, while the third part covers operator analysis and partial differential equations. Each chapter in these sections provides an up-to-date introduction to such topics as sampling theory, probability and statistics, compression, numerical analysis, turbulence, operator theory, and harmonic analysis.
The book is ideal for a general scientific and engineering audience, yet it is mathematically precise. It will be an especially useful reference for harmonic analysts, partial differential equation researchers, signal processing engineers, numerical analysts, fluids researchers, and applied mathematicians.},
   language = {en},
   urldate = {2024-08-15},
   publisher = {CRC Press},
   author = {Benedetto, John J. and Frazier, Michael W.},
   year = {1993},
   file = {Snapshot:C\:\\Users\\Nicholas\\Zotero\\storage\\3Z78QJVN\\wavelets-john-benedetto.html:text/html;Wavelets  Mathematics and Applications  John J. .pdf:C\:\\Users\\Nicholas\\Zotero\\storage\\HU446ISF\\Wavelets  Mathematics and Applications  John J. .pdf:application/pdf},
}


%%%%%%%%%%%%%%%%%%%%%
%% EMD and Friends %%
%%%%%%%%%%%%%%%%%%%%%

@article{huang_empirical_1998,
   title = {The empirical mode decomposition and the {Hilbert} spectrum for nonlinear and non-stationary time series analysis},
   volume = {454},
   url = {https://royalsocietypublishing.org/doi/abs/10.1098/rspa.1998.0193},
   doi = {10.1098/rspa.1998.0193},
   abstract = {A new method for analysing nonlinear and non-stationary data has been developed. The key part of the method is the ‘empirical mode decomposition’ method with which any complicated data set can be decomposed into a finite and often small number of ‘intrinsic mode functions’ that admit well-behaved Hilbert transforms. This decomposition method is adaptive, and, therefore, highly efficient. Since the decomposition is based on the local characteristic time scale of the data, it is applicable to nonlinear and non-stationary processes. With the Hilbert transform, the ‘instrinic mode functions’ yield instantaneous frequencies as functions of time that give sharp identifications of imbedded structures. The final presentation of the results is an energy-frequency-time distribution, designated as the Hilbert spectrum. In this method, the main conceptual innovations are the introduction of ‘intrinsic mode functions’ based on local properties of the signal, which make the instantaneous frequency meaningful; and the introduction of the instantaneous frequencies for complicated data sets, which eliminate the need for spurious harmonics to represent nonlinear and non-stationary signals. Examples from the numerical results of the classical nonlinear equation systems and data representing natural phenomena are given to demonstrate the power of this new method. Classical nonlinear system data are especially interesting, for they serve to illustrate the roles played by the nonlinear and non-stationary effects in the energy-frequency-time distribution.},
   number = {1971},
   urldate = {2024-08-09},
   journal = {Proceedings of the Royal Society of London. Series A: Mathematical, Physical and Engineering Sciences},
   author = {Huang, Norden E. and Shen, Zheng and Long, Steven R. and Wu, Manli C. and Shih, Hsing H. and Zheng, Quanan and Yen, Nai-Chyuan and Tung, Chi Chao and Liu, Henry H.},
   month = mar,
   year = {1998},
   note = {Publisher: Royal Society},
   keywords = {empirical mode decomposition, frequency-time spectrum, Hilbert spectral analysis, intrinsic time scale, non-stationary time series, nonlinear differential equations},
   pages = {903--995},
   file = {Submitted Version:C\:\\Users\\Nicholas\\Zotero\\storage\\RMLHIPJC\\Huang et al. - 1998 - The empirical mode decomposition and the Hilbert s.pdf:application/pdf},
}

@article{wu_ensemble_2009,
   title = {Ensemble empirical mode decomposition: a noise-assisted data analysis method},
   volume = {01},
   issn = {1793-5369},
   shorttitle = {Ensemble empirical mode decomposition},
   url = {https://www.worldscientific.com/doi/abs/10.1142/S1793536909000047},
   doi = {10.1142/S1793536909000047},
   abstract = {A new Ensemble Empirical Mode Decomposition (EEMD) is presented. This new approach consists of sifting an ensemble of white noise-added signal (data) and treats the mean as the final true result. Finite, not infinitesimal, amplitude white noise is necessary to force the ensemble to exhaust all possible solutions in the sifting process, thus making the different scale signals to collate in the proper intrinsic mode functions (IMF) dictated by the dyadic filter banks. As EEMD is a time–space analysis method, the added white noise is averaged out with sufficient number of trials; the only persistent part that survives the averaging process is the component of the signal (original data), which is then treated as the true and more physical meaningful answer. The effect of the added white noise is to provide a uniform reference frame in the time–frequency space; therefore, the added noise collates the portion of the signal of comparable scale in one IMF. With this ensemble mean, one can separate scales naturally without any a priori subjective criterion selection as in the intermittence test for the original EMD algorithm. This new approach utilizes the full advantage of the statistical characteristics of white noise to perturb the signal in its true solution neighborhood, and to cancel itself out after serving its purpose; therefore, it represents a substantial improvement over the original EMD and is a truly noise-assisted data analysis (NADA) method.},
   number = {01},
   urldate = {2024-08-09},
   journal = {Advances in Adaptive Data Analysis},
   author = {Wu, Zhaohua and Huang, Norden E.},
   month = jan,
   year = {2009},
   note = {Publisher: World Scientific Publishing Co.},
   keywords = {Empirical Mode Decomposition (EMD), end effect reduction, ensemble empirical mode decompositions, Intrinsic Mode Function (IMF), noise-assisted data analysis (NADA), shifting stoppage criteria},
   pages = {1--41},
}

@inproceedings{torres_complete_2011,
   title = {A complete ensemble empirical mode decomposition with adaptive noise},
   url = {https://ieeexplore.ieee.org/document/5947265},
   doi = {10.1109/ICASSP.2011.5947265},
   abstract = {In this paper an algorithm based on the ensemble empirical mode decomposition (EEMD) is presented. The key idea on the EEMD relies on averaging the modes obtained by EMD applied to several realizations of Gaussian white noise added to the original signal. The resulting decomposition solves the EMD mode mixing problem, however it introduces new ones. In the method here proposed, a particular noise is added at each stage of the decomposition and a unique residue is computed to obtain each mode. The resulting decomposition is complete, with a numerically negligible error. Two examples are presented: a discrete Dirac delta function and an electrocardiogram signal. The results show that, compared with EEMD, the new method here presented also provides a better spectral separation of the modes and a lesser number of sifting iterations is needed, reducing the computational cost.},
   urldate = {2024-08-14},
   booktitle = {2011 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
   author = {Torres, María E. and Colominas, Marcelo A. and Schlotthauer, Gastón and Flandrin, Patrick},
   month = may,
   year = {2011},
   note = {ISSN: 2379-190X},
   keywords = {Biomedical Signal Processing, Computational efficiency, Electrocardiography, Empirical Mode Decomposition, Heart Rate Variability, Oscillators, Signal processing algorithms, Signal to noise ratio, White noise},
   pages = {4144--4147},
   file = {IEEE Xplore Abstract Record:C\:\\Users\\Nicholas\\Zotero\\storage\\TL7TX5CF\\5947265.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\A8XBGPWL\\Torres et al. - 2011 - A complete ensemble empirical mode decomposition w.pdf:application/pdf},
}

@article{gilles_empirical_2013,
   title = {Empirical {Wavelet} {Transform}},
   volume = {61},
   issn = {1941-0476},
   url = {https://ieeexplore.ieee.org/document/6522142},
   doi = {10.1109/TSP.2013.2265222},
   abstract = {Some recent methods, like the empirical mode decomposition (EMD), propose to decompose a signal accordingly to its contained information. Even though its adaptability seems useful for many applications, the main issue with this approach is its lack of theory. This paper presents a new approach to build adaptive wavelets. The main idea is to extract the different modes of a signal by designing an appropriate wavelet filter bank. This construction leads us to a new wavelet transform, called the empirical wavelet transform. Many experiments are presented showing the usefulness of this method compared to the classic EMD.},
   number = {16},
   urldate = {2024-08-14},
   journal = {IEEE Transactions on Signal Processing},
   author = {Gilles, Jérôme},
   month = aug,
   year = {2013},
   note = {Conference Name: IEEE Transactions on Signal Processing},
   keywords = {Adaptation models, Adaptive filtering, empirical mode decomposition, Mathematical model, Signal processing algorithms, wavelet, Wavelet analysis, Wavelet packets},
   pages = {3999--4010},
   file = {IEEE Xplore Abstract Record:C\:\\Users\\Nicholas\\Zotero\\storage\\YKI6EFQU\\6522142.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\M9VMU6Z3\\Gilles - 2013 - Empirical Wavelet Transform.pdf:application/pdf},
}

@article{dragomiretskiy_variational_2014,
   title = {Variational {Mode} {Decomposition}},
   volume = {62},
   issn = {1941-0476},
   url = {https://ieeexplore.ieee.org/abstract/document/6655981?casa_token=PC335mzgxvwAAAAA:1iQNbuj4EdQREeI6EogXeeipRfgRP287zowsJtKEgZ6TotoVPeAGXhSTcAKEDvP81-CdlgtMPw},
   doi = {10.1109/TSP.2013.2288675},
   abstract = {During the late 1990s, Huang introduced the algorithm called Empirical Mode Decomposition, which is widely used today to recursively decompose a signal into different modes of unknown but separate spectral bands. EMD is known for limitations like sensitivity to noise and sampling. These limitations could only partially be addressed by more mathematical attempts to this decomposition problem, like synchrosqueezing, empirical wavelets or recursive variational decomposition. Here, we propose an entirely non-recursive variational mode decomposition model, where the modes are extracted concurrently. The model looks for an ensemble of modes and their respective center frequencies, such that the modes collectively reproduce the input signal, while each being smooth after demodulation into baseband. In Fourier domain, this corresponds to a narrow-band prior. We show important relations to Wiener filter denoising. Indeed, the proposed method is a generalization of the classic Wiener filter into multiple, adaptive bands. Our model provides a solution to the decomposition problem that is theoretically well founded and still easy to understand. The variational model is efficiently optimized using an alternating direction method of multipliers approach. Preliminary results show attractive performance with respect to existing mode decomposition models. In particular, our proposed model is much more robust to sampling and noise. Finally, we show promising practical decomposition results on a series of artificial and real data.},
   number = {3},
   urldate = {2024-08-14},
   journal = {IEEE Transactions on Signal Processing},
   author = {Dragomiretskiy, Konstantin and Zosso, Dominique},
   month = feb,
   year = {2014},
   note = {Conference Name: IEEE Transactions on Signal Processing},
   keywords = {AM-FM, augmented Lagrangian, Bandwidth, Fourier transform, Frequency estimation, Frequency modulation, Hilbert transform, mode decomposition, Noise, Robustness, spectral decomposition, variational problem, Wavelet transforms, Wiener filter},
   pages = {531--544},
   file = {IEEE Xplore Abstract Record:C\:\\Users\\Nicholas\\Zotero\\storage\\5MRN8EZL\\6655981.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\UNVIQ7IS\\Dragomiretskiy and Zosso - 2014 - Variational Mode Decomposition.pdf:application/pdf},
}

@article{rehman_multivariate_2019,
   title = {Multivariate {Variational} {Mode} {Decomposition}},
   volume = {67},
   issn = {1941-0476},
   url = {https://ieeexplore.ieee.org/abstract/document/8890883?casa_token=P_c5Gd1JPrUAAAAA:vAVpetd5P8BJLj-EUk7QRwxJEzZMbibyjNf0btabu_7DkHtmpAWe-oOyBQf_vXz_ffIkuLY0Kw},
   doi = {10.1109/TSP.2019.2951223},
   abstract = {We present a generic extension of variational mode decomposition (VMD) algorithm to multivariate or multichannel data. The proposed method utilizes a model for multivariate modulated oscillations that is based on the presence of a joint or common frequency component among all channels of input data. We then formulate a variational optimization problem that aims to extract an ensemble of band-limited modes containing inherent multivariate modulated oscillations present in the data. The cost function to be minimized is the sum of bandwidths of all signal modes across all input data channels, which is a generic extension of the cost function used in standard VMD to multivariate data. Minimization of the resulting variational model is achieved through the alternating direction method of multipliers (ADMM) that yields an optimal set of multivariate modes in terms of narrow bandwidth and corresponding center frequencies. The proposed extension is elegant as it does not require any extra user-defined parameters for its operation i.e., it uses the same parameters as standard VMD. We demonstrate the effectiveness of the proposed method through results obtained from extensive simulations involving test (synthetic) and real world multivariate data sets. Specifically, we highlight the utility of the proposed method in two real world applications which include the separation of alpha rhythms in multivariate electroencephalogram (EEG) data and the decomposition of bivariate cardiotocographic signals that consist of fetal heart rate and maternal uterine contraction (FHR-UC) as its two channels.},
   number = {23},
   urldate = {2024-08-14},
   journal = {IEEE Transactions on Signal Processing},
   author = {Rehman, Naveed ur and Aftab, Hania},
   month = dec,
   year = {2019},
   note = {Conference Name: IEEE Transactions on Signal Processing},
   keywords = {biomedical applications, Data models, empirical mode decomposition, multivariate data, Oscillators, Signal processing algorithms, Signal resolution, Standards, Time-frequency analysis, Transforms, variational mode decomposition},
   pages = {6039--6052},
   file = {IEEE Xplore Abstract Record:C\:\\Users\\Nicholas\\Zotero\\storage\\R68ZSX36\\8890883.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\MDFQWSTX\\Rehman and Aftab - 2019 - Multivariate Variational Mode Decomposition.pdf:application/pdf},
}
%%%%%%%%%%%%%%%%%%%%%
%% Factor Analysis %%
%%%%%%%%%%%%%%%%%%%%%

@article{bickel_projection_2018,
   title = {Projection pursuit in high dimensions},
   volume = {115},
   url = {https://www.pnas.org/doi/abs/10.1073/pnas.1801177115},
   doi = {10.1073/pnas.1801177115},
   abstract = {Projection pursuit is a classical exploratory data analysis method to detect interesting low-dimensional structures in multivariate data. Originally, projection pursuit was applied mostly to data of moderately low dimension. Motivated by contemporary applications, we here study its properties in high-dimensional settings. Specifically, we analyze the asymptotic properties of projection pursuit on structureless multivariate Gaussian data with an identity covariance, as both dimension p and sample size n tend to infinity, with p/n→γ∈[0,∞]. Our main results are that (i) if γ=∞, then there exist projections whose corresponding empirical cumulative distribution function can approximate any arbitrary distribution; and (ii) if γ∈(0,∞), not all limiting distributions are possible. However, depending on the value of γ, various non-Gaussian distributions may still be approximated. In contrast, if we restrict to sparse projections, involving only a few of the p variables, then asymptotically all empirical cumulative distribution functions are Gaussian. And (iii) if γ=0, then asymptotically all projections are Gaussian. Some of these results extend to mean-centered sub-Gaussian data and to projections into k dimensions. Hence, in the “small n, large p” setting, unless sparsity is enforced, and regardless of the chosen projection index, projection pursuit may detect an apparent structure that has no statistical significance. Furthermore, our work reveals fundamental limitations on the ability to detect non-Gaussian signals in high-dimensional data, in particular through independent component analysis and related non-Gaussian component analysis.},
   number = {37},
   urldate = {2024-08-14},
   journal = {Proceedings of the National Academy of Sciences},
   author = {Bickel, Peter J. and Kur, Gil and Nadler, Boaz},
   month = sep,
   year = {2018},
   note = {Publisher: Proceedings of the National Academy of Sciences},
   pages = {9151--9156},
   file = {Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\6HPDJACY\\Bickel et al. - 2018 - Projection pursuit in high dimensions.pdf:application/pdf},
}


@article{sawada_blind_2006,
   title = {Blind {Extraction} of {Dominant} {Target} {Sources} {Using} {ICA} and {Time}-{Frequency} {Masking}},
   volume = {14},
   issn = {1558-7924},
   url = {https://ieeexplore.ieee.org/abstract/document/1709904?casa_token=QhWxEIufSmkAAAAA:plv_4a6N6IynfRtwhzIm97l0rixVzDpNAQ8jCbzTwMJps4pSE4T0qRFvCdKkmNzV0f5adxqVvw},
   doi = {10.1109/TASL.2006.872599},
   abstract = {This paper presents a method for enhancing target sources of interest and suppressing other interference sources. The target sources are assumed to be close to sensors, to have dominant powers at these sensors, and to have non-Gaussianity. The enhancement is performed blindly, i.e., without knowing the position and active time of each source. We consider a general case where the total number of sources is larger than the number of sensors, and neither the number of target sources nor the total number of sources is known. The method is based on a two-stage process where independent component analysis (ICA) is first employed in each frequency bin and then time-frequency masking is used to improve the performance further. We propose a new sophisticated method for deciding the number of target sources and then selecting their frequency components. We also propose a new criterion for specifying time-frequency masks. Experimental results for simulated cocktail party situations in a room, whose reverberation time was 130 ms, are presented to show the effectiveness and characteristics of the proposed method},
   number = {6},
   urldate = {2024-08-14},
   journal = {IEEE Transactions on Audio, Speech, and Language Processing},
   author = {Sawada, H. and Araki, S. and Mukai, R. and Makino, S.},
   month = nov,
   year = {2006},
   note = {Conference Name: IEEE Transactions on Audio, Speech, and Language Processing},
   keywords = {Blind source extraction, Blind source separation, blind source separation (BSS), convolutive mixture, frequency domain, Frequency domain analysis, independent component analysis, Independent component analysis, Interference suppression, permutation problem, Reverberation, Sensor phenomena and characterization, Source separation, Speech processing, Time frequency analysis, time-frequency masking},
   pages = {2165--2173},
   file = {IEEE Xplore Abstract Record:C\:\\Users\\Nicholas\\Zotero\\storage\\PGLCYQEY\\1709904.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\MWTYNBYD\\Sawada et al. - 2006 - Blind Extraction of Dominant Target Sources Using .pdf:application/pdf},
}

@article{yang_projection-pursuit-based_2013,
   title = {Projection-{Pursuit}-{Based} {Method} for {Blind} {Separation} of {Nonnegative} {Sources}},
   volume = {24},
   issn = {2162-2388},
   url = {https://ieeexplore.ieee.org/document/6365335/?arnumber=6365335},
   doi = {10.1109/TNNLS.2012.2224124},
   abstract = {This paper presents a projection pursuit (PP) based method for blind separation of nonnegative sources. First, the available observation matrix is mapped to construct a new mixing model, in which the inaccessible source matrix is normalized to be column-sum-to-1. Then, the PP method is proposed to solve this new model, where the mixing matrix is estimated column by column through tracing the projections to the mapped observations in specified directions, which leads to the recovery of the sources. The proposed method is much faster than Chan's method, which has similar assumptions to ours, due to the usage of optimal projection. It is also more advantageous in separating cross-correlated sources than the independence- and uncorrelation-based methods, as it does not employ any statistical information of the sources. Furthermore, the new method does not require the mixing matrix to be nonnegative. Simulation results demonstrate the superior performance of our method.},
   number = {1},
   urldate = {2024-08-14},
   journal = {IEEE Transactions on Neural Networks and Learning Systems},
   author = {Yang, Zuyuan and Xiang, Yong and Rong, Yue and Xie, Shengli},
   month = jan,
   year = {2013},
   note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
   keywords = {Blind source separation, Educational institutions, Equations, Hyperspectral imaging, Indexes, Learning systems, linear programming (LP), Mathematical model, nonnegative sources, projection pursuit (PP), Vectors},
   pages = {47--57},
   file = {IEEE Xplore Abstract Record:C\:\\Users\\Nicholas\\Zotero\\storage\\CN48N8SZ\\6365335.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\V4ZR68TP\\Yang et al. - 2013 - Projection-Pursuit-Based Method for Blind Separati.pdf:application/pdf},
}

@article{chan_convex_2008,
   title = {A {Convex} {Analysis} {Framework} for {Blind} {Separation} of {Non}-{Negative} {Sources}},
   volume = {56},
   issn = {1941-0476},
   url = {https://ieeexplore.ieee.org/document/4579130/?arnumber=4579130},
   doi = {10.1109/TSP.2008.928937},
   abstract = {This paper presents a new framework for blind source separation (BSS) of non-negative source signals. The proposed framework, referred herein to as convex analysis of mixtures of non-negative sources (CAMNS), is deterministic requiring no source independence assumption, the entrenched premise in many existing (usually statistical) BSS frameworks. The development is based on a special assumption called local dominance. It is a good assumption for source signals exhibiting sparsity or high contrast, and thus is considered realistic to many real-world problems such as multichannel biomedical imaging. Under local dominance and several standard assumptions, we apply convex analysis to establish a new BSS criterion, which states that the source signals can be perfectly identified (in a blind fashion) by finding the extreme points of an observation-constructed polyhedral set. Methods for fulfilling the CAMNS criterion are also derived, using either linear programming or simplex geometry. Simulation results on several data sets are presented to demonstrate the efficacy of the proposed method over several other reported BSS methods.},
   number = {10},
   urldate = {2024-08-14},
   journal = {IEEE Transactions on Signal Processing},
   author = {Chan, Tsung-Han and Ma, Wing-Kin and Chi, Chong-Yung and Wang, Yue},
   month = oct,
   year = {2008},
   note = {Conference Name: IEEE Transactions on Signal Processing},
   keywords = {Acoustic signal processing, Acoustical engineering, Biomedical imaging, Biomedical signal processing, Blind separation, Blind source separation, convex analysis criterion, Convex analysis criterion, convex optimization, Convex optimization, Councils, Geometry, Independent component analysis, linear program, Linear program, non-negative sources, Non-negative sources, simplex geometry, Simplex geometry, Source separation, Speech processing},
   pages = {5120--5134},
   file = {IEEE Xplore Abstract Record:C\:\\Users\\Nicholas\\Zotero\\storage\\RR6QI98H\\4579130.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\3VSQKHKF\\Chan et al. - 2008 - A Convex Analysis Framework for Blind Separation o.pdf:application/pdf},
}

@inproceedings{wang_blind_2006,
   address = {Berlin, Heidelberg},
   title = {Blind {Separation} of {Multichannel} {Biomedical} {Image} {Patterns} by {Non}-negative {Least}-{Correlated} {Component} {Analysis}},
   isbn = {978-3-540-37447-3},
   doi = {10.1007/11818564_17},
   abstract = {Cellular and molecular imaging promises powerful tools for the visualization and elucidation of important disease-causing biological processes. Recent research aims to simultaneously assess the spatial-spectral/temporal distributions of multiple biomarkers, where the signals often represent a composite of more than one distinct source independent of spatial resolution. We report here a blind source separation method for quantitative dissection of mixed yet correlated biomarker patterns. The computational solution is based on a latent variable model, whose parameters are estimated using the non-negative least-correlated component analysis (nLCA) proposed in this paper. We demonstrate the efficacy of the nLCA with real bio-imaging data. With accurate and robust performance, it has powerful features which are of considerable widespread applicability.},
   language = {en},
   booktitle = {Pattern {Recognition} in {Bioinformatics}},
   publisher = {Springer},
   author = {Wang, Fa-Yu and Wang, Yue and Chan, Tsung-Han and Chi, Chong-Yung},
   editor = {Rajapakse, Jagath C. and Wong, Limsoon and Acharya, Raj},
   year = {2006},
   keywords = {Blind Separation, Blind Source Separation, Blind Source Separation Algorithm, Independent Component Analysis, Latent Variable Model},
   pages = {151--162},
   file = {Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\2YABXPCY\\Wang et al. - 2006 - Blind Separation of Multichannel Biomedical Image .pdf:application/pdf},
}

@article{oja_blind_2004,
   title = {Blind {Separation} of {Positive} {Sources} by {Globally} {Convergent} {Gradient} {Search}},
   volume = {16},
   issn = {0899-7667},
   url = {https://doi.org/10.1162/0899766041336413},
   doi = {10.1162/0899766041336413},
   abstract = {The instantaneous noise-free linear mixing model in independent component analysis is largely a solved problem under the usual assumption of independent nongaussian sources and full column rank mixing matrix. However, with some prior information on the sources, like positivity, new analysis and perhaps simplified solution methods may yet become possible. In this letter, we consider the task of independent component analysis when the independent sources are known to be nonnegative and well grounded, which means that they have a nonzero pdf in the region of zero. It can be shown that in this case, the solution method is basically very simple: an orthogonal rotation of the whitened observation vector into nonnegative outputs will give a positive permutation of the original sources. We propose a cost function whose minimum coincides with nonnegativity and derive the gradient algorithm under the whitening constraint, under which the separating matrix is orthogonal. We further prove that in the Stiefel manifold of orthogonal matrices, the cost function is a Lyapunov function for the matrix gradient flow, implying global convergence. Thus, this algorithm is guaranteed to find the nonnegative well-grounded independent sources. The analysis is complemented by a numerical simulation, which illustrates the algorithm.},
   number = {9},
   urldate = {2024-08-14},
   journal = {Neural Computation},
   author = {Oja, Erkki and Plumbley, Mark},
   month = sep,
   year = {2004},
   pages = {1811--1825},
   file = {Snapshot:C\:\\Users\\Nicholas\\Zotero\\storage\\6CTMD5F3\\Blind-Separation-of-Positive-Sources-by-Globally.html:text/html;Submitted Version:C\:\\Users\\Nicholas\\Zotero\\storage\\6ECHFK74\\Oja and Plumbley - 2004 - Blind Separation of Positive Sources by Globally C.pdf:application/pdf},
}

@article{pearson_liii_1901,
   title = {{LIII}. {On} lines and planes of closest fit to systems of points in space},
   url = {https://www.tandfonline.com/doi/abs/10.1080/14786440109462720},
   doi = {10.1080/14786440109462720},
   abstract = {(1901). LIII. On lines and planes of closest fit to systems of points in space . The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science: Vol. 2, No. 11, pp. 559-572.},
   language = {EN},
   urldate = {2024-08-14},
   journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
   author = {Pearson, Karl},
   month = nov,
   year = {1901},
   note = {Publisher: Taylor \& Francis Group},
   file = {Snapshot:C\:\\Users\\Nicholas\\Zotero\\storage\\P4C3BCGS\\14786440109462720.html:text/html;Submitted Version:C\:\\Users\\Nicholas\\Zotero\\storage\\3FYKQU3J\\F.R.S - 1901 - LIII. On lines and planes of closest fit to system.pdf:application/pdf},
}

@article{wold_principal_1987,
   series = {Proceedings of the {Multivariate} {Statistical} {Workshop} for {Geologists} and {Geochemists}},
   title = {Principal component analysis},
   volume = {2},
   issn = {0169-7439},
   url = {https://www.sciencedirect.com/science/article/pii/0169743987800849},
   doi = {10.1016/0169-7439(87)80084-9},
   abstract = {Principal component analysis of a data matrix extracts the dominant patterns in the matrix in terms of a complementary set of score and loading plots. It is the responsibility of the data analyst to formulate the scientific issue at hand in terms of PC projections, PLS regressions, etc. Ask yourself, or the investigator, why the data matrix was collected, and for what purpose the experiments and measurements were made. Specify before the analysis what kinds of patterns you would expect and what you would find exciting. The results of the analysis depend on the scaling of the matrix, which therefore must be specified. Variance scaling, where each variable is scaled to unit variance, can be recommended for general use, provided that almost constant variables are left unscaled. Combining different types of variables warrants blockscaling. In the initial analysis, look for outliers and strong groupings in the plots, indicating that the data matrix perhaps should be “polished” or whether disjoint modeling is the proper course. For plotting purposes, two or three principal components are usually sufficient, but for modeling purposes the number of significant components should be properly determined, e.g. by cross-validation. Use the resulting principal components to guide your continued investigation or chemical experimentation, not as an end in itself.},
   number = {1},
   urldate = {2024-08-14},
   journal = {Chemometrics and Intelligent Laboratory Systems},
   author = {Wold, Svante and Esbensen, Kim and Geladi, Paul},
   month = aug,
   year = {1987},
   pages = {37--52},
   file = {ScienceDirect Snapshot:C\:\\Users\\Nicholas\\Zotero\\storage\\49WP76GD\\0169743987800849.html:text/html;Wold et al. - 1987 - Principal component analysis.pdf:C\:\\Users\\Nicholas\\Zotero\\storage\\ELJYNSID\\Wold et al. - 1987 - Principal component analysis.pdf:application/pdf},
}

@article{abdi_principal_2010,
   title = {Principal component analysis},
   volume = {2},
   copyright = {Copyright © 2010 John Wiley \& Sons, Inc.},
   issn = {1939-0068},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wics.101},
   doi = {10.1002/wics.101},
   abstract = {Principal component analysis (PCA) is a multivariate technique that analyzes a data table in which observations are described by several inter-correlated quantitative dependent variables. Its goal is to extract the important information from the table, to represent it as a set of new orthogonal variables called principal components, and to display the pattern of similarity of the observations and of the variables as points in maps. The quality of the PCA model can be evaluated using cross-validation techniques such as the bootstrap and the jackknife. PCA can be generalized as correspondence analysis (CA) in order to handle qualitative variables and as multiple factor analysis (MFA) in order to handle heterogeneous sets of variables. Mathematically, PCA depends upon the eigen-decomposition of positive semi-definite matrices and upon the singular value decomposition (SVD) of rectangular matrices. Copyright © 2010 John Wiley \& Sons, Inc. This article is categorized under: Statistical and Graphical Methods of Data Analysis {\textgreater} Multivariate Analysis Statistical and Graphical Methods of Data Analysis {\textgreater} Dimension Reduction},
   language = {en},
   number = {4},
   urldate = {2024-08-14},
   journal = {WIREs Computational Statistics},
   author = {Abdi, Hervé and Williams, Lynne J.},
   year = {2010},
   note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/wics.101},
   keywords = {bilinear decomposition, factor scores and loadings, multiple factor analysis, RESS PRESS, singular and eigen value decomposition},
   pages = {433--459},
   file = {Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\SRRLSGFH\\Abdi and Williams - 2010 - Principal component analysis.pdf:application/pdf;Snapshot:C\:\\Users\\Nicholas\\Zotero\\storage\\28WADFTH\\wics.html:text/html},
}

@article{friedman_projection_1974,
   title = {A {Projection} {Pursuit} {Algorithm} for {Exploratory} {Data} {Analysis}},
   volume = {C-23},
   issn = {1557-9956},
   url = {https://ieeexplore.ieee.org/abstract/document/1672644?casa_token=JmMBh7pWKTQAAAAA:1sZIOYgZT0I_yNu8gxrjcLvOWR3WtZV4QZ_ZTec8aM7LXsVyHgHXi3y02-EFvGOI9sV6faYM7w},
   doi = {10.1109/T-C.1974.224051},
   abstract = {An algorithm for the analysis of multivariate data is presented and is discussed in terms of specific examples. The algorithm seeks to find one-and two-dimensional linear projections of multivariate data that are relatively highly revealing.},
   number = {9},
   urldate = {2024-08-14},
   journal = {IEEE Transactions on Computers},
   author = {Friedman, J.H. and Tukey, J.W.},
   month = sep,
   year = {1974},
   note = {Conference Name: IEEE Transactions on Computers},
   keywords = {Clustering, dimensionality reduction, mappings, multidimensional scaling, multivariate data analysis, nonparametric pattern recognition, statistics.},
   pages = {881--890},
   file = {IEEE Xplore Abstract Record:C\:\\Users\\Nicholas\\Zotero\\storage\\JGW2HXPP\\1672644.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\5VZ8W6CZ\\Friedman and Tukey - 1974 - A Projection Pursuit Algorithm for Exploratory Dat.pdf:application/pdf},
}

@article{plumbley_algorithms_2003,
   title = {Algorithms for nonnegative independent component analysis},
   volume = {14},
   issn = {1941-0093},
   url = {https://ieeexplore.ieee.org/abstract/document/1199651?casa_token=csfa2Y7TfEcAAAAA:-6SddM_f4ARV30WbvFmZT6jYYGl-ePfgzmmi_LYloKIedvZlpHjlDHrv_jPuJyn4_RVko0upZw},
   doi = {10.1109/TNN.2003.810616},
   abstract = {We consider the task of solving the independent component analysis (ICA) problem x=As given observations x, with a constraint of nonnegativity of the source random vector s. We refer to this as nonnegative independent component analysis and we consider methods for solving this task. For independent sources with nonzero probability density function (pdf) p(s) down to s=0 it is sufficient to find the orthonormal rotation y=Wz of prewhitened sources z=Vx, which minimizes the mean squared error of the reconstruction of z from the rectified version y/sup +/ of y. We suggest some algorithms which perform this, both based on a nonlinear principal component analysis (PCA) approach and on a geodesic search method driven by differential geometry considerations. We demonstrate the operation of these algorithms on an image separation problem, which shows in particular the fast convergence of the rotation and geodesic methods and apply the approach to a musical audio analysis task.},
   number = {3},
   urldate = {2024-08-14},
   journal = {IEEE Transactions on Neural Networks},
   author = {Plumbley, M.D.},
   month = may,
   year = {2003},
   note = {Conference Name: IEEE Transactions on Neural Networks},
   keywords = {Algorithm design and analysis, Image analysis, Image reconstruction, Independent component analysis, Neural networks, Principal component analysis, Probability density function, Search methods, Signal processing algorithms, Vectors},
   pages = {534--543},
   file = {IEEE Xplore Abstract Record:C\:\\Users\\Nicholas\\Zotero\\storage\\G55ND7VJ\\1199651.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\Y3T6KNL7\\Plumbley - 2003 - Algorithms for nonnegative independent component a.pdf:application/pdf},
}

@article{comon_independent_1994,
   series = {Higher {Order} {Statistics}},
   title = {Independent component analysis, {A} new concept?},
   volume = {36},
   issn = {0165-1684},
   url = {https://www.sciencedirect.com/science/article/pii/0165168494900299},
   doi = {10.1016/0165-1684(94)90029-9},
   abstract = {The independent component analysis (ICA) of a random vector consists of searching for a linear transformation that minimizes the statistical dependence between its components. In order to define suitable search criteria, the expansion of mutual information is utilized as a function of cumulants of increasing orders. An efficient algorithm is proposed, which allows the computation of the ICA of a data matrix within a polynomial time. The concept of ICA may actually be seen as an extension of the principal component analysis (PCA), which can only impose independence up to the second order and, consequently, defines directions that are orthogonal. Potential applications of ICA include data analysis and compression, Bayesian detection, localization of sources, and blind identification and deconvolution.
Zusammenfassung
Die Analyse unabhängiger Komponenten (ICA) eines Vektors beruht auf der Suche nach einer linearen Transformation, die die statistische Abhängigkeit zwischen den Komponenten minimiert. Zur Definition geeigneter Such-Kriterien wird die Entwicklung gemeinsamer Information als Funktion von Kumulanten steigender Ordnung genutzt. Es wird ein effizienter Algorithmus vorgeschlagen, der die Berechnung der ICA für Datenmatrizen innerhalb einer polynomischen Zeit erlaubt. Das Konzept der ICA kann eigentlich als Erweiterung der ‘Principal Component Analysis‘ (PCA) betrachtet werden, die nur die Unabhängigkeit bis zur zweiten Ordnung erzwingen kann und deshalb Richtungen definiert, die orthogonal sind. Potentielle Anwendungen der ICA beinhalten Daten-Analyse und Kompression, Bayes-Detektion, Quellenlokalisierung und blinde Identifikation und Entfaltung.
Résumé
L'Analyse en Composantes Indépendentes (ICA) d'un vecteur aléatoire consiste en la recherche d'une transformation linéaire qui minimise la dépendance statistique entre ses composantes. Afin de définir des critères d'optimisation appropriés, on utilise un développment en série de l'information mutuelle en fonction de cumulants d'ordre croissant. On propose ensuite un algorithme pratique permettant le calcul de l'ICA d'une matrice de données en un temps polynomial. Le concept d'ICA peut être vu en réalité comme une extention de l'Analyse en Composantes Principales (PCA) qui, elle, ne peut imposer l'indépendence qu'au second ordre et définit par conséquent des directions orthogonales. Les applications potentielles de l'ICA incluent l'analyse et la compression de données, la détection bayesienne, la localisation de sources, et l'identification et la déconvolution aveugles.},
   number = {3},
   urldate = {2024-08-14},
   journal = {Signal Processing},
   author = {Comon, Pierre},
   month = apr,
   year = {1994},
   pages = {287--314},
   file = {Full Text:C\:\\Users\\Nicholas\\Zotero\\storage\\S4BC7VJV\\Comon - 1994 - Independent component analysis, A new concept.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Nicholas\\Zotero\\storage\\B9BIQNHX\\0165168494900299.html:text/html},
}

@article{hyvarinen_fast_1999,
   title = {Fast and robust fixed-point algorithms for independent component analysis},
   volume = {10},
   issn = {1941-0093},
   url = {https://ieeexplore.ieee.org/abstract/document/761722?casa_token=i4pCP8Cr0CwAAAAA:T9oM91osGMCWZW4mZtyr4TN8uezqvfi6mC_dZ0u_rCAqktCK324Z4aP9nR7-dLXv_NVTOBZZQA},
   doi = {10.1109/72.761722},
   abstract = {Independent component analysis (ICA) is a statistical method for transforming an observed multidimensional random vector into components that are statistically as independent from each other as possible. We use a combination of two different approaches for linear ICA: Comon's information theoretic approach and the projection pursuit approach. Using maximum entropy approximations of differential entropy, we introduce a family of new contrast functions for ICA. These contrast functions enable both the estimation of the whole decomposition by minimizing mutual information, and estimation of individual independent components as projection pursuit directions. The statistical properties of the estimators based on such contrast functions are analyzed under the assumption of the linear mixture model, and it is shown how to choose contrast functions that are robust and/or of minimum variance. Finally, we introduce simple fixed-point algorithms for practical optimization of the contrast functions.},
   number = {3},
   urldate = {2024-08-14},
   journal = {IEEE Transactions on Neural Networks},
   author = {Hyvarinen, A.},
   month = may,
   year = {1999},
   note = {Conference Name: IEEE Transactions on Neural Networks},
   keywords = {Blind source separation, Data mining, Entropy, Feature extraction, Independent component analysis, Information theory, Mutual information, Robustness, Signal processing algorithms, Vectors},
   pages = {626--634},
   file = {IEEE Xplore Abstract Record:C\:\\Users\\Nicholas\\Zotero\\storage\\HG8UK4XN\\761722.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\97TQMXII\\Hyvarinen - 1999 - Fast and robust fixed-point algorithms for indepen.pdf:application/pdf},
}


@inproceedings{lee_algorithms_2000,
   title = {Algorithms for {Non}-negative {Matrix} {Factorization}},
   volume = {13},
   url = {https://papers.nips.cc/paper/2000/hash/f9d1152547c0bde01830b7e8bd60024c-Abstract.html},
   abstract = {Non-negative matrix factorization (NMF) has previously been shown to
be a useful decomposition for multivariate data. Two different multi-
plicative algorithms for NMF are analyzed. They differ only slightly in
the multiplicative factor used in the update rules. One algorithm can be
shown to minimize the conventional least squares error while the other
minimizes the generalized Kullback-Leibler divergence. The monotonic
convergence of both algorithms can be proven using an auxiliary func-
tion analogous to that used for proving convergence of the Expectation-
Maximization algorithm. The algorithms can also be interpreted as diag-
onally rescaled gradient descent, where the rescaling factor is optimally
chosen to ensure convergence.},
   urldate = {2023-03-20},
   booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
   publisher = {MIT Press},
   author = {Lee, Daniel and Seung, H. Sebastian},
   year = {2000},
   file = {Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\35MGYSUR\\Lee and Seung - 2000 - Algorithms for Non-negative Matrix Factorization.pdf:application/pdf},
}

@article{cohen_nonnegative_1993,
   title = {Nonnegative ranks, decompositions, and factorizations of nonnegative matrices},
   volume = {190},
   issn = {0024-3795},
   url = {https://www.sciencedirect.com/science/article/pii/002437959390224C},
   doi = {10.1016/0024-3795(93)90224-C},
   abstract = {The nonnegative rank of a nonnegative matrix is the smallest number of nonnegative rank-one matrices into which the matrix can be decomposed additively. Such decompositions are useful in diverse scientific disciplines. We obtain characterizations and bounds and show that the nonnegative rank can be computed exactly over the reals by a finite algorithm.},
   urldate = {2024-03-18},
   journal = {Linear Algebra and its Applications},
   author = {Cohen, Joel E. and Rothblum, Uriel G.},
   month = sep,
   year = {1993},
   pages = {149--168},
   file = {Cohen and Rothblum - 1993 - Nonnegative ranks, decompositions, and factorizati.pdf:C\:\\Users\\Nicholas\\Zotero\\storage\\GFSA8HKC\\Cohen and Rothblum - 1993 - Nonnegative ranks, decompositions, and factorizati.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Nicholas\\Zotero\\storage\\TJ5FP327\\002437959390224C.html:text/html},
}

@book{gillis_nonnegative_2020,
   address = {Philadelphia, PA},
   title = {Nonnegative {Matrix} {Factorization}},
   url = {https://epubs.siam.org/doi/abs/10.1137/1.9781611976410},
   abstract = {Nonnegative matrix factorization (NMF) in its modern form has become a standard tool in the analysis of high-dimensional data sets. This book provides a comprehensive and up-to-date account of the most important aspects of the NMF problem and is the first to detail its theoretical aspects, including geometric interpretation, nonnegative rank, complexity, and uniqueness. It explains why understanding these theoretical insights is key to using this computational tool effectively and meaningfully.},
   publisher = {Society for Industrial and Applied Mathematics},
   author = {Gillis, Nicolas},
   year = {2020},
   doi = {10.1137/1.9781611976410},
   note = {\_eprint: https://epubs.siam.org/doi/pdf/10.1137/1.9781611976410},
}

@article{gillis_geometric_2012,
   title = {On the geometric interpretation of the nonnegative rank},
   volume = {437},
   issn = {0024-3795},
   url = {https://www.sciencedirect.com/science/article/pii/S0024379512005022},
   doi = {10.1016/j.laa.2012.06.038},
   abstract = {The nonnegative rank of a nonnegative matrix is the minimum number of nonnegative rank-one factors needed to reconstruct it exactly. The problem of determining this rank and computing the corresponding nonnegative factors is difficult; however it has many potential applications, e.g., in data mining and graph theory. In particular, it can be used to characterize the minimal size of any extended reformulation of a given polytope. In this paper, we introduce and study a related quantity, called the restricted nonnegative rank. We show that computing this quantity is equivalent to a problem in computational geometry, and fully characterize its computational complexity. This in turn sheds new light on the nonnegative rank problem, and in particular allows us to provide new improved lower bounds based on its geometric interpretation. We apply these results to slack matrices and linear Euclidean distance matrices and obtain counter-examples to two conjectures of Beasley and Laffey, namely we show that the nonnegative rank of linear Euclidean distance matrices is not necessarily equal to their dimension, and that the rank of a matrix is not always greater than the nonnegative rank of its square.},
   number = {11},
   urldate = {2024-08-16},
   journal = {Linear Algebra and its Applications},
   author = {Gillis, Nicolas and Glineur, François},
   month = dec,
   year = {2012},
   keywords = {Computational complexity, Computational geometry, Extended formulations, Linear Euclidean distance matrices, Nested polytopes problem, Nonnegative rank, Restricted nonnegative rank},
   pages = {2685--2712},
   file = {ScienceDirect Snapshot:C\:\\Users\\Nicholas\\Zotero\\storage\\QHL2JIGB\\S0024379512005022.html:text/html;Submitted Version:C\:\\Users\\Nicholas\\Zotero\\storage\\RVIR64HG\\Gillis and Glineur - 2012 - On the geometric interpretation of the nonnegative.pdf:application/pdf},
}


%%%%%%%%%%%%%%%%%% Applications %%%%%%%%%%%%%%%%%%

%% Finance %%

@article{cao_financial_2019,
   title = {Financial time series forecasting model based on {CEEMDAN} and {LSTM}},
   volume = {519},
   issn = {0378-4371},
   url = {https://www.sciencedirect.com/science/article/pii/S0378437118314985},
   doi = {10.1016/j.physa.2018.11.061},
   abstract = {In order to improve the accuracy of the stock market prices forecasting, two hybrid forecasting models are proposed in this paper which combine the two kinds of empirical mode decomposition (EMD) with the long short-term memory (LSTM). The financial time series is a kind of non-linear and non-stationary random signal, which can be decomposed into several intrinsic mode functions of different time scales by the original EMD and the complete ensemble empirical mode decomposition with adaptive noise (CEEMDAN). To ensure the effect of historical data onto the prediction result, the LSTM prediction models are established for all each characteristic series from EMD and CEEMDAN deposition. The final prediction results are obtained by reconstructing each prediction series. The forecasting performance of the proposed models is verified by linear regression analysis of the major global stock market indices. Compared with single LSTM model, support vector machine (SVM), multi-layer perceptron (MLP) and other hybrid models, the experimental results show that the proposed models display a better performance in one-step-ahead forecasting of financial time series.},
   urldate = {2024-08-15},
   journal = {Physica A: Statistical Mechanics and its Applications},
   author = {Cao, Jian and Li, Zhi and Li, Jian},
   month = apr,
   year = {2019},
   keywords = {CEEMDAN-LSTM prediction, EMD-LSTM prediction, Financial time series forecasting},
   pages = {127--139},
   file = {ScienceDirect Snapshot:C\:\\Users\\Nicholas\\Zotero\\storage\\KLMXBPYX\\S0378437118314985.html:text/html},
}


@article{de_frein_analysis_2008,
   title = {Analysis of {Financial} {Data} {Using} {Non}-{Negative} {Matrix} {Factorization}},
   url = {https://arrow.tudublin.ie/engscheleart2/206},
   doi = {https://doi.org/10.12988/imf},
   journal = {Articles},
   author = {de Fréin, Ruairí and Drakakis, Konstantinos and Rickard, Scott and Cichocki, Andrzej},
   month = jan,
   year = {2008},
   file = {"Analysis of Financial Data Using Non-Negative Matrix Factorization" by Ruairí de Fréin, Konstantinos Drakakis et al.:C\:\\Users\\Nicholas\\Zotero\\storage\\ES52ZFV3\\206.html:text/html;de Fréin et al. - 2008 - Analysis of Financial Data Using Non-Negative Matr.pdf:C\:\\Users\\Nicholas\\Zotero\\storage\\ZFJCPYKC\\de Fréin et al. - 2008 - Analysis of Financial Data Using Non-Negative Matr.pdf:application/pdf},
}

@article{nobre_combining_2019,
   title = {Combining {Principal} {Component} {Analysis}, {Discrete} {Wavelet} {Transform} and {XGBoost} to trade in the financial markets},
   volume = {125},
   issn = {0957-4174},
   url = {https://www.sciencedirect.com/science/article/pii/S0957417419300995},
   doi = {10.1016/j.eswa.2019.01.083},
   abstract = {When investing in financial markets it is crucial to determine a trading signal that can provide the investor with the best entry and exit points of the financial market, however this is a difficult task and has become a very popular research topic in the financial area. This paper presents an expert system in the financial area that combines Principal Component Analysis (PCA), Discrete Wavelet Transform (DWT), Extreme Gradient Boosting (XGBoost) and a Multi-Objective Optimization Genetic Algorithm (MOO-GA) in order to achieve high returns with a low level of risk. PCA is used to reduce the dimensionality of the financial input data set and the DWT is used to perform a noise reduction to every feature. The resultant data set is then fed to an XGBoost binary classifier that has its hyperparameters optimized by a MOO-GA. The importance of the PCA is analyzed and the results obtained show that it greatly improves the performance of the system. In order to improve even more the results obtained in the system using PCA, the PCA and the DWT are then applied together in one system and the results obtained show that this system is capable of outperforming the Buy and Hold (B\&H) strategy in three of the five analyzed financial markets, achieving an average rate of return of 49.26\% in the portfolio, while the B\&H achieves on average 32.41\%.},
   urldate = {2024-08-15},
   journal = {Expert Systems with Applications},
   author = {Nobre, João and Neves, Rui Ferreira},
   month = jul,
   year = {2019},
   keywords = {Discrete Wavelet Transform (DWT), Extreme Gradient Boosting (XGBoost), Financial markets, Multi-Objective Optimization Genetic Algorithm (MOO-GA), Principal Component Analysis (PCA)},
   pages = {181--194},
   file = {Nobre and Neves - 2019 - Combining Principal Component Analysis, Discrete W.pdf:C\:\\Users\\Nicholas\\Zotero\\storage\\RIV8IUQJ\\Nobre and Neves - 2019 - Combining Principal Component Analysis, Discrete W.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Nicholas\\Zotero\\storage\\JUY4I6JN\\S0957417419300995.html:text/html},
}

%% Seismology %%

@article{wu_white_2019,
   title = {White noise attenuation of seismic trace by integrating variational mode decomposition with convolutional neural network},
   volume = {84},
   issn = {0016-8033},
   url = {https://library.seg.org/doi/abs/10.1190/geo2018-0635.1},
   doi = {10.1190/geo2018-0635.1},
   abstract = {Seismic noise attenuation is an important step in seismic data processing. Most noise attenuation algorithms are based on the analysis of time-frequency characteristics of the seismic data and noise. We have aimed to attenuate white noise of seismic data using the convolutional neural network (CNN). Traditional CNN-based noise attenuation algorithms need prior information (the “clean” seismic data or the noise contained in the seismic) in the training process. However, it is difficult to obtain such prior information in practice. We assume that the white noise contained in the seismic data can be simulated by a sufficient number of user-generated white noise realizations. We then attenuate the seismic white noise using the modified denoising CNN (MDnCNN). The MDnCNN does not need prior clean seismic data nor pure noise in the training procedure. To accurately and efficiently learn the features of seismic data and band-limited noise at different frequency bandwidths, we first decomposed the seismic data into several intrinsic mode functions (IMFs) using variational mode decomposition and then apply our denoising process to the IMFs. We use synthetic and field data examples to illustrate the robustness and superiority of our method over the traditional methods. The experiments demonstrate that our method can not only attenuate most of the white noise but it also rejects the migration artifacts.},
   number = {5},
   urldate = {2024-08-14},
   journal = {GEOPHYSICS},
   author = {Wu, Hao and Zhang, Bo and Lin, Tengfei and Li, Fangyu and Liu, Naihao},
   month = sep,
   year = {2019},
   note = {Publisher: Society of Exploration Geophysicists},
   keywords = {neural network, noise attenuation, signal decomposition},
   pages = {V307--V317},
   file = {Wu et al. - 2019 - White noise attenuation of seismic trace by integr.pdf:C\:\\Users\\Nicholas\\Zotero\\storage\\MF3NII2Z\\Wu et al. - 2019 - White noise attenuation of seismic trace by integr.pdf:application/pdf},
}

@article{huang2015synchrosqueezing,
  title={Synchrosqueezing {S}-transform and its application in seismic spectral decomposition},
  author={Huang, Zhong-lai and Zhang, Jianzhong and Zhao, Tie-hu and Sun, Yunbao},
  journal={{IEEE} Transactions on Geoscience and Remote Sensing},
  volume={54},
  number={2},
  pages={817--825},
  year={2015},
  publisher={IEEE}
}

@article{liu_seismic_2016,
   title = {Seismic {Time}–{Frequency} {Analysis} via {Empirical} {Wavelet} {Transform}},
   volume = {13},
   issn = {1558-0571},
   url = {https://ieeexplore.ieee.org/abstract/document/7321780?casa_token=2jdt6QfhTRQAAAAA:TAlV_SsJUQu8Wf5_Op9DHQvsBTX8dxriCTsHLWNlTyZuIfM73pGMnu3WKfyeQ7YMrEKKBFfxgw},
   doi = {10.1109/LGRS.2015.2493198},
   abstract = {Time-frequency analysis is able to reveal the useful information hidden in the seismic data. The high resolution of the time-frequency representation is of great importance to depict geological structures. In this letter, we propose a novel seismic time-frequency analysis approach using the newly developed empirical wavelet transform (EWT). It is the first time that EWT is applied in analyzing multichannel seismic data for the purpose of seismic exploration. EWT is a fully adaptive signal-analysis approach, which is similar to the empirical mode decomposition but has a consolidated mathematical background. EWT first estimates the frequency components presented in the seismic signal, then computes the boundaries, and extracts oscillatory components based on the boundaries computed. Synthetic, 2-D, and 3-D real seismic data are used to comprehensively demonstrate the effectiveness of the proposed seismic time-frequency analysis approach. Results show that the EWT can provide a much higher resolution than the traditional continuous wavelet transform and offers the potential in precisely highlighting geological and stratigraphic information.},
   number = {1},
   urldate = {2024-08-14},
   journal = {IEEE Geoscience and Remote Sensing Letters},
   author = {Liu, Wei and Cao, Siyuan and Chen, Yangkang},
   month = jan,
   year = {2016},
   note = {Conference Name: IEEE Geoscience and Remote Sensing Letters},
   keywords = {Continuous wavelet transform (CWT), Continuous wavelet transforms, empirical wavelet transform (EWT), Geology, instantaneous frequency, Signal resolution, sparse representation, Time-frequency analysis, time–frequency analysis, Wavelet analysis},
   pages = {28--32},
   file = {IEEE Xplore Abstract Record:C\:\\Users\\Nicholas\\Zotero\\storage\\BKK4RCJ5\\7321780.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\AQQ48JG6\\Liu et al. - 2016 - Seismic Time–Frequency Analysis via Empirical Wave.pdf:application/pdf},
}

@article{zhu_seismic_2019,
   title = {Seismic {Signal} {Denoising} and {Decomposition} {Using} {Deep} {Neural} {Networks}},
   volume = {57},
   issn = {1558-0644},
   url = {https://ieeexplore.ieee.org/abstract/document/8802278?casa_token=c8r_IK_8B8cAAAAA:lwiQJyJYHoNRMV0G1OK0tME_b1RmhD_aofq-jPi1_m8tK-axC1lA2PJlcV6NfZ6yC6H7qBEG3A},
   doi = {10.1109/TGRS.2019.2926772},
   abstract = {Frequency filtering is widely used in routine processing of seismic data to improve the signal-to-noise ratio (SNR) of recorded signals and by doing so to improve subsequent analyses. In this paper, we develop a new denoising/decomposition method, DeepDenoiser, based on a deep neural network. This network is able to simultaneously learn a sparse representation of data in the time-frequency domain and a non-linear function that maps this representation into masks that decompose input data into a signal of interest and noise (defined as any non-seismic signal). We show that DeepDenoiser achieves impressive denoising of seismic signals even when the signal and noise share a common frequency band. Because the noise statistics are automatically learned from data and require no assumptions, our method properly handles white noise, a variety of colored noise, and non-earthquake signals. DeepDenoiser can significantly improve the SNR with minimal changes in the waveform shape of interest, even in the presence of high noise levels. We demonstrate the effect of our method on improving earthquake detection. There are clear applications of DeepDenoiser to seismic imaging, micro-seismic monitoring, and preprocessing of ambient noise data. We also note that the potential applications of our approach are not limited to these applications or even to earthquake data and that our approach can be adapted to diverse signals and applications in other settings.},
   number = {11},
   urldate = {2024-08-15},
   journal = {IEEE Transactions on Geoscience and Remote Sensing},
   author = {Zhu, Weiqiang and Mousavi, S. Mostafa and Beroza, Gregory C.},
   month = nov,
   year = {2019},
   note = {Conference Name: IEEE Transactions on Geoscience and Remote Sensing},
   keywords = {Convolutional neural networks, decomposition, deep learning, Deep learning, Earthquakes, Neural networks, Noise measurement, Noise reduction, seismic denoising, Time-domain analysis, Transforms},
   pages = {9476--9488},
   file = {IEEE Xplore Abstract Record:C\:\\Users\\Nicholas\\Zotero\\storage\\TNIQ4FNB\\8802278.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\BFBGMEVN\\Zhu et al. - 2019 - Seismic Signal Denoising and Decomposition Using D.pdf:application/pdf},
}

%% Medical %%

@article{tang2020eeg,
    title = {Motor imagery {EEG} recognition based on conditional optimization empirical mode decomposition and multi-scale convolutional neural network},
    journal = {Expert Systems with Applications},
    volume = {149},
    pages = {113285},
    year = {2020},
    issn = {0957-4174},
    doi = {https://doi.org/10.1016/j.eswa.2020.113285},
    url = {https://www.sciencedirect.com/science/article/pii/S095741742030110X},
    author = {Xianlun Tang and Wei Li and Xingchen Li and Weichang Ma and Xiaoyuan Dang},
    keywords = {Empirical mode decomposition, Convolutional neural network, Motor imagery EEG, Feature extraction, Intelligent wheelchair}}

@article{carvalho2020vmdpyeeg,
    title = {Evaluating five different adaptive decomposition methods for {EEG} signal seizure detection and classification},
    journal = {Biomedical Signal Processing and Control},
    volume = {62},
    pages = {102073},
    year = {2020},
    issn = {1746-8094},
    doi = {https://doi.org/10.1016/j.bspc.2020.102073},
    url = {https://www.sciencedirect.com/science/article/pii/S1746809420302299},
    author = {Vinícius R. Carvalho and Márcio F.D. Moraes and Antônio P. Braga and Eduardo M.A.M. Mendes}
}

@article{ramli_blind_2020,
	series = {Knowledge-{Based} and {Intelligent} {Information} \& {Engineering} {Systems}: {Proceedings} of the 24th {International} {Conference} {KES2020}},
	title = {Blind {Source} {Separation} ({BSS}) of {Mixed} {Maternal} and {Fetal} {Electrocardiogram} ({ECG}) {Signal}: {A} comparative {Study}},
	volume = {176},
	issn = {1877-0509},
	shorttitle = {Blind {Source} {Separation} ({BSS}) of {Mixed} {Maternal} and {Fetal} {Electrocardiogram} ({ECG}) {Signal}},
	url = {https://www.sciencedirect.com/science/article/pii/S1877050920318846},
	doi = {10.1016/j.procs.2020.08.060},
	abstract = {Electrocardiogram (ECG) test is very important for fetus condition inspection so as to avoid stillbirth and neonatal death during pregnancy. It is well known and widely used as a medical tool as it is convenient and non-invasive. Nevertheless, analysing fetal ECG (FECG) signal by using naked eye is tedious as the observed signal is a mixed signal which consists of weak FECG, maternal ECG (MECG) and also other signals including mother’s respiratory noise. Hence, in this paper, Blind Source Separation (BSS) is used to extract the estimated desired signal i.e. FECG from the mixed signal. BSS is a well-known separation method that is able to extract desired signal without knowing any information of the source signal. The aim of this study is to elucidate the performance of BSS algorithms i.e. Fast Fixed-Point for Independent Component Analysis (FastICA), Joint Approximate Diagonalization of Eigenmatrix (JADE) and Principal Component Analysis (PCA) for FECG extraction. We integrate R-peak detection algorithm as a post-separation process to the BSS system in order to distinguish the estimated FECG and MECG for easier analysis process. Estimated signals are evaluated based on waveform characteristics observation and Signal-to Interference Ratio (SIR) parameter. We can conclude that JADE performance performs better in term of accuracy while FastICA is good in term of the computational time. However, FastICA manages to get comparable result as JADE after many fine-tuning steps and it is more flexible compared to JADE as it is not sensitive to low quality input signal.},
	urldate = {2024-08-09},
	journal = {Procedia Computer Science},
	author = {Ramli, Dzati Athiar and Shiong, Yeoh Hong and Hassan, Norsalina},
	month = jan,
	year = {2020},
	keywords = {Blind Source Separation (BSS), Electrocardiogram (ECG), Fetal ECG (FECG), Fetus, Maternal ECG (MECG)},
	pages = {582--591},
	file = {Ramli et al. - 2020 - Blind Source Separation (BSS) of Mixed Maternal an.pdf:C\:\\Users\\Nicholas\\Zotero\\storage\\VHI8QBAD\\Ramli et al. - 2020 - Blind Source Separation (BSS) of Mixed Maternal an.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Nicholas\\Zotero\\storage\\QTCDA8FC\\S1877050920318846.html:text/html},
   note = {ecg, pca, ica}
}


@inproceedings{ronneberger_u-net_2015,
   address = {Cham},
   title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
   isbn = {978-3-319-24574-4},
   shorttitle = {U-{Net}},
   doi = {10.1007/978-3-319-24574-4_28},
   abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
   language = {en},
   booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention} – {MICCAI} 2015},
   publisher = {Springer International Publishing},
   author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
   editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
   year = {2015},
   keywords = {Convolutional Layer, Data Augmentation, Deep Network, Ground Truth Segmentation, Training Image},
   pages = {234--241},
   file = {Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\J9634KB9\\Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf:application/pdf},
}

%% Music/Audio %%

@article{karamatli_audio_2019,
   title = {Audio {Source} {Separation} {Using} {Variational} {Autoencoders} and {Weak} {Class} {Supervision}},
   volume = {26},
   issn = {1558-2361},
   url = {https://ieeexplore.ieee.org/abstract/document/8769885?casa_token=NhNw23K3Xi8AAAAA:U_sY0GFX6bygnlXVBsOH-WS_RIeOl12OxTgahYZ_O-vdZNF-a0ciHf5piMq53D5P2x7jN_FbBQ},
   doi = {10.1109/LSP.2019.2929440},
   abstract = {In this letter, we propose a source separation method that is trained by observing the mixtures and the class labels of the sources present in the mixture without any access to isolated sources. Since our method does not require source class labels for every time-frequency bin but only a single label for each source constituting the mixture signal, we call this scenario as weak class supervision. We associate a variational autoencoder (VAE) with each source class within a nonnegative (compositional) model. Each VAE provides a prior model to identify the signal from its associated class in a sound mixture. After training the model on mixtures, we obtain a generative model for each source class and demonstrate our method on one-second mixtures of utterances of digits from 0 to 9. We show that the separation performance obtained by source class supervision is as good as the performance obtained by source signal supervision.},
   number = {9},
   urldate = {2024-08-15},
   journal = {IEEE Signal Processing Letters},
   author = {Karamatlı, Ertuğ and Cemgil, Ali Taylan and Kırbız, Serap},
   month = sep,
   year = {2019},
   note = {Conference Name: IEEE Signal Processing Letters},
   keywords = {Convolution, Decoding, Instruments, Source separation, Source Separation, Spectrogram, Time-frequency analysis, Training, Variational Autoencoders, Weak Supervision},
   pages = {1349--1353},
   file = {IEEE Xplore Abstract Record:C\:\\Users\\Nicholas\\Zotero\\storage\\NQUKWBKG\\8769885.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\HZ3Q8DPT\\Karamatlı et al. - 2019 - Audio Source Separation Using Variational Autoenco.pdf:application/pdf},
}

@article{hennequin_spleeter_2020,
	title = {Spleeter: a fast and efficient music source separation tool with pre-trained models},
	volume = {5},
	issn = {2475-9066},
	shorttitle = {Spleeter},
	url = {https://joss.theoj.org/papers/10.21105/joss.02154},
	doi = {10.21105/joss.02154},
	abstract = {Hennequin et al., (2020). Spleeter: a fast and efficient music source separation tool with pre-trained models. Journal of Open Source Software, 5(50), 2154, https://doi.org/10.21105/joss.02154},
	language = {en},
	number = {50},
	urldate = {2024-08-09},
	journal = {Journal of Open Source Software},
	author = {Hennequin, Romain and Khlif, Anis and Voituret, Felix and Moussallam, Manuel},
	month = jun,
	year = {2020},
	pages = {2154},
	file = {Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\KZXH2NWM\\Hennequin et al. - 2020 - Spleeter a fast and efficient music source separa.pdf:application/pdf},
}

@inproceedings{cohen-hadria_improving_2019,
   title = {Improving singing voice separation using {Deep} {U}-{Net} and {Wave}-{U}-{Net} with data augmentation},
   url = {https://ieeexplore.ieee.org/abstract/document/8902810?casa_token=9cDESmaHjhYAAAAA:5bC2025t_dRY1EgGhrXbDLN8IcDpJmUUW-E-IXkuZdtXdln-goCpkYzHtd46-WSlP7iq153_mw},
   doi = {10.23919/EUSIPCO.2019.8902810},
   abstract = {State-of-the-art singing voice separation is based on deep learning making use of CNN structures with skip connections (like U-Net model, Wave-U-Net model, or MSDENSELSTM). A key to the success of these models is the availability of a large amount of training data. In the following study, we are interested in singing voice separation for mono signals and will investigate into comparing the U-Net and the Wave-U-Net that are structurally similar, but work on different input representations. First, we report a few results on variations of the U-Net model. Second, we will discuss the potential of state of the art speech and music transformation algorithms for augmentation of existing data sets and demonstrate that the effect of these augmentations depends on the signal representations used by the model. The results demonstrate a considerable improvement due to the augmentation for both models. But pitch transposition is the most effective augmentation strategy for the U-Net model, while transposition, time stretching, and formant shifting have a much more balanced effect on the Wave-U-Net model. Finally, we compare the two models on the same dataset.},
   urldate = {2024-08-14},
   booktitle = {2019 27th {European} {Signal} {Processing} {Conference} ({EUSIPCO})},
   author = {Cohen-Hadria, Alice and Roebel, Axel and Peeters, Geoffroy},
   month = sep,
   year = {2019},
   note = {ISSN: 2076-1465},
   keywords = {Biological system modeling, convolutional neural network, data augmentation, Data models, Decoding, Singing voice separation, Source separation, Spectrogram, Training},
   pages = {1--5},
   file = {IEEE Xplore Abstract Record:C\:\\Users\\Nicholas\\Zotero\\storage\\UIJM2WNV\\8902810.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\DZCM8Q8B\\Cohen-Hadria et al. - 2019 - Improving singing voice separation using Deep U-Ne.pdf:application/pdf},
}

@article{stoter_open-unmix_2019,
	title = {Open-{Unmix} - {A} {Reference} {Implementation} for {Music} {Source} {Separation}},
	volume = {4},
	issn = {2475-9066},
	url = {https://joss.theoj.org/papers/10.21105/joss.01667},
	doi = {10.21105/joss.01667},
	abstract = {Stöter et al., (2019). Open-Unmix - A Reference Implementation for Music Source Separation. Journal of Open Source Software, 4(41), 1667, https://doi.org/10.21105/joss.01667},
	language = {en},
	number = {41},
	urldate = {2024-08-09},
	journal = {Journal of Open Source Software},
	author = {Stöter, Fabian-Robert and Uhlich, Stefan and Liutkus, Antoine and Mitsufuji, Yuki},
	month = sep,
	year = {2019},
	pages = {1667},
	file = {Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\3M5TVTJ6\\Stöter et al. - 2019 - Open-Unmix - A Reference Implementation for Music .pdf:application/pdf},
}

@misc{clarke_audio_2017,
	title = {Audio {Source} {Separation}: "{Demixing}" for {Production}},
	url = {https://www.mathworks.com/videos/audio-source-separation-demixing-for-production-1509622598413.html},
	abstract = {"Live at the Hollywood Bowl" was the only live album from The Beatles, and a hard one to listen to because of the screaming fans drowning out the music. The simple concert recording technology available in 1964 provided only a 3-track recording that was impossible to remix effectively using traditional methods. However, Abbey Road Studios recently used MATLAB® to create a new proprietary algorithm that can separate out the individual original sources from the old recordings in order to remaster the recording and remove 95\% of the background noise from the fans. In this talk, you will learn how a single engineer built a tool that exceeded any solution available on the market through community collaboration, code reuse, and the ability to quickly turn mathematical algorithms into an effective application. The complex tool they developed includes a mixture of spectral analysis, optimization, control theory, and musical modelling, and allowed them to release a new version of the album where the Fab Four can be heard live and like never before.},
	language = {English},
	urldate = {2024-08-09},
	author = {Clarke, James},
	month = oct,
	year = {2017},
}

%% Imaging %%

@article{qian_matrix-vector_2017,
   title = {Matrix-{Vector} {Nonnegative} {Tensor} {Factorization} for {Blind} {Unmixing} of {Hyperspectral} {Imagery}},
   volume = {55},
   issn = {1558-0644},
   doi = {10.1109/TGRS.2016.2633279},
   abstract = {Many spectral unmixing approaches ranging from geometry, algebra to statistics have been proposed, in which nonnegative matrix factorization (NMF)-based ones form an important family. The original NMF-based unmixing algorithm loses the spectral and spatial information between mixed pixels when stacking the spectral responses of the pixels into an observed matrix. Therefore, various constrained NMF methods are developed to impose spectral structure, spatial structure, and spectral-spatial joint structure into NMF to enforce the estimated endmembers and abundances preserve these structures. Compared with matrix format, the third-order tensor is more natural to represent a hyperspectral data cube as a whole, by which the intrinsic structure of hyperspectral imagery can be losslessly retained. Extended from NMF-based methods, a matrix-vector nonnegative tensor factorization (NTF) model is proposed in this paper for spectral unmixing. Different from widely used tensor factorization models, such as canonical polyadic decomposition CPD) and Tucker decomposition, the proposed method is derived from block term decomposition, which is a combination of CPD and Tucker decomposition. This leads to a more flexible frame to model various application-dependent problems. The matrix-vector NTF decomposes a third-order tensor into the sum of several component tensors, with each component tensor being the outer product of a vector (endmember) and a matrix (corresponding abundances). From a formal perspective, this tensor decomposition is consistent with linear spectral mixture model. From an informative perspective, the structures within spatial domain, within spectral domain, and cross spectral-spatial domain are retreated interdependently. Experiments demonstrate that the proposed method has outperformed several state-of-the-art NMF-based unmixing methods.},
   number = {3},
   journal = {IEEE Transactions on Geoscience and Remote Sensing},
   author = {Qian, Yuntao and Xiong, Fengchao and Zeng, Shan and Zhou, Jun and Tang, Yuan Yan},
   month = mar,
   year = {2017},
   note = {Conference Name: IEEE Transactions on Geoscience and Remote Sensing},
   keywords = {Tensile stress, Feature extraction, Matrix decomposition, Distance measurement, Hyperspectral imagery (HSI), Hyperspectral imaging, Mixture models, spectral unmixing, spectral-spatial structure, tensor factorization},
   pages = {1776--1792},
   file = {IEEE Xplore Abstract Record:C\:\\Users\\Nicholas\\Zotero\\storage\\CJE4YCQ7\\7784711.html:text/html;Qian et al. - 2017 - Matrix-Vector Nonnegative Tensor Factorization for.pdf:C\:\\Users\\Nicholas\\Zotero\\storage\\8KCUS7EF\\Qian et al. - 2017 - Matrix-Vector Nonnegative Tensor Factorization for.pdf:application/pdf},
}

@article{cun_improving_2020,
   title = {Improving the {Harmony} of the {Composite} {Image} by {Spatial}-{Separated} {Attention} {Module}},
   volume = {29},
   issn = {1941-0042},
   url = {https://ieeexplore.ieee.org/abstract/document/9018370?casa_token=2mAGgSrFRO0AAAAA:Llhz1LY_ZcN2LocLSQYYyEjXKHQ-aqmWWectWdsBx_qnL9ojvKCDXCrYGD80w1KpNBFjq1zowQ},
   doi = {10.1109/TIP.2020.2975979},
   abstract = {Image composition is one of the most important applications in image processing. However, the inharmonious appearance between the spliced region and background degrade the quality of the image. Thus, we address the problem of Image Harmonization: Given a spliced image and the mask of the spliced region, we try to harmonize the “style” of the pasted region with the background (non-spliced region). Previous approaches have been focusing on learning directly by the neural network. In this work, we start from an empirical observation: the differences can only be found in the spliced region between the spliced image and the harmonized result while they share the same semantic information and the appearance in the non-spliced region. Thus, in order to learn the feature map in the masked region and the others individually, we propose a novel attention module named Spatial-Separated Attention Module (S2AM). Furthermore, we design a novel image harmonization framework by inserting the S2AM in the coarser low-level features of the Unet structure by two different ways. Besides image harmonization, we make a big step for harmonizing the composite image without the specific mask under previous observation. The experiments show that the proposed S2AM performs better than other state-of-the-art attention modules in our task. Moreover, we demonstrate the advantages of our model against other state-of-the-art image harmonization methods via criteria from multiple points of view.},
   urldate = {2024-08-14},
   journal = {IEEE Transactions on Image Processing},
   author = {Cun, Xiaodong and Pun, Chi-Man},
   year = {2020},
   note = {Conference Name: IEEE Transactions on Image Processing},
   keywords = {attention mechanism, Convolution, Gallium nitride, Image color analysis, Image harmonization, image synthesis, Neural networks, Semantics, Task analysis},
   pages = {4759--4771},
   file = {IEEE Xplore Abstract Record:C\:\\Users\\Nicholas\\Zotero\\storage\\FSHKZSLP\\9018370.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\RDKF3RYR\\Cun and Pun - 2020 - Improving the Harmony of the Composite Image by Sp.pdf:application/pdf},
}

@article{tan_separating_2005,
   title = {Separating reflection components of textured surfaces using a single image},
   volume = {27},
   issn = {1939-3539},
   url = {https://ieeexplore.ieee.org/abstract/document/1374865},
   doi = {10.1109/TPAMI.2005.36},
   abstract = {In inhomogeneous objects, highlights are linear combinations of diffuse and specular reflection components. To our knowledge, all methods that use a single input image require explicit color segmentation to deal with multicolored surfaces. Unfortunately, for complex textured images, current color segmentation algorithms are still problematic to segment correctly. Consequently, a method without explicit color segmentation becomes indispensable and This work presents such a method. The method is based solely on colors, particularly chromaticity, without requiring any geometrical information. One of the basic ideas is to iteratively compare the intensity logarithmic differentiation of an input image and its specular-free image. A specular-free image is an image that has exactly the same geometrical profile as the diffuse component of the input image and that can be generated by shifting each pixel's intensity and maximum chromaticity nonlinearly. Unlike existing methods using a single image, all processes in the proposed method are done locally, involving a maximum of only two neighboring pixels. This local operation is useful for handling textured objects with complex multicolored scenes. Evaluations by comparison with the results of polarizing filters demonstrate the effectiveness of the proposed method.},
   number = {2},
   urldate = {2024-08-15},
   journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
   author = {Tan, R.T. and Ikeuchi, K.},
   month = feb,
   year = {2005},
   note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
   keywords = {chromaticity, dichromatic reflection model, diffuse reflection, Image segmentation, Index Terms- Reflection components separation, Optical polarization, Optical reflection, Physical optics, Pixel, Power distribution, Rough surfaces, Solid modeling, specular reflection, specular-free image., specular-to-diffuse mechanism, Surface roughness, Surface texture},
   pages = {178--193},
   file = {IEEE Xplore Abstract Record:C\:\\Users\\Nicholas\\Zotero\\storage\\62ZRD4I2\\1374865.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\P4QCY3PQ\\Tan and Ikeuchi - 2005 - Separating reflection components of textured surfa.pdf:application/pdf},
}

@inproceedings{li_single_2014,
   title = {Single {Image} {Layer} {Separation} using {Relative} {Smoothness}},
   url = {https://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Li_Single_Image_Layer_2014_CVPR_paper.html},
   urldate = {2024-08-15},
   author = {Li, Yu and Brown, Michael S.},
   year = {2014},
   pages = {2752--2759},
   file = {Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\CN66JDMI\\Li and Brown - 2014 - Single Image Layer Separation using Relative Smoot.pdf:application/pdf},
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Optimization (and convex analysis)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{friedlander_exact_2008,
   title = {Exact {Regularization} of {Convex} {Programs}},
   volume = {18},
   issn = {1052-6234},
   url = {https://epubs.siam.org/doi/abs/10.1137/060675320},
   doi = {10.1137/060675320},
   abstract = {We propose a new regularization scheme for mathematical programs with complementarity constraints (MPCC) by relaxing all the constraints of the complementarity system. We show that, under the MPCC-linear independence constraint qualifications (MPCC-LICQ), the Lagrange multipliers exist for this regularization. Our method has strong convergence properties under MPCC-linear independence constraint qualifications and some weak conditions of the strict complementarity. In particular, under MPCC-LICQ, it is shown that any accumulation point of the regularized stationary points is M-stationary for the MPCC problem, and if the asymptotically weak nondegeneracy condition holds at a stationary point of the regularized problem, then it is strongly stationary. An algorithm for solving the proposed regularization is presented and numerical experiments are reported. Some comparisons with other methods are discussed with illustrative examples.},
   number = {4},
   urldate = {2024-09-04},
   journal = {SIAM Journal on Optimization},
   author = {Friedlander, Michael P. and Tseng, Paul},
   month = jan,
   year = {2008},
   note = {Publisher: Society for Industrial and Applied Mathematics},
   pages = {1326--1350},
   file = {Friedlander and Tseng - 2008 - Exact Regularization of Convex Programs.pdf:C\:\\Users\\Nicholas\\Zotero\\storage\\3293EEF8\\Friedlander and Tseng - 2008 - Exact Regularization of Convex Programs.pdf:application/pdf},
}

@book{foucart_mathematical_2013,
   address = {New York, NY},
   series = {Applied and {Numerical} {Harmonic} {Analysis}},
   title = {A {Mathematical} {Introduction} to {Compressive} {Sensing}},
   copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
   isbn = {978-0-8176-4947-0 978-0-8176-4948-7},
   url = {https://link.springer.com/10.1007/978-0-8176-4948-7},
   language = {en},
   urldate = {2024-09-04},
   publisher = {Springer},
   author = {Foucart, Simon and Rauhut, Holger},
   year = {2013},
   doi = {10.1007/978-0-8176-4948-7},
   keywords = {compressed sampling, compressed sensing, compressive sampling, greedy algorithms, optimization theory, probability theory, random matrices, sampling theory, signal processing, sparse recovery},
   file = {Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\6HWLP43P\\Foucart and Rauhut - 2013 - A Mathematical Introduction to Compressive Sensing.pdf:application/pdf},
}

@article{zou2005regularization,
  title={Regularization and variable selection via the elastic net},
  author={Zou, Hui and Hastie, Trevor},
  journal={Journal of the Royal Statistical Society: Series B (statistical methodology)},
  volume={67},
  number={2},
  pages={301--320},
  year={2005},
  publisher={Wiley Online Library}
}


@article{rudin_nonlinear_1992,
   title = {Nonlinear total variation based noise removal algorithms},
   volume = {60},
   issn = {0167-2789},
   url = {https://www.sciencedirect.com/science/article/pii/016727899290242F},
   doi = {10.1016/0167-2789(92)90242-F},
   abstract = {A constrained optimization type of numerical algorithm for removing noise from images is presented. The total variation of the image is minimized subject to constraints involving the statistics of the noise. The constraints are imposed using Lanrange multipliers. The solution is obtained using the gradient-projection method. This amounts to solving a time dependent partial differential equation on a manifold determined by the constraints. As t → ∞ the solution converges to a steady state which is the denoised image. The numerical algorithm is simple and relatively fast. The results appear to be state-of-the-art for very noisy images. The method is noninvasive, yielding sharp edges in the image. The technique could be interpreted as a first step of moving each level set of the image normal to itself with velocity equal to the curvature of the level set divided by the magnitude of the gradient of the image, and a second step which projects the image back onto the constraint set.},
   number = {1},
   urldate = {2024-08-15},
   journal = {Physica D: Nonlinear Phenomena},
   author = {Rudin, Leonid I. and Osher, Stanley and Fatemi, Emad},
   month = nov,
   year = {1992},
   pages = {259--268},
   file = {Rudin et al. - 1992 - Nonlinear total variation based noise removal algo.pdf:C\:\\Users\\Nicholas\\Zotero\\storage\\ZFQGU6VD\\Rudin et al. - 1992 - Nonlinear total variation based noise removal algo.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Nicholas\\Zotero\\storage\\SHIRTBM7\\016727899290242F.html:text/html},
}

@book{boumal_introduction_2023,
   address = {Cambridge},
   title = {An {Introduction} to {Optimization} on {Smooth} {Manifolds}},
   isbn = {978-1-00-916617-1},
   url = {https://www.cambridge.org/core/books/an-introduction-to-optimization-on-smooth-manifolds/EAF2B35457B7034AC747188DC2FFC058},
   abstract = {Optimization on Riemannian manifolds-the result of smooth geometry and optimization merging into one elegant modern framework-spans many areas of science and engineering, including machine learning, computer vision, signal processing, dynamical systems and scientific computing. This text introduces the differential geometry and Riemannian geometry concepts that will help students and researchers in applied mathematics, computer science and engineering gain a firm mathematical grounding to use these tools confidently in their research. Its charts-last approach will prove more intuitive from an optimizer's viewpoint, and all definitions and theorems are motivated to build time-tested optimization algorithms. Starting from first principles, the text goes on to cover current research on topics including worst-case complexity and geodesic convexity. Readers will appreciate the tricks of the trade for conducting research and for numerical implementations sprinkled throughout the book.},
   urldate = {2024-09-24},
   publisher = {Cambridge University Press},
   author = {Boumal, Nicolas},
   year = {2023},
   doi = {10.1017/9781009166164},
   file = {Boumal - 2023 - An Introduction to Optimization on Smooth Manifold.pdf:C\:\\Users\\Nicholas\\Zotero\\storage\\YTFLERQZ\\Boumal - 2023 - An Introduction to Optimization on Smooth Manifold.pdf:application/pdf;Snapshot:C\:\\Users\\Nicholas\\Zotero\\storage\\KNVRNH8G\\EAF2B35457B7034AC747188DC2FFC058.html:text/html},
}

@article{hu_brief_2020,
   title = {A {Brief} {Introduction} to {Manifold} {Optimization}},
   volume = {8},
   issn = {2194-6698},
   url = {https://doi.org/10.1007/s40305-020-00295-9},
   doi = {10.1007/s40305-020-00295-9},
   abstract = {Manifold optimization is ubiquitous in computational and applied mathematics, statistics, engineering, machine learning, physics, chemistry, etc. One of the main challenges usually is the non-convexity of the manifold constraints. By utilizing the geometry of manifold, a large class of constrained optimization problems can be viewed as unconstrained optimization problems on manifold. From this perspective, intrinsic structures, optimality conditions and numerical algorithms for manifold optimization are investigated. Some recent progress on the theoretical results of manifold optimization is also presented.},
   language = {en},
   number = {2},
   urldate = {2024-08-15},
   journal = {Journal of the Operations Research Society of China},
   author = {Hu, Jiang and Liu, Xin and Wen, Zai-Wen and Yuan, Ya-Xiang},
   month = jun,
   year = {2020},
   keywords = {15A18, 49Q99, 65K05, 90C22, 90C26, 90C27, 90C30, Convergence, First-order-type algorithms, Manifold optimization, Retraction, Second-order-type algorithms},
   pages = {199--248},
   file = {Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\EUQ9AN6Z\\Hu et al. - 2020 - A Brief Introduction to Manifold Optimization.pdf:application/pdf},
}


@article{beck_mirror_2003,
   title = {Mirror descent and nonlinear projected subgradient methods for convex optimization},
   volume = {31},
   issn = {0167-6377},
   url = {https://www.sciencedirect.com/science/article/pii/S0167637702002316},
   doi = {10.1016/S0167-6377(02)00231-6},
   abstract = {The mirror descent algorithm (MDA) was introduced by Nemirovsky and Yudin for solving convex optimization problems. This method exhibits an efficiency estimate that is mildly dependent in the decision variables dimension, and thus suitable for solving very large scale optimization problems. We present a new derivation and analysis of this algorithm. We show that the MDA can be viewed as a nonlinear projected-subgradient type method, derived from using a general distance-like function instead of the usual Euclidean squared distance. Within this interpretation, we derive in a simple way convergence and efficiency estimates. We then propose an Entropic mirror descent algorithm for convex minimization over the unit simplex, with a global efficiency estimate proven to be mildly dependent in the dimension of the problem.},
   number = {3},
   urldate = {2024-08-15},
   journal = {Operations Research Letters},
   author = {Beck, Amir and Teboulle, Marc},
   month = may,
   year = {2003},
   keywords = {Complexity analysis, Global rate of convergence, Mirror descent algorithms, Nonlinear projections, Nonsmooth convex minimization, Projected subgradient methods, Relative entropy},
   pages = {167--175},
   file = {Beck and Teboulle - 2003 - Mirror descent and nonlinear projected subgradient.pdf:C\:\\Users\\Nicholas\\Zotero\\storage\\KU8RJJUC\\Beck and Teboulle - 2003 - Mirror descent and nonlinear projected subgradient.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Nicholas\\Zotero\\storage\\ANNVQZQM\\S0167637702002316.html:text/html},
}

@article{kim_algorithms_2014,
   title = {Algorithms for nonnegative matrix and tensor factorizations: a unified view based on block coordinate descent framework},
   volume = {58},
   issn = {1573-2916},
   shorttitle = {Algorithms for nonnegative matrix and tensor factorizations},
   url = {https://doi.org/10.1007/s10898-013-0035-4},
   doi = {10.1007/s10898-013-0035-4},
   abstract = {We review algorithms developed for nonnegative matrix factorization (NMF) and nonnegative tensor factorization (NTF) from a unified view based on the block coordinate descent (BCD) framework. NMF and NTF are low-rank approximation methods for matrices and tensors in which the low-rank factors are constrained to have only nonnegative elements. The nonnegativity constraints have been shown to enable natural interpretations and allow better solutions in numerous applications including text analysis, computer vision, and bioinformatics. However, the computation of NMF and NTF remains challenging and expensive due the constraints. Numerous algorithmic approaches have been proposed to efficiently compute NMF and NTF. The BCD framework in constrained non-linear optimization readily explains the theoretical convergence properties of several efficient NMF and NTF algorithms, which are consistent with experimental observations reported in literature. In addition, we discuss algorithms that do not fit in the BCD framework contrasting them from those based on the BCD framework. With insights acquired from the unified perspective, we also propose efficient algorithms for updating NMF when there is a small change in the reduced dimension or in the data. The effectiveness of the proposed updating algorithms are validated experimentally with synthetic and real-world data sets.},
   language = {en},
   number = {2},
   urldate = {2024-08-15},
   journal = {Journal of Global Optimization},
   author = {Kim, Jingu and He, Yunlong and Park, Haesun},
   month = feb,
   year = {2014},
   keywords = {Block coordinate descent, Low-rank approximation, Nonnegative matrix factorization, Nonnegative tensor factorization},
   pages = {285--319},
   file = {Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\X3Y5BNFT\\Kim et al. - 2014 - Algorithms for nonnegative matrix and tensor facto.pdf:application/pdf},
}


@article{nasser_deep_2022,
   title = {Deep {Unfolding} for {Non}-{Negative} {Matrix} {Factorization} with {Application} to {Mutational} {Signature} {Analysis}},
   volume = {29},
   url = {https://www.liebertpub.com/doi/full/10.1089/cmb.2021.0438},
   doi = {10.1089/cmb.2021.0438},
   abstract = {Non-negative matrix factorization (NMF) is a fundamental matrix decomposition technique that is used primarily for dimensionality reduction and is increasing in popularity in the biological domain. Although finding a unique NMF is generally not possible, there are various iterative algorithms for NMF optimization that converge to locally optimal solutions. Such techniques can also serve as a starting point for deep learning methods that unroll the algorithmic iterations into layers of a deep network. In this study, we develop unfolded deep networks for NMF and several regularized variants in both a supervised and an unsupervised setting. We apply our method to various mutation data sets to reconstruct their underlying mutational signatures and their exposures. We demonstrate the increased accuracy of our approach over standard formulations in analyzing simulated and real mutation data.},
   number = {1},
   urldate = {2023-03-08},
   journal = {Journal of Computational Biology},
   author = {Nasser, Rami and Eldar, Yonina C. and Sharan, Roded},
   month = jan,
   year = {2022},
   note = {Publisher: Mary Ann Liebert, Inc., publishers},
   keywords = {NMF, unfold and deep network},
   pages = {45--55},
   file = {Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\KVKREBKD\\Nasser et al. - 2022 - Deep Unfolding for Non-Negative Matrix Factorizati.pdf:application/pdf},
}

@inproceedings{kondor_multiresolution_2014,
   title = {Multiresolution {Matrix} {Factorization}},
   url = {https://proceedings.mlr.press/v32/kondor14.html},
   abstract = {The types of large matrices that appear in modern Machine Learning problems often have complex hierarchical structures that go beyond what can be found by traditional linear algebra tools, such as eigendecompositions. Inspired by ideas from multiresolution analysis,   this paper introduces a new notion of matrix factorization that can capture structure in matrices at multiple different scales. The resulting Multiresolution Matrix Factorizations (MMFs) not only provide a wavelet basis for sparse approximation, but can also be used for matrix compression (similar to Nystrom approximations) and as a prior for matrix completion.},
   language = {en},
   urldate = {2024-08-15},
   booktitle = {Proceedings of the 31st {International} {Conference} on {Machine} {Learning}},
   publisher = {PMLR},
   author = {Kondor, Risi and Teneva, Nedelina and Garg, Vikas},
   month = jun,
   year = {2014},
   note = {ISSN: 1938-7228},
   pages = {1620--1628},
   file = {Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\4KHYQDZY\\Kondor et al. - 2014 - Multiresolution Matrix Factorization.pdf:application/pdf},
}


@article{yang_unified_2011,
   title = {Unified {Development} of {Multiplicative} {Algorithms} for {Linear} and {Quadratic} {Nonnegative} {Matrix} {Factorization}},
   volume = {22},
   issn = {1941-0093},
   url = {https://ieeexplore.ieee.org/abstract/document/6045343},
   doi = {10.1109/TNN.2011.2170094},
   abstract = {Multiplicative updates have been widely used in approximative nonnegative matrix factorization (NMF) optimization because they are convenient to deploy. Their convergence proof is usually based on the minimization of an auxiliary upper-bounding function, the construction of which however remains specific and only available for limited types of dissimilarity measures. Here we make significant progress in developing convergent multiplicative algorithms for NMF. First, we propose a general approach to derive the auxiliary function for a wide variety of NMF problems, as long as the approximation objective can be expressed as a finite sum of monomials with real exponents. Multiplicative algorithms with theoretical guarantee of monotonically decreasing objective function sequence can thus be obtained. The solutions of NMF based on most commonly used dissimilarity measures such as α - and β-divergence as well as many other more comprehensive divergences can be derived by the new unified principle. Second, our method is extended to a nonseparable case that includes e.g., γ-divergence and Rényi divergence. Third, we develop multiplicative algorithms for NMF using second-order approximative factorizations, in which each factorizing matrix may appear twice. Preliminary numerical experiments demonstrate that the multiplicative algorithms developed using the proposed procedure can achieve satisfactory Karush-Kuhn-Tucker optimality. We also demonstrate NMF problems where algorithms by the conventional method fail to guarantee descent at each iteration but those by our principle are immune to such violation.},
   number = {12},
   urldate = {2024-08-15},
   journal = {IEEE Transactions on Neural Networks},
   author = {Yang, Zhirong and Oja, Erkki},
   month = dec,
   year = {2011},
   note = {Conference Name: IEEE Transactions on Neural Networks},
   keywords = {Approximation error, Convergence, Divergence, matrix factorization, Minimization, multiplicative, nonnegative, optimization, Polynomials, Upper bound},
   pages = {1878--1891},
   file = {IEEE Xplore Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\PVCWTY7I\\Yang and Oja - 2011 - Unified Development of Multiplicative Algorithms f.pdf:application/pdf},
}

@misc{esposito_chordal-nmf_2024,
   title = {Chordal-{NMF} with {Riemannian} {Multiplicative} {Update}},
   url = {http://arxiv.org/abs/2405.12823},
   doi = {10.48550/arXiv.2405.12823},
   abstract = {Nonnegative Matrix Factorization (NMF) is the problem of approximating a given nonnegative matrix M through the conic combination of two nonnegative low-rank matrices W and H. Traditionally NMF is tackled by optimizing a specific objective function evaluating the quality of the approximation. This assessment is often done based on the Frobenius norm. In this study, we argue that the Frobenius norm as the "point-to-point" distance may not always be appropriate. Due to the nonnegative combination resulting in a polyhedral cone, this conic perspective of NMF may not naturally align with conventional point-to-point distance measures. Hence, a ray-to-ray chordal distance is proposed as an alternative way of measuring the discrepancy between M and WH. This measure is related to the Euclidean distance on the unit sphere, motivating us to employ nonsmooth manifold optimization approaches. We apply Riemannian optimization technique to solve chordal-NMF by casting it on a manifold. Unlike existing works on Riemannian optimization that require the manifold to be smooth, the nonnegativity in chordal-NMF is a non-differentiable manifold. We propose a Riemannian Multiplicative Update (RMU) that preserves the convergence properties of Riemannian gradient descent without breaking the smoothness condition on the manifold. We showcase the effectiveness of the Chordal-NMF on synthetic datasets as well as real-world multispectral images.},
   urldate = {2024-08-15},
   publisher = {arXiv},
   author = {Esposito, Flavia and Ang, Andersen},
   month = may,
   year = {2024},
   note = {arXiv:2405.12823 [cs, math]},
   keywords = {15A23, 78M50, 49Q99, 90C26, 90C30, Mathematics - Numerical Analysis, Mathematics - Optimization and Control},
   annote = {Comment: 32 pages, 7 figures, 3 tables. arXiv admin note: text overlap with arXiv:1907.02404 by other authors},
   file = {arXiv Fulltext PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\YBLPADIW\\Esposito and Ang - 2024 - Chordal-NMF with Riemannian Multiplicative Update.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Nicholas\\Zotero\\storage\\GMTAMS2R\\2405.html:text/html},
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% IAM Retreat
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{vavasis_complexity_2010,
	title = {On the {Complexity} of {Nonnegative} {Matrix} {Factorization}},
	volume = {20},
	issn = {1052-6234},
	url = {https://epubs.siam.org/doi/10.1137/070709967},
	doi = {10.1137/070709967},
	abstract = {Nonnegative matrix factorization (NMF) determines a lower rank approximation of a matrix \$A {\textbackslash}in {\textbackslash}mathbb\{R\}{\textasciicircum}\{m {\textbackslash}times n\} {\textbackslash}approx WH\$ where an integer \$k {\textbackslash}ll {\textbackslash}min(m,n)\$ is given and nonnegativity is imposed on all components of the factors \$W {\textbackslash}in {\textbackslash}mathbb\{R\}{\textasciicircum}\{m {\textbackslash}times k\}\$ and \$H {\textbackslash}in {\textbackslash}mathbb\{R\}{\textasciicircum}\{k {\textbackslash}times n\}\$. NMF has attracted much attention for over a decade and has been successfully applied to numerous data analysis problems. In applications where the components of the data are necessarily nonnegative, such as chemical concentrations in experimental results or pixels in digital images, NMF provides a more relevant interpretation of the results since it gives nonsubtractive combinations of nonnegative basis vectors. In this paper, we introduce an algorithm for NMF based on alternating nonnegativity constrained least squares (NMF/ANLS) and the active set–based fast algorithm for nonnegativity constrained least squares with multiple right-hand side vectors, and we discuss its convergence properties and a rigorous convergence criterion based on the Karush–Kuhn–Tucker (KKT) conditions. In addition, we also describe algorithms for sparse NMFs and regularized NMF. We show how we impose a sparsity constraint on one of the factors by \$L\_1\$-norm minimization and discuss its convergence properties. Our algorithms are compared to other commonly used NMF algorithms in the literature on several test data sets in terms of their convergence behavior.},
	number = {3},
	urldate = {2024-03-18},
	journal = {SIAM Journal on Optimization},
	author = {Vavasis, Stephen A.},
	month = jan,
	year = {2010},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {1364--1377},
	file = {Submitted Version:C\:\\Users\\Nicholas\\Zotero\\storage\\ZNN9CEYW\\Vavasis - 2010 - On the Complexity of Nonnegative Matrix Factorizat.pdf:application/pdf},
}


@book{vershynin_HighDimensionalProbability_2018,
  title = {High-{{Dimensional Probability}}},
  author = {Vershynin, Roman},
  date = {2018-10},
  series = {Cambridge {{Series}} in {{Statistical}} and {{Probabilistic Mathematics}}},
  publisher = {{Cambridge University Press}},
  location = {{University of California, Irvine}},
  url = {https://www.math.uci.edu/~rvershyn/papers/HDP-book/HDP-book.html#},
  urldate = {2023-04-11},
  isbn = {978-1-108-24625-5},
  langid = {english},
  file = {C\:\\Users\\Nicholas\\Zotero\\storage\\BFQVITX9\\Vershynin - High-Dimensional Probability.pdf;C\:\\Users\\Nicholas\\Zotero\\storage\\I27NIDQR\\high-dimensional-probability-introduction-applications-data-science.html}
}

@incollection{wenBlockCoordinateDescent2012,
  title = {Block {{Coordinate Descent Methods}} for {{Semidefinite Programming}}},
  booktitle = {Handbook on {{Semidefinite}}, {{Conic}} and {{Polynomial Optimization}}},
  author = {Wen, Zaiwen and Goldfarb, Donald and Scheinberg, Katya},
  editor = {Anjos, Miguel F. and Lasserre, Jean B.},
  date = {2012},
  series = {International {{Series}} in {{Operations Research}} \& {{Management Science}}},
  pages = {533--564},
  publisher = {{Springer US}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4614-0769-0_19},
  url = {https://doi.org/10.1007/978-1-4614-0769-0_19},
  urldate = {2023-08-28},
  abstract = {We consider in this chapter block coordinate descent (BCD) methods for solving semidefinite programming (SDP) problems. These methods are based on sequentially minimizing the SDP problem’s objective function over blocks of variables corresponding to the elements of a single row (and column) of the positive semidefinite matrix X; hence, we will also refer to these methods as row-by-row (RBR) methods. Using properties of the (generalized) Schur complement with respect to the remaining fixed (n − 1)-dimensional principal submatrix of X, the positive semidefiniteness constraint on X reduces to a simple second-order cone constraint. It is well known that without certain safeguards, BCD methods cannot be guaranteed to converge in the presence of general constraints. Hence, to handle linear equality constraints, the methods that we describe here use an augmented Lagrangian approach. Since BCD methods are first-order methods, they are likely to work well only if each subproblem minimization can be performed very efficiently. Fortunately, this is the case for several important SDP problems, including the maxcut SDP relaxation and the minimum nuclear norm matrix completion problem, since closed-form solutions for the BCD subproblems that arise in these cases are available. We also describe how BCD can be applied to solve the sparse inverse covariance estimation problem by considering a dual formulation of this problem. The BCD approach is further generalized by using a rank-two update so that the coordinates can be changed in more than one row and column at each iteration. Finally, numerical results on the maxcut SDP relaxation and matrix completion problems are presented to demonstrate the robustness and efficiency of the BCD approach, especially if only moderately accurate solutions are desired.},
  isbn = {978-1-4614-0769-0},
  langid = {english},
  keywords = {Augmented Lagrangian Function,Augmented Lagrangian Method,Cholesky Factorization,Coordinate Descent Method,Matrix Completion},
  file = {C\:\\Users\\Nicholas\\Zotero\\storage\\XL4KE66R\\Wen et al. - 2012 - Block Coordinate Descent Methods for Semidefinite .pdf}
}

@article{xu_BlockCoordinateDescent_2013,
  title = {A {{Block Coordinate Descent Method}} for {{Regularized Multiconvex Optimization}} with {{Applications}} to {{Nonnegative Tensor Factorization}} and {{Completion}}},
  author = {Xu, Yangyang and Yin, Wotao},
  date = {2013-01},
  journaltitle = {SIAM Journal on Imaging Sciences},
  shortjournal = {SIAM J. Imaging Sci.},
  volume = {6},
  number = {3},
  pages = {1758--1789},
  issn = {1936-4954},
  doi = {10.1137/120887795},
  url = {http://epubs.siam.org/doi/10.1137/120887795},
  urldate = {2023-05-08},
  langid = {english}
}
@article{kroonenbergPrincipalComponentAnalysis1980,
  title = {Principal Component Analysis of Three-Mode Data by Means of Alternating Least Squares Algorithms},
  author = {Kroonenberg, Pieter M. and De Leeuw, Jan},
  date = {1980-03},
  journaltitle = {Psychometrika},
  shortjournal = {Psychometrika},
  volume = {45},
  number = {1},
  pages = {69--97},
  issn = {0033-3123, 1860-0980},
  doi = {10.1007/BF02293599},
  url = {http://link.springer.com/10.1007/BF02293599},
  urldate = {2023-12-08},
  langid = {english},
  file = {/Users/mpf/Documents/Zotero/storage/PANCYD92/Kroonenberg and De Leeuw - 1980 - Principal component analysis of three-mode data by.pdf}
}

@article{bezanson2017julia,
  title={Julia: A fresh approach to numerical computing},
  author={Bezanson, Jeff and Edelman, Alan and Karpinski, Stefan and Shah, Viral B},
  journal={SIAM review},
  volume={59},
  number={1},
  pages={65--98},
  year={2017},
  publisher={SIAM},
  url={https://doi.org/10.1137/141000671}
}

@book{hornTopicsMatrixAnalysis1991,
  title = {Topics in {{Matrix Analysis}}},
  author = {Horn, Roger A. and Johnson, Charles R.},
  year = {1991},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  doi = {10.1017/CBO9780511840371},
  urldate = {2024-03-11},
  abstract = {Building on the foundations of its predecessor volume, Matrix Analysis, this book treats in detail several topics in matrix theory not included in the previous volume, but with important applications and of special mathematical interest. As with the previous volume, the authors assume a background knowledge of elementary linear algebra and rudimentary analytical concepts. Many examples and exercises of varying difficulty are included.},
  isbn = {978-0-521-46713-1},
  file = {C:\Users\Nicholas\Zotero\storage\NVC3D7N3\B988495A235F1C3406EA484A2C477B03.html}
}

@misc{golubRosetakDocumentRank1977,
  type = {SSRN Scholarly Paper},
  title = {Rosetak {{Document}} 4: {{Rank Degeneracies}} and {{Least Square Problems}}},
  shorttitle = {Rosetak {{Document}} 4},
  author = {Golub, Gene H. and Klema, Virginia and Stewart, Gilbert W.},
  year = {1977},
  month = feb,
  number = {260355},
  address = {Rochester, NY},
  urldate = {2024-04-10},
  langid = {english},
  keywords = {Gene H. Golub,Gilbert W. Stewart,Rosetak Document 4: Rank Degeneracies and Least Square Problems,SSRN,Virginia Klema},
  file = {C:\Users\Nicholas\Zotero\storage\5DXV76WA\Golub et al. - 1977 - Rosetak Document 4 Rank Degeneracies and Least Sq.pdf}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Sediment Analysis
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@misc{Richardson_SedimentAnalysis_jl,
author = {Richardson, Nicholas and Graham, Naomi and Friedlander, Michael P. and Saylor, Joel},
title = {{SedimentAnalysis.jl}},
publisher = {GitHub},
journal = {GitHub},
howpublished = {\url{https://github.com/njericha/Sediment-Source-Analysis.jl}},
year = {2024},
}

@misc{Richardson_MatrixTensorFactorization_jl,
author = {Richardson, Nicholas and Graham, Naomi and Friedlander, Michael P.},
title = {{MatrixTensorFactor.jl}},
publisher = {GitHub},
journal = {GitHub},
howpublished = {\url{https://github.com/njericha/MatrixTensorFactor.jl}},
year = {2024},
}

@article{Montoison_Krylov_jl_A_Julia_2023,
author = {Montoison, Alexis and Orban, Dominique},
doi = {10.21105/joss.05187},
journal = {Journal of Open Source Software},
month = sep,
number = {89},
pages = {5187},
title = {{Krylov.jl: A Julia basket of hand-picked Krylov methods}},
url = {https://joss.theoj.org/papers/10.21105/joss.05187},
volume = {8},
year = {2023}
}

@book{abramowitz_HandbookMathematicalFunctions_1972,
  title = {Handbook of Mathematical Functions: With Formulas, Graphs and Mathematical Tables},
  shorttitle = {Handbook of Mathematical Functions},
  author = {Abramowitz, Milton and Stegun, Irene A.},
  editora = {Conference on mathematical tables and National science foundation and Massachusetts institute of technology},
  editoratype = {collaborator},
  date = {1972},
  series = {Dover Books on Advanced Mathematics},
  edition = {Unabridged, unaltered and corr. republ. of the 1964 ed},
  publisher = {{Dover publ}},
  location = {{New York}},
  isbn = {978-0-486-61272-0},
  langid = {english},
  file = {C\:\\Users\\Nicholas\\Zotero\\storage\\L7T95WSD\\abramowitz_and_stegun.pdf}
}


@book{barber_BayesianReasoningMachine_2012a,
  title = {Bayesian {{Reasoning}} and {{Machine Learning}}},
  author = {Barber, David},
  date = {2012-06-05},
  edition = {1},
  publisher = {{Cambridge University Press}},
  doi = {10.1017/CBO9780511804779},
  url = {https://www.cambridge.org/core/product/identifier/9780511804779/type/book},
  urldate = {2023-08-14},
  abstract = {Machine learning methods extract value from vast data sets quickly and with modest resources. They are established tools in a wide range of industrial applications, including search engines, DNA sequencing, stock market analysis, and robot locomotion, and their use is spreading rapidly. People who know the methods have their choice of rewarding jobs. This hands-on text opens these opportunities to computer science students with modest mathematical backgrounds. It is designed for final-year undergraduates and master's students with limited background in linear algebra and calculus. Comprehensive and coherent, it develops everything from basic reasoning to advanced techniques within the framework of graphical models. Students learn more than a menu of techniques, they develop analytical and problem-solving skills that equip them for the real world. Numerous examples and exercises, both computer based and theoretical, are included in every chapter. Resources for students and instructors, including a MATLAB toolbox, are available online.},
  isbn = {978-0-521-51814-7},
  langid = {english},
  file = {C\:\\Users\\Nicholas\\Zotero\\storage\\494KFDMV\\Barber - 2012 - Bayesian Reasoning and Machine Learning.pdf}
}

@article{besancon_DistributionsJlDefinition_2021,
  title = {Distributions.Jl: {{Definition}} and {{Modeling}} of {{Probability Distributions}} in the {{JuliaStats Ecosystem}}},
  shorttitle = {Distributions.Jl},
  author = {Besançon, Mathieu and Papamarkou, Theodore and Anthoff, David and Arslan, Alex and Byrne, Simon and Lin, Dahua and Pearson, John},
  date = {2021},
  journaltitle = {Journal of Statistical Software},
  shortjournal = {J. Stat. Soft.},
  volume = {98},
  number = {16},
  eprint = {1907.08611},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  issn = {1548-7660},
  doi = {10.18637/jss.v098.i16},
  url = {http://arxiv.org/abs/1907.08611},
  urldate = {2023-06-29},
  abstract = {Random variables and their distributions are a central part in many areas of statistical methods. The Distributions.jl package provides Julia users and developers tools for working with probability distributions, leveraging Julia features for their intuitive and flexible manipulation, while remaining highly efficient through zero-cost abstractions.},
  keywords = {Computer Science - Mathematical Software,Statistics - Computation},
  file = {C\:\\Users\\Nicholas\\Zotero\\storage\\KEQKW4A9\\Besançon et al. - 2021 - Distributions.jl Definition and Modeling of Proba.pdf;C\:\\Users\\Nicholas\\Zotero\\storage\\L678EHPK\\1907.html}
}

@article{hitchcockExpressionTensorPolyadic1927,
  title = {The {{Expression}} of a {{Tensor}} or a {{Polyadic}} as a {{Sum}} of {{Products}}},
  author = {Hitchcock, Frank L.},
  date = {1927},
  journaltitle = {Journal of Mathematics and Physics},
  volume = {6},
  number = {1-4},
  pages = {164--189},
  issn = {1467-9590},
  doi = {10.1002/sapm192761164},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sapm192761164},
  urldate = {2023-08-24},
  langid = {english},
  file = {C\:\\Users\\Nicholas\\Zotero\\storage\\3HXJIHYZ\\Hitchcock - 1927 - The Expression of a Tensor or a Polyadic as a Sum .pdf;C\:\\Users\\Nicholas\\Zotero\\storage\\MVJ6BWTI\\sapm192761164.html}
}

@article{kolda_TensorDecompositionsApplications_2009,
  title = {Tensor {{Decompositions}} and {{Applications}}},
  author = {Kolda, Tamara G. and Bader, Brett W.},
  date = {2009-08-06},
  journaltitle = {SIAM Review},
  shortjournal = {SIAM Rev.},
  volume = {51},
  number = {3},
  pages = {455--500},
  issn = {0036-1445, 1095-7200},
  doi = {10.1137/07070111X},
  url = {http://epubs.siam.org/doi/10.1137/07070111X},
  urldate = {2022-11-01},
  abstract = {This survey provides an overview of higher-order tensor decompositions, their applications, and available software. A tensor is a multidimensional or N -way array. Decompositions of higher-order tensors (i.e., N -way arrays with N ≥ 3) have applications in psychometrics, chemometrics, signal processing, numerical linear algebra, computer vision, numerical analysis, data mining, neuroscience, graph analysis, and elsewhere. Two particular tensor decompositions can be considered to be higher-order extensions of the matrix singular value decomposition: CANDECOMP/PARAFAC (CP) decomposes a tensor as a sum of rank-one tensors, and the Tucker decomposition is a higher-order form of principal component analysis. There are many other tensor decompositions, including INDSCAL, PARAFAC2, CANDELINC, DEDICOM, and PARATUCK2 as well as nonnegative variants of all of the above. The N-way Toolbox, Tensor Toolbox, and Multilinear Engine are examples of software packages for working with tensors.},
  langid = {english},
  file = {C\:\\Users\\Nicholas\\Zotero\\storage\\5HV3NICR\\Kolda and Bader - 2009 - Tensor Decompositions and Applications.pdf}
}

@article{lee_RecognizingPorphyryCopper_2021,
  title = {Recognizing {{Porphyry Copper Potential From Till Zircon Composition}}: {{A Case Study From The Highland Valley Porphyry District}}, {{South-central British Columbia}}},
  shorttitle = {{{RECOGNIZING PORPHYRY COPPER POTENTIAL FROM TILL ZIRCON COMPOSITION}}},
  author = {Lee, Robert G. and Plouffe, Alain and Ferbey, Travis and Hart, Craig J.R. and Hollings, Pete and Gleeson, Sarah A.},
  date = {2021-06-01},
  journaltitle = {Economic Geology},
  shortjournal = {Economic Geology},
  volume = {116},
  number = {4},
  pages = {1035--1045},
  issn = {0361-0128},
  doi = {10.5382/econgeo.4808},
  url = {https://doi.org/10.5382/econgeo.4808},
  urldate = {2023-08-17},
  abstract = {The detrital zircons in tills overlying the Guichon Creek batholith, British Columbia, Canada, have trace element concentrations and ages similar to those of zircons from the bedrock samples from which they are interpreted to have been sourced. Rocks from the core of the batholith that host porphyry copper mineralization have distinct zircon compositions relative to the distal, barren margin. We analyzed 296 zircons separated from 12 subglacial till samples to obtain U-Pb ages and trace element compositions. Laser ablation U-Pb ages of the detrital zircons overlap within error with chemical abrasion-thermal ionization mass spectrometry U-Pb ages of the Late Triassic Guichon Creek batholith and confirm that the detrital zircons are likely derived from the batholith. The youngest intrusions of the batholith produced the Highland Valley Copper porphyry deposits and contain distinctive zircons with elevated Eu/EuN* \&gt;0.4 attributed to high magmatic water contents and oxidation states, indicating higher porphyry copper potential. Zircon from till samples adjacent to and 9~km down-ice from the mineralized centers have mean Eu/EuN* \&gt;0.4, which are indicative of potential porphyry copper mineralization. Detrital zircon grains from more distal up- and down-ice locations (10--15~km) have zircon Eu/EuN* mean values of 0.26 to 0.37, reflecting background values. We conclude that detrital zircon compositions in glacial sediments transported several kilometers can be used to establish the regional potential for porphyry copper mineralization.},
  file = {C\:\\Users\\Nicholas\\Zotero\\storage\\A2NN8TPJ\\Lee et al. - 2021 - RECOGNIZING PORPHYRY COPPER POTENTIAL FROM TILL ZI.pdf;C\:\\Users\\Nicholas\\Zotero\\storage\\E7RLQK99\\RECOGNIZING-PORPHYRY-COPPER-POTENTIAL-FROM-TILL.html}
}

@article{leone_FoldedNormalDistribution_1961,
  title = {The {{Folded Normal Distribution}}},
  author = {Leone, F. C. and Nelson, L. S. and Nottingham, R. B.},
  date = {1961},
  journaltitle = {Technometrics},
  volume = {3},
  number = {4},
  eprint = {1266560},
  eprinttype = {jstor},
  pages = {543--550},
  publisher = {{[Taylor \& Francis, Ltd., American Statistical Association, American Society for Quality]}},
  issn = {0040-1706},
  doi = {10.2307/1266560},
  url = {https://www.jstor.org/stable/1266560},
  urldate = {2023-08-09},
  abstract = {Measurements are frequently recorded without their algebraic sign. As a consequence, the underlying distribution of measurements is replaced by a distribution of absolute measurements. When the underlying distribution is normal, the resulting distribution is called the "folded normal distribution". The authors describe methods for estimating the mean and standard deviation of the normal distribution based on estimates of the mean and standard deviation determined from the folded normal. Tables are provided to assist in the estimation procedure and an example included.}
}

@article{luo_TensorFactorizationPrecision_2017,
  title = {Tensor Factorization toward Precision Medicine},
  author = {Luo, Yuan and Wang, Fei and Szolovits, Peter},
  date = {2017-05-01},
  journaltitle = {Briefings in Bioinformatics},
  shortjournal = {Briefings in Bioinformatics},
  volume = {18},
  number = {3},
  pages = {511--514},
  issn = {1467-5463},
  doi = {10.1093/bib/bbw026},
  url = {https://doi.org/10.1093/bib/bbw026},
  urldate = {2023-08-21},
  abstract = {Precision medicine initiatives come amid the rapid growth in quantity and variety of biomedical data, which exceeds the capacity of matrix-oriented data representations and many current analysis algorithms. Tensor factorizations extend the matrix view to multiple modalities and support dimensionality reduction methods that identify latent groups of data for meaningful summarization of both features and instances. In this opinion article, we analyze the modest literature on applying tensor factorization to various biomedical fields including genotyping and phenotyping. Based on the cited work including work of our own, we suggest that tensor applications could serve as an effective tool to enable frequent updating of medical knowledge based on the continually growing scientific and clinical evidence. We encourage extensive experimental studies to tackle challenges including design choice of factorizations, integrating temporality and algorithm scalability.},
  file = {C\:\\Users\\Nicholas\\Zotero\\storage\\QU64YAAC\\Luo et al. - 2017 - Tensor factorization toward precision medicine.pdf;C\:\\Users\\Nicholas\\Zotero\\storage\\VP5B8L3D\\2453284.html}
}

@incollection{nesterov_NonlinearOptimization_2018,
  title = {Nonlinear {{Optimization}}},
  booktitle = {Lectures on {{Convex Optimization}}},
  author = {Nesterov, Yurii},
  editor = {Nesterov, Yurii},
  date = {2018},
  series = {Springer {{Optimization}} and {{Its Applications}}},
  pages = {3--58},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-91578-4_1},
  url = {https://doi.org/10.1007/978-3-319-91578-4_1},
  urldate = {2023-08-16},
  abstract = {In this chapter, we introduce the main notations and concepts used in Continuous Optimization. The first theoretical results are related to Complexity Analysis of the problems of Global Optimization. For these problems, we start with a very pessimistic lower performance guarantee. It implies that for any method there exists an optimization problem in ℝn\$\$\textbackslash mathbb \{R\}\^n\$\$which needs at least O1𝜖n\$\$O\textbackslash left (\{1 \textbackslash over \textbackslash epsilon \^n\}\textbackslash right )\$\$computations of the function values in order to approximate its global solution up to accuracy 𝜖. Therefore, in the next section we pass to local optimization, and consider two main methods, the Gradient Method and the Newton Method. For both of them, we establish some local rates of convergence. In the last section, we present some standard methods in General Nonlinear Optimization: the conjugate gradient methods, quasi-Newton methods, theory of Lagrangian relaxation, barrier methods and penalty function methods. For some of them, we prove global convergence results.},
  isbn = {978-3-319-91578-4},
  langid = {english},
  file = {C\:\\Users\\Nicholas\\Zotero\\storage\\IGK2TSNL\\Nesterov - 2018 - Nonlinear Optimization.pdf}
}

@article{naserErrorMetricsPerformance2023,
  title = {Error {{Metrics}} and {{Performance Fitness Indicators}} for {{Artificial Intelligence}} and {{Machine Learning}} in {{Engineering}} and {{Sciences}}},
  author = {Naser, M. Z. and Alavi, Amir H.},
  year = {2023},
  month = dec,
  journal = {Architecture, Structures and Construction},
  volume = {3},
  number = {4},
  pages = {499--517},
  issn = {2730-9894},
  doi = {10.1007/s44150-021-00015-8},
  urldate = {2024-03-16},
  abstract = {Artificial intelligence (AI) and~Machine learning (ML) train machines to achieve a high level of cognition and perform human-like analysis. Both AI and~ML seemingly fit into our daily lives as well as complex and interdisciplinary fields. With the rise of commercial, open-source, and user-catered AI/ML tools, a key question often arises whenever AI/ML is applied to explore a phenomenon or a scenario: what constitutes a good AI/ML~model? Keeping in mind that a proper answer to this question depends on various factors, this work presumes that a goodmodel optimally performs and best describes the phenomenon on hand. From this perspective, identifying proper assessment metrics to evaluate the performance of AI/ML models is not only necessary but is also warranted. As such, this paper examines 78 of the most commonly-used performance fitness and error metrics for regression and classification algorithms, with emphasis on engineering and sciences~applications.},
  langid = {english},
  keywords = {Classification,Error metrics,Machine learning,Regression},
  file = {C:\Users\Nicholas\Zotero\storage\MSQKRI2I\Naser and Alavi - 2023 - Error Metrics and Performance Fitness Indicators f.pdf}
}

@article{papastergiou_TensorDecompositionMultipleInstance_2018,
  title = {Tensor {{Decomposition}} for {{Multiple-Instance Classification}} of {{High-Order Medical Data}}},
  author = {Papastergiou, Thomas and Zacharaki, Evangelia I. and Megalooikonomou, Vasileios},
  date = {2018-12-06},
  journaltitle = {Complexity},
  shortjournal = {Complexity},
  volume = {2018},
  pages = {1--13},
  issn = {1076-2787, 1099-0526},
  doi = {10.1155/2018/8651930},
  url = {https://www.hindawi.com/journals/complexity/2018/8651930/},
  urldate = {2023-08-21},
  abstract = {Multidimensional data that occur in a variety of applications in clinical diagnostics and health care can naturally be represented by multidimensional arrays (i.e., tensors). Tensor decompositions offer valuable and powerful tools for latent concept discovery that can handle effectively missing values and noise. We propose a seamless, application-independent feature extraction and multiple-instance (MI) classification method, which represents the raw multidimensional, possibly incomplete, data by means of learning a high-order dictionary. The effectiveness of the proposed method is demonstrated in two application scenarios: (i) prediction of frailty in older people using multisensor recordings and (ii) breast cancer classification based on histopathology images. The proposed method outperforms or is comparable to the state-of-the-art multiple-instance learning classifiers highlighting its potential for computer-assisted diagnosis and health care support.},
  langid = {english},
  file = {C\:\\Users\\Nicholas\\Zotero\\storage\\PK64LU7F\\Papastergiou et al. - 2018 - Tensor Decomposition for Multiple-Instance Classif.pdf}
}

@online{qi_TripleDecompositionTensor_2020,
  title = {Triple {{Decomposition}} and {{Tensor Recovery}} of {{Third Order Tensors}}},
  author = {Qi, Liqun and Chen, Yannan and Bakshi, Mayank and Zhang, Xinzhen},
  date = {2020-03-01},
  eprint = {2002.02259},
  eprinttype = {arxiv},
  eprintclass = {cs, math},
  doi = {10.48550/arXiv.2002.02259},
  url = {http://arxiv.org/abs/2002.02259},
  urldate = {2023-08-01},
  abstract = {In this paper, we introduce a new tensor decomposition for third order tensors, which decomposes a third order tensor to three third order low rank tensors in a balanced way. We call such a decomposition the triple decomposition, and the corresponding rank the triple rank. For a third order tensor, its CP decomposition can be regarded as a special case of its triple decomposition. The triple rank of a third order tensor is not greater than the middle value of the Tucker rank, and is strictly less than the middle value of the Tucker rank for an essential class of examples. These indicate that practical data can be approximated by low rank triple decomposition as long as it can be approximated by low rank CP or Tucker decomposition. This theoretical discovery is confirmed numerically. Numerical tests show that third order tensor data from practical applications such as internet traffic and video image are of low triple ranks. A tensor recovery method based on low rank triple decomposition is proposed. Its convergence and convergence rate are established. Numerical experiments confirm the efficiency of this method.},
  pubstate = {preprint},
  keywords = {Mathematics - Numerical Analysis},
  file = {C\:\\Users\\Nicholas\\Zotero\\storage\\XNXUKF5Z\\Qi et al. - 2020 - Triple Decomposition and Tensor Recovery of Third .pdf;C\:\\Users\\Nicholas\\Zotero\\storage\\G8MNVYR8\\2002.html}
}

@book{rockafellar_ConvexAnalysis_1970,
  title = {Convex Analysis},
  author = {Rockafellar, R. Tyrrell},
  date = {1970},
  series = {Princeton Mathematical Series},
  number = {28},
  publisher = {{Princeton University Press}},
  location = {{Princeton, N.J}},
  isbn = {978-0-691-08069-7},
  langid = {english},
  pagetotal = {451},
  keywords = {Convex domains,Mathematical analysis},
  file = {C\:\\Users\\Nicholas\\Zotero\\storage\\HRFQKU44\\Rockafellar - 1970 - Convex analysis.pdf}
}


@article{saylor_CharacterizingSedimentSources_2019,
  title = {Characterizing Sediment Sources by Non-Negative Matrix Factorization of Detrital Geochronological Data},
  author = {Saylor, J.E. and Sundell, K.E. and Sharman, G.R.},
  date = {2019-04},
  journaltitle = {Earth and Planetary Science Letters},
  shortjournal = {Earth and Planetary Science Letters},
  volume = {512},
  pages = {46--58},
  issn = {0012821X},
  doi = {10.1016/j.epsl.2019.01.044},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0012821X19300706},
  urldate = {2023-05-05},
  langid = {english},
  file = {C\:\\Users\\Nicholas\\Zotero\\storage\\SQJ3V4JZ\\Saylor et al. - 2019 - Characterizing sediment sources by non-negative ma.pdf}
}
@book{silvermanDensityEstimationStatistics2018,
  title = {Density Estimation for Statistics and Data Analysis},
  author = {Silverman, Bernard W.},
  date = {2018},
  publisher = {{Routledge}},
  url = {https://www.taylorfrancis.com/books/mono/10.1201/9781315140919/density-estimation-statistics-data-analysis-bernard-silverman},
  urldate = {2023-12-18},
  file = {/Users/mpf/Documents/Zotero/storage/VH9AI2CK/Silverman - 2018 - Density estimation for statistics and data analysi.pdf}
}

@book{silverman_DensityEstimationStatistics_1986,
  title = {Density {{Estimation}} for {{Statistics}} and {{Data Analysis}}},
  author = {Silverman, Bernard W.},
  date = {1986-04-01},
  publisher = {{CRC Press}},
  abstract = {Although there has been a surge of interest in density estimation in recent years, much of the published research has been concerned with purely technical matters with insufficient emphasis given to the technique's practical value. Furthermore, the subject has been rather inaccessible to the general statistician.The account presented in this book places emphasis on topics of methodological importance, in the hope that this will facilitate broader practical application of density estimation and also encourage research into relevant theoretical work. The book also provides an introduction to the subject for those with general interests in statistics. The important role of density estimation as a graphical technique is reflected by the inclusion of more than 50 graphs and figures throughout the text.Several contexts in which density estimation can be used are discussed, including the exploration and presentation of data, nonparametric discriminant analysis, cluster analysis, simulation and the bootstrap, bump hunting, projection pursuit, and the estimation of hazard rates and other quantities that depend on the density. This book includes general survey of methods available for density estimation. The Kernel method, both for univariate and multivariate data, is discussed in detail, with particular emphasis on ways of deciding how much to smooth and on computation aspects. Attention is also given to adaptive methods, which smooth to a greater degree in the tails of the distribution, and to methods based on the idea of penalized likelihood.},
  isbn = {978-0-412-24620-3},
  langid = {english},
  pagetotal = {190},
  keywords = {Mathematics / Probability \& Statistics / General}
}

@article{sundell_CrustalThickeningNorthern_2022,
  title = {Crustal Thickening of the {Northern Central Andean Plateau} Inferred From Trace Elements in Zircon},
  author = {Sundell, Kurt E. and George, Sarah W.M. and Carrapa, Barbara and Gehrels, George E. and Ducea, Mihai N. and Saylor, Joel E. and Pepper, Martin},
  date = {2022},
  journaltitle = {Geophysical Research Letters},
  volume = {49},
  number = {3},
  pages = {1-9},
  issn = {1944-8007},
  doi = {10.1029/2021GL096443},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2021GL096443},
  urldate = {2023-06-30},
  abstract = {The timing of crustal thickening in the northern Central Andean Plateau (CAP), at 13--20°S, and its relationship to surface uplift is debated. Zircon qualitatively records crustal thickness as its trace element chemistry is controlled by the growth of cogenetic minerals and relative uptake of light and heavy Rare Earth Elements. Jurassic to Neogene zircons from volcanic rocks, sandstones, and river sediments reveal shifts in trace element ratios suggesting major crustal thickening at 80--55 Ma and 35--0 Ma, coincident with high-flux magmatism. An intervening magmatic lull due to shallow subduction obscures the magmatic record from 55 to 35 Ma during which thickening continued via crustal shortening. Protracted thickening since the Late Cretaceous correlates with early elevation gain of the CAP western margin, but contrasts with Miocene establishment of near modern elevation in the northern CAP and the onset of hyperaridity along the Pacific coast, highlighting their complex spatial and temporal relationship.},
  langid = {english},
  file = {C\:\\Users\\Nicholas\\Zotero\\storage\\BCGQI646\\Sundell et al. - 2022 - Crustal Thickening of the Northern Central Andean .pdf;C\:\\Users\\Nicholas\\Zotero\\storage\\NPXWRL5H\\2021GL096443.html}
}
% Joel's references

@article{Belousova2002,
   author = {Belousova, E. and Griffin, W. and O'Reilly, Suzanne Y. and Fisher, N.},
   title = {Igneous zircon: trace element composition as an indicator of source rock type},
   journal = {Contributions to Mineralogy and Petrology},
   volume = {143},
   number = {5},
   pages = {602-622},
   ISSN = {1432-0967},
   DOI = {10.1007/s00410-002-0364-7},
   url = {https://doi.org/10.1007/s00410-002-0364-7},
   year = {2002},
   type = {Journal Article}
}

@article{Campbell2005,
   author = {Campbell, I. H. and Reiners, P. W. and Allen, C. M. and Nicolescu, S. and Upadhyay, R.},
   title = {He-Pb double dating of detrital zircons from the Ganges and Indus Rivers: Implication for quantifying sediment recycling and provenance studies},
   journal = {Earth and Planetary Science Letters},
   volume = {237},
   number = {3-4},
   pages = {402--432},
   ISSN = {0012-821X},
   DOI = {10.1016/j.epsl.2005.06.043},
   url = {<Go to ISI>://000232155200007},
   year = {2005},
   type = {Journal Article}
}

@article{Capaldi2019,
   author = {Capaldi, Tomas N. and George, Sarah W. M. and Hirtz, Jaime A. and Horton, Brian K. and Stockli, Daniel F.},
   title = {Fluvial and Eolian Sediment Mixing During Changing Climate Conditions Recorded in Holocene Andean Foreland Deposits From Argentina (31--33°S)},
   journal = {Frontiers in Earth Science},
   volume = {7},
   number = {298},
   ISSN = {2296-6463},
   DOI = {10.3389/feart.2019.00298},
   url = {https://www.frontiersin.org/article/10.3389/feart.2019.00298},
   year = {2019},
   type = {Journal Article}
}

@article{Caracciolo2020,
   author = {Caracciolo, L},
   title = {Sediment generation and sediment routing systems from a quantitative provenance analysis perspective: Review, application and future development},
   journal = {Earth-Science Reviews},
   volume = {209},
   pages = {103226},
   ISSN = {0012-8252},
   year = {2020},
   type = {Journal Article}
}

@article{Chapman2015,
   author = {Chapman, James B. and Ducea, Mihai N. and DeCelles, Peter G. and Profeta, Lucia},
   title = {Tracking changes in crustal thickness during orogenic evolution with Sr/Y: An example from the North American Cordillera},
   journal = {Geology},
   volume = {43},
   number = {10},
   pages = {919-922},
   ISSN = {0091-7613},
   DOI = {10.1130/g36996.1},
   url = {https://doi.org/10.1130/G36996.1},
   year = {2015},
   type = {Journal Article}
}

@article{Cherniak1997a,
   author = {Cherniak, Daniele J and Hanchar, JM and Watson, EB},
   title = {Rare-earth diffusion in zircon},
   journal = {Chemical Geology},
   volume = {134},
   number = {4},
   pages = {289-301},
   ISSN = {0009-2541},
   year = {1997},
   type = {Journal Article}
}

@article{Cherniak1997b,
   author = {Cherniak, Daniele J and Hanchar, John M and Watson, E Bruce},
   title = {Diffusion of tetravalent cations in zircon},
   journal = {Contributions to Mineralogy and Petrology},
   volume = {127},
   pages = {383-390},
   ISSN = {0010-7999},
   year = {1997},
   type = {Journal Article}
}

@article{Dickinson1983,
   author = {Dickinson, W. R. and Beard, L. S. and Brakenridge, G. R. and Erjavec, J. L. and Ferguson, R. C. and Inman, K. F. and Knepp, R. A. and Lindberg, F. A. and Ryberg, P. T.},
   title = {Provenance of North-American Phanerozoic sandstones in relation to tectonic setting},
   journal = {Geological Society of America Bulletin},
   volume = {94},
   number = {2},
   pages = {222--235},
   ISSN = {0016-7606},
   url = {<Go to ISI>://A1983QJ57200005},
   year = {1983},
   type = {Journal Article}
}

@article{Dickinson2009,
   author = {Dickinson, W. R. and Lawton, T. F. and Gehrels, G. E.},
   title = {Recycling detrital zircons: A case study from the Cretaceous Bisbee Group of southern Arizona},
   journal = {Geology},
   volume = {37},
   number = {6},
   pages = {503--506},
   ISSN = {0091-7613},
   DOI = {10.1130/g25646a.1},
   url = {<Go to ISI>://000266655500008},
   year = {2009},
   type = {Journal Article}
}

@inbook{Fedo2003,
   author = {Fedo, C. M. and Sircombe, K. N. and Rainbird, R. H.},
   title = {Detrital zircon analysis of the sedimentary record},
   booktitle = {Zircon: Reviews in Mineralogy and Geochemistry},
   editor = {Hanchar, J. M. and Hoskin, P. W. O.},
   series = {Reviews in Mineralogy \& Geochemistry},
   volume = {53},
   pages = {277--303},
   ISBN = {1529-6466
0-93995-065-0},
   url = {<Go to ISI>://000189199900010},
   year = {2003},
   type = {Book Section}
}

@article{Gehrels2014,
   author = {Gehrels, George and Pecha, Mark},
   title = {Detrital zircon U-Pb geochronology and Hf isotope geochemistry of Paleozoic and Triassic passive margin strata of western North America},
   journal = {Geosphere},
   volume = {10},
   number = {1},
   pages = {49--65},
   DOI = {10.1130/ges00889.1},
   year = {2014},
   type = {Journal Article}
}

@article{Jerolmack2010,
   author = {Jerolmack, Douglas J. and Paola, Chris},
   title = {Shredding of environmental signals by sediment transport},
   journal = {Geophysical Research Letters},
   volume = {37},
   number = {19},
   ISSN = {0094-8276},
   DOI = {https://doi.org/10.1029/2010GL044638},
   url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2010GL044638},
   year = {2010},
   type = {Journal Article}
}

@article{Johnsson1991,
   author = {Johnsson, M. J. and Stallard, R. F. and Lundberg, N.},
   title = {Controls on the composition of fluvial sands from a tropical weathering environment - Sands of the Orinoco River drainage-basin, Venezuala and Colombia},
   journal = {Geological Society of America Bulletin},
   volume = {103},
   number = {12},
   pages = {1622--1647},
   ISSN = {0016-7606},
   url = {<Go to ISI>://A1991GU00700007},
   year = {1991},
   type = {Journal Article}
}


# duplicate?
@article{RN33,
   author = {Kroonenberg, Pieter M and De Leeuw, Jan},
   title = {Principal component analysis of three-mode data by means of alternating least squares algorithms},
   journal = {Psychometrika},
   volume = {45},
   pages = {69-97},
   ISSN = {0033-3123},
   year = {1980},
   type = {Journal Article}
}

@article{Lawton2015,
   author = {Lawton, Timothy F. and Buller, Cody D. and Parr, Todd R.},
   title = {Provenance of a Permian erg on the western margin of Pangea: Depositional system of the Kungurian (late Leonardian) Castle Valley and White Rim sandstones and subjacent Cutler Group, Paradox Basin, Utah, USA},
   journal = {Geosphere},
   DOI = {10.1130/ges01174.1},
   url = {http://geosphere.gsapubs.org/content/early/2015/08/05/GES01174.1.abstract},
   year = {2015},
   type = {Journal Article}
}

@article{Lee2021,
   author = {Lee, Robert G and Plouffe, Alain and Ferbey, Travis and Hart, Craig JR and Hollings, Pete and Gleeson, Sarah A},
   title = {Recognizing porphyry copper potential from till zircon composition: A case study from the Highland Valley Porphyry District, south-central British Columbia},
   journal = {Economic Geology},
   year = {2021},
   type = {Journal Article}
}

@article{Mahoney2021,
   author = {Mahoney, J. Brian and Haggart, James W. and Grove, Marty and Kimbrough, David L. and Isava, Virginia and Link, Paul K. and Pecha, Mark E. and Fanning, C. Mark},
   title = {Evolution of the Late Cretaceous Nanaimo Basin, British Columbia, Canada: Definitive provenance links to northern latitudes},
   journal = {Geosphere},
   volume = {17},
   number = {6},
   pages = {2197-2233},
   ISSN = {1553-040X},
   DOI = {10.1130/ges02394.1},
   url = {https://doi.org/10.1130/GES02394.1},
   year = {2021},
   type = {Journal Article}
}

@article{Rahl2003,
   author = {Rahl, R. M. and Reiners, P. W. and Campbell, I. H. and Nicolescu, S. and Allen, C. M.},
   title = {Combined single-grain (U-Th)/He and U/Pb dating of detrital zircons from the Navajo Sandstone, Utah},
   journal = {Geology},
   volume = {31},
   number = {9},
   pages = {761--764},
   ISSN = {0091-7613},
   url = {<Go to ISI>://000185084800007},
   year = {2003},
   type = {Journal Article}
}

@article{Saylor2021tracking,
   author = {Saylor, Joel E. and Sundell, Kurt E.},
   title = {Tracking Proterozoic--Triassic sediment routing to western Laurentia via bivariate non-negative matrix factorization of detrital provenance data},
   journal = {Journal of the Geological Society},
   journaltitle = {Journal of the Geological Society},
   shortjournal = {Journal of the Geological Society},
   ISSN = {0016-7649},
   DOI = {10.1144/jgs2020-215},
   url = {https://doi.org/10.1144/jgs2020-215},
   year = {2021},
   type = {Journal Article}
}

@article{Saylor2023basin,
   author = {Saylor, Joel E. and Sundell, Kurt E. and Perez, Nicholas D. and Hensley, Jeffrey B. and McCain, Payton and Runyon, Brook and Alvarez, Paola and Cárdenas, José and Usnayo, Whitney P. and Valer, Carlos S.},
   title = {Basin formation, magmatism, and exhumation document southward migrating flat-slab subduction in the central Andes},
   journal = {Earth and Planetary Science Letters},
   volume = {606},
   pages = {118050},
   ISSN = {0012-821X},
   DOI = {https://doi.org/10.1016/j.epsl.2023.118050},
   url = {https://www.sciencedirect.com/science/article/pii/S0012821X23000638},
   year = {2023},
   type = {Journal Article}
}

@article{Sharman2017,
   author = {Sharman, Glenn R. and Johnstone, Samuel A.},
   title = {Sediment unmixing using detrital geochronology},
   journal = {Earth and Planetary Science Letters},
   volume = {477},
   number = {Supplement C},
   pages = {183--194},
   ISSN = {0012-821X},
   DOI = {https://doi.org/10.1016/j.epsl.2017.07.044},
   url = {http://www.sciencedirect.com/science/article/pii/S0012821X17304284},
   year = {2017},
   type = {Journal Article}
}

@article{Smith2023a,
   author = {Smith, Tyson M. and Saylor, Joel E. and Lapen, Tom J. and Hatfield, Kendall and Sundell, Kurt E.},
   title = {Identifying sources of non-unique detrital age distributions through integrated provenance analysis: An example from the Paleozoic Central Colorado Trough},
   journal = {Geosphere},
   ISSN = {1553-040X},
   DOI = {10.1130/ges02541.1},
   url = {https://doi.org/10.1130/GES02541.1},
   year = {2023},
   type = {Journal Article}
}

@article{Smith2023b,
   author = {Smith, Tyson M. and Saylor, Joel E. and Lapen, Tom J. and Leary, Ryan J. and Sundell, Kurt E.},
   title = {Large detrital zircon data set investigation and provenance mapping: Local versus regional and continental sediment sources before, during, and after Ancestral Rocky Mountain deformation},
   journal = {GSA Bulletin},
   ISSN = {0016-7606},
   DOI = {10.1130/b36285.1},
   url = {https://doi.org/10.1130/B36285.1},
   year = {2023},
   type = {Journal Article}
}

@inbook{Sundell2019,
   author = {Sundell, Kurt and Saylor, Joel E. and Pecha, Mark},
   title = {Provenance and recycling of detrital zircons from Cenozoic Altiplano strata and the crustal evolution of western South America from combined U-Pb and Lu-Hf isotopic analysis},
   booktitle = {Andean Tectonics},
   editor = {Horton, Brian K. and Folguera, Andrés},
   publisher = {Elsevier},
   pages = {363-397},
   ISBN = {978-0-12-816009-1},
   DOI = {10.1016/B978-0-12-816009-1.00014-9},
   url = {http://www.sciencedirect.com/science/article/pii/B9780128160091000149},
   year = {2019},
   type = {Book Section}
}

@article{Sundell2023,
   author = {Sundell, K. E. and Gehrels, G. E.  and Blum, M. D.  and Saylor, J. E. and Pecha, M. E. and Hundley, B. P.},
   title = {An exploratory study of “large-n” detrital zircon geochronology of the Book Cliffs, UT via rapid (3 s/analysis) U--Pb dating},
   journal = {Basin Research},
   DOI = {https://doi.org/10.1111/bre.12840},
   year = {2023},
   type = {Journal Article}
}

@article{Sundell2021,
   author = {Sundell, K. E. and Saylor, J. E.},
   title = {Two-dimensional Quantitative Comparison of Density Distributions in Detrital Geochronology and Geochemistry},
   journal = {Geochemistry, Geophysics, Geosystems},
   volume = {n/a},
   number = {n/a},
   pages = {e2020GC009559},
   ISSN = {1525-2027},
   DOI = {https://doi.org/10.1029/2020GC009559},
   url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2020GC009559},
   year = {2021},
   type = {Journal Article}
}

@article{Tang2020,
   author = {Tang, Ming and Ji, Wei-Qiang and Chu, Xu and Wu, Anbin and Chen, Chen},
   title = {Reconstructing crustal thickness evolution from europium anomalies in detrital zircons},
   journal = {Geology},
   volume = {49},
   number = {1},
   pages = {76-80},
   ISSN = {0091-7613},
   DOI = {10.1130/g47745.1},
   url = {https://doi.org/10.1130/G47745.1},
   year = {2020},
   type = {Journal Article}
}

@article{Vermeesch2012,
   author = {Vermeesch, P.},
   title = {On the visualisation of detrital age distributions},
   journal = {Chemical Geology},
   volume = {312},
   pages = {190--194},
   ISSN = {0009-2541},
   DOI = {10.1016/j.chemgeo.2012.04.021},
   url = {<Go to ISI>://WOS:000305862000015},
   year = {2012},
   type = {Journal Article}
}

@article{Vermeesch2015,
   author = {Vermeesch, Pieter and Garzanti, Eduardo},
   title = {Making geological sense of 'Big Data' in sedimentary provenance analysis},
   journal = {Chemical Geology},
   volume = {409},
   pages = {20--27},
   ISSN = {0009-2541},
   DOI = {http://dx.doi.org/10.1016/j.chemgeo.2015.05.004},
   url = {http://www.sciencedirect.com/science/article/pii/S0009254115002387},
   year = {2015},
   type = {Journal Article}
}

@article{Watson1997,
   author = {Watson, EB and Cherniak, DJ},
   title = {Oxygen diffusion in zircon},
   journal = {Earth and Planetary Science Letters},
   volume = {148},
   number = {3-4},
   pages = {527-544},
   ISSN = {0012-821X},
   year = {1997},
   type = {Journal Article}
}

@article{Watson2006,
   author = {Watson, EB and Wark, DA and Thomas, JB},
   title = {Crystallization thermometers for zircon and rutile},
   journal = {Contributions to Mineralogy and Petrology},
   volume = {151},
   number = {4},
   pages = {413},
   ISSN = {0010-7999},
   year = {2006},
   type = {Journal Article}
}

@article{Watson2005,
   author = {Watson, E Bruce and Harrison, TM},
   title = {Zircon thermometer reveals minimum melting conditions on earliest Earth},
   journal = {Science},
   volume = {308},
   number = {5723},
   pages = {841-844},
   ISSN = {0036-8075},
   year = {2005},
   type = {Journal Article}
}

@article{Weltje2004,
   author = {Weltje, Gert Jan},
   title = {A quantitative approach to capturing the compositional variability of modern sands},
   journal = {Sedimentary Geology},
   volume = {171},
   number = {1-4},
   pages = {59-77},
   ISSN = {0037-0738},
   year = {2004},
   type = {Journal Article}
}

@article{Weltje2012,
   author = {Weltje, G. J.},
   title = {Quantitative models of sediment generation and provenance: State of the art and future developments},
   journal = {Sedimentary Geology},
   volume = {280},
   pages = {4--20},
   ISSN = {0037-0738},
   DOI = {10.1016/j.sedgeo.2012.03.010},
   url = {<Go to ISI>://WOS:000311263300002},
   year = {2012},
   type = {Journal Article}
}




@article{Amidon2005,
   author = {Amidon, W.H. and Burbank, D.W. and Gehrels, G.E.},
   title = {Construction of detrital mineral populations: insights from mixing of U-Pb zircon ages in Himalayan rivers},
   journal = {Basin Research},
   volume = {17},
   pages = {463–485},
   year = {2005},
   type = {Journal Article}
}

@article{Bhattacharya1943,
   author = {Bhattacharya, A.},
   title = {On a measure of divergence between two statistical populations defined by their probability distributions},
   journal = {Bulletin of the Calcutta Mathmatical Society},
   volume = {35},
   pages = {99-109},
   year = {1943},
   type = {Journal Article}
}

@article{Campbell2020,
   author = {Campbell, Matthew J and Rosenbaum, Gideon and Allen, Charlotte M and Spandler, Carl},
   title = {Continental crustal growth processes revealed by detrital zircon petrochronology: Insights from Zealandia},
   journal = {Journal of Geophysical Research: Solid Earth},
   volume = {125},
   number = {8},
   pages = {e2019JB019075},
   ISSN = {2169-9313},
   year = {2020},
   type = {Journal Article}
}

@article{Carrapa2009,
   author = {Carrapa, B and DeCelles, PG and Reiners, PW and Gehrels, GE and Sudo, M},
   title = {Apatite triple dating and white mica 40Ar/39Ar thermochronology of syntectonic detritus in the Central Andes: A multiphase tectonothermal history},
   journal = {Geology},
   volume = {37},
   number = {5},
   pages = {407-410},
   ISSN = {1943-2682},
   year = {2009},
   type = {Journal Article}
}

@article{DeGraaff2003,
   author = {DeGraaff-Surpless, K. and Mahoney, J. B. and Wooden, J. L. and McWilliams, M. O.},
   title = {Lithofacies control in detrital zircon provenance studies: Insights from the Cretaceous Methow basin, southern Canadian Cordillera},
   journal = {Geological Society of America Bulletin},
   volume = {115},
   number = {8},
   pages = {899–915},
   ISSN = {0016-7606},
   url = {<Go to ISI>://000184652300001},
   year = {2003},
   type = {Journal Article}
}

@inbook{Gehrels2000,
   author = {Gehrels, George E. and Dickinson, William R.},
   title = {Detrital zircon geochronology of the Antler overlap and foreland basin assemblages, Nevada},
   booktitle = {Paleozoic and Triassic paleogeography and tectonics of western Nevada and Northern California},
   editor = {Soreghan, Michael J. and Gehrels, George E.},
   publisher = {Geological Society of America},
   volume = {347},
   pages = {57-63},
   ISBN = {9780813723471},
   DOI = {10.1130/0-8137-2347-7.57},
   url = {https://doi.org/10.1130/0-8137-2347-7.57},
   year = {2000},
   type = {Book Section}
}

@article{Licht2016,
   author = {Licht, A and Pullen, A and Kapp, P and Abell, J and Giesler, N},
   title = {Eolian cannibalism: Reworked loess and fluvial sediment as the main sources of the Chinese Loess Plateau},
   journal = {Geological Society of America Bulletin},
   volume = {128},
   number = {5-6},
   pages = {944–956},
   ISSN = {0016-7606},
   year = {2016},
   type = {Journal Article}
}

@article{Satkoski2013,
   author = {Satkoski, A. M. and Wilkinson, B. H. and Hietpas, J. and Samson, S. D.},
   title = {Likeness among detrital zircon populations-An approach to the comparison of age frequency data in time and space},
   journal = {Geological Society of America Bulletin},
   volume = {125},
   number = {11-12},
   pages = {1783–1799},
   ISSN = {0016-7606},
   DOI = {10.1130/b30888.1},
   url = {<Go to ISI>://WOS:000328507400006},
   year = {2013},
   type = {Journal Article}
}

@article{Saylor2012,
   author = {Saylor, J.E. and Stockli, D.F. and Horton, B. K. and Nie, J. and Mora, A},
   title = {Discriminating rapid exhumation from syndepositional volcanism using detrital zircon double dating: Implications for the tectonic history of the Eastern Cordillera, Colombia},
   journal = {Geological Society of America Bulletin},
   volume = {124},
   number = {5-6},
   pages = {762–779},
   DOI = {10.1130/B30534.1},
   year = {2012},
   type = {Journal Article}
}

@article{Saylor2016,
   author = {Saylor, Joel E. and Sundell, Kurt E.},
   title = {Quantifying comparison of large detrital geochronology data sets},
   journal = {Geosphere},
   volume = {12},
   pages = {203–220},
   DOI = {10.1130/ges01237.1},
   url = {http://geosphere.gsapubs.org/content/early/2016/01/07/GES01237.1.abstract},
   year = {2016},
   type = {Journal Article}
}


@article{Sircombe2000,
   author = {Sircombe, K. N.},
   title = {Quantitative comparison of large sets of geochronological data using multivariate analysis: A provenance study example from Australia},
   journal = {Geochimica et Cosmochimica Acta},
   volume = {64},
   number = {9},
   pages = {1593–1616},
   ISSN = {0016-7037},
   DOI = {10.1016/s0016-7037(99)00388-9},
   url = {<Go to ISI>://WOS:000086732400010},
   year = {2000},
   type = {Journal Article}
}

@article{Sundell2017,
   author = {Sundell, K.E. and Saylor, J. E.},
   title = {Unmixing detrital geochronology age distributions},
   journal = {Geophysics, Geochemistry, Geosystems},
   volume = {18},
   DOI = {10.1002/2016GC006774 },
   year = {2017},
   type = {Journal Article}
}

@article{Weltje1997,
   author = {Weltje, G. J.},
   title = {End-member modeiling of compositional data: Numerical-statistical algorithms for solving the explicit mixing problem},
   journal = {Mathematical Geology},
   volume = {29},
   pages = {503–549},
   year = {1997},
   type = {Journal Article}
}



@misc{graham_tracing_2024,
	title = {Tracing {Sedimentary} {Origins} in {Multivariate} {Geochronology} via {Constrained} {Tensor} {Factorization}},
	copyright = {All rights reserved},
	url = {https://friedlander.io/publications/2024-sediment-source-analysis/},
	urldate = {2024-06-28},
	author = {Graham, Naomi and Richardson, Nicholas and Friedlander, Michael P. and Saylor, Joel},
	month = may,
	year = {2024},
	file = {Michael P. Friedlander  UBC  Computational Mathe.pdf:C\:\\Users\\Nicholas\\Zotero\\storage\\JGF342AF\\Michael P. Friedlander  UBC  Computational Mathe.pdf:application/pdf;Michael P. Friedlander | UBC | Computational Mathematics:C\:\\Users\\Nicholas\\Zotero\\storage\\6JKKQWAN\\2024-sediment-source-analysis.html:text/html},
   note = {Preprint},
   journal = {Preprint},
}


@article{chen_tutorial_2017,
	title = {A tutorial on kernel density estimation and recent advances},
	volume = {1},
	issn = {2470-9360},
	url = {https://doi.org/10.1080/24709360.2017.1396742},
	doi = {10.1080/24709360.2017.1396742},
	abstract = {This tutorial provides a gentle introduction to kernel density estimation (KDE) and recent advances regarding confidence bands and geometric/topological features. We begin with a discussion of basic properties of KDE: the convergence rate under various metrics, density derivative estimation, and bandwidth selection. Then, we introduce common approaches to the construction of confidence intervals/bands, and we discuss how to handle bias. Next, we talk about recent advances in the inference of geometric and topological features of a density function using KDE. Finally, we illustrate how one can use KDE to estimate a cumulative distribution function and a receiver operating characteristic curve. We provide R implementations related to this tutorial at the end.},
	number = {1},
	urldate = {2023-11-09},
	journal = {Biostatistics \& Epidemiology},
	author = {Chen, Yen-Chi},
	month = jan,
	year = {2017},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/24709360.2017.1396742},
	keywords = {bootstrap, confidence bands, Kernel density estimation, nonparametric statistics},
	pages = {161--187},
	file = {Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\TDQP8KDZ\\Chen - 2017 - A tutorial on kernel density estimation and recent.pdf:application/pdf},
}



# spatial transcriptomics

@article{marxMethodYearSpatially2021,
  title = {Method of the {{Year}}: Spatially Resolved Transcriptomics},
  author = {Marx, Vivien},
  year = {2021},
  month = jan,
  journal = {Nature Methods},
  volume = {18},
  number = {1},
  pages = {9--14},
  issn = {1548-7105},
  doi = {10.1038/s41592-020-01033-y},
  abstract = {Nature Methods has crowned spatially resolved transcriptomics Method of the Year 2020.}
}

@article{peyre_NonlocalRegularizationInverse_2011a,
  title = {Non-Local {{Regularization}} of {{Inverse Problems}}},
  author = {Peyr{\'e}, Gabriel and Bougleux, S{\'e}bastien and Cohen, Laurent D.},
  year = {2011},
  journal = {Inverse Problems and Imaging},
  volume = {5},
  number = {2},
  pages = {511--530},
  publisher = {AIMS American Institute of Mathematical Sciences},
  doi = {10.3934/ipi.2011.5.511},
  urldate = {2024-06-29},
  abstract = {This article proposes a new framework to regularize imaging lin- ear inverse problems using an adaptive non-local energy. A non-local graph is optimized to match the structures of the image to recover. This allows a better reconstruction of geometric edges and textures present in natural images. A fast algorithm computes iteratively both the solution of the regularization pro- cess and the non-local graph adapted to this solution. The graph adaptation is efficient to solve inverse problems with randomized measurements such as inpainting random pixels or compressive sensing recovery. Our non-local regularization gives state-of-the-art results for this class of inverse problems. On more challenging problems such as image super-resolution, our method gives results comparable to sparse regularization in a translation invariant wavelet frame.},
  keywords = {compressive sensing,inpainting,Non-local regularization,super-resolution},
  file = {/Users/naomigraham/Zotero/storage/GTEDCTSQ/Peyré et al. - 2011 - Non-local Regularization of Inverse Problems.pdf}
}


@article{williams_introduction_2022,
   title = {An introduction to spatial transcriptomics for biomedical research},
   volume = {14},
   issn = {1756-994X},
   url = {https://doi.org/10.1186/s13073-022-01075-1},
   doi = {10.1186/s13073-022-01075-1},
   abstract = {Single-cell transcriptomics (scRNA-seq) has become essential for biomedical research over the past decade, particularly in developmental biology, cancer, immunology, and neuroscience. Most commercially available scRNA-seq protocols require cells to be recovered intact and viable from tissue. This has precluded many cell types from study and largely destroys the spatial context that could otherwise inform analyses of cell identity and function. An increasing number of commercially available platforms now facilitate spatially resolved, high-dimensional assessment of gene transcription, known as ‘spatial transcriptomics’. Here, we introduce different classes of method, which either record the locations of hybridized mRNA molecules in tissue, image the positions of cells themselves prior to assessment, or employ spatial arrays of mRNA probes of pre-determined location. We review sizes of tissue area that can be assessed, their spatial resolution, and the number and types of genes that can be profiled. We discuss if tissue preservation influences choice of platform, and provide guidance on whether specific platforms may be better suited to discovery screens or hypothesis testing. Finally, we introduce bioinformatic methods for analysing spatial transcriptomic data, including pre-processing, integration with existing scRNA-seq data, and inference of cell-cell interactions. Spatial -omics methods are already improving our understanding of human tissues in research, diagnostic, and therapeutic settings. To build upon these recent advancements, we provide entry-level guidance for those seeking to employ spatial transcriptomics in their own biomedical research.},
   number = {1},
   urldate = {2024-11-01},
   journal = {Genome Medicine},
   author = {Williams, Cameron G. and Lee, Hyun Jae and Asatsuma, Takahiro and Vento-Tormo, Roser and Haque, Ashraful},
   month = jun,
   year = {2022},
   pages = {68},
   file = {Full Text PDF:C\:\\Users\\Nicholas\\Zotero\\storage\\3X4BFJQZ\\Williams et al. - 2022 - An introduction to spatial transcriptomics for bio.pdf:application/pdf;Snapshot:C\:\\Users\\Nicholas\\Zotero\\storage\\IJNGVAY5\\s13073-022-01075-1.html:text/html},
}
